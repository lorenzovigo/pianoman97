{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RS_PYTORCH.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHcRKrNEbGNk"
      },
      "source": [
        "## **Installation . . .**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5si03_5ELaD"
      },
      "source": [
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "    raise Exception(\"You should enable GPU runtime\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXyKg_GqEMkC"
      },
      "source": [
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18g0kRL32ldP"
      },
      "source": [
        "## **Installing tensorboard and setting it up . . .**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d31BpRdgtcPL"
      },
      "source": [
        "In this session, I wanted to use the original Tensorboard instead of using the TensorboardColab version. Doing this, for example, we are able to add images or graphs and not just scalars. Besides, we are able to load different experiments on the same graphics thus allowing us to compare them in the same plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI4CPvqJ2VWL"
      },
      "source": [
        "%load_ext tensorboard "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMrWJHW82k4H"
      },
      "source": [
        "import os\n",
        "logs_base_dir = \"runs\"\n",
        "os.makedirs(logs_base_dir, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avgB5dpoCkXE"
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "tb_fm = SummaryWriter(log_dir=f'{logs_base_dir}/{logs_base_dir}_FM/')\n",
        "tb_gcn = SummaryWriter(log_dir=f'{logs_base_dir}/{logs_base_dir}_GCN/')\n",
        "tb_gcn_attention = SummaryWriter(log_dir=f'{logs_base_dir}/{logs_base_dir}_GCN_att/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q_Ioyxr0kas"
      },
      "source": [
        "## **Movielens - 100k dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR1yjsk30tbF"
      },
      "source": [
        "MovieLens [datasets](https://grouplens.org/datasets/movielens/) were collected by the GroupLens Research Project at the University of Minnesota.\n",
        " \n",
        "&nbsp;\n",
        "\n",
        "\n",
        "This data set consists of:\n",
        "\n",
        "* 100,000 ratings (1-5) from 943 users on 1682 movies. \n",
        "* Each user has rated at least 20 movies. \n",
        "* Simple demographic info for the users (age, gender, occupation, zip)\n",
        "\n",
        " &nbsp;\n",
        "\n",
        "The data was collected through the MovieLens web site (movielens.umn.edu) during the seven-month period from September 19th, 1997 through April 22nd, 1998. This data has been cleaned up - users who had less than 20 ratings or did not have complete demographic information were removed from this data set. \n",
        "\n",
        "\n",
        "> Note that the rating matrix is quite sparse (93.6% to be precise) as it only holds 100,000 ratings out of a possible 1,586,126 (943*1682).\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "For this notebook, we will use a preprocessed version of the original data in order to avoid the part of splitting the data in a specific way. The preprocessed dataset has been splitted following the *leave-one-out* strategy and so it has holded out one interaction of each user for testing / validation while keeeping the others for training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLZh09ZZEh9Q"
      },
      "source": [
        "### Preparing imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw3DnPH5Ej4z"
      },
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from IPython import embed\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "import scipy.sparse as sp\n",
        "from tqdm import tqdm, trange\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWJ7GgTNvEww"
      },
      "source": [
        "### **Downloading data and loading it with pandas ...**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haRxYM3Jb3Oh"
      },
      "source": [
        "if not os.path.exists('data/ml-100k'):\n",
        "    !gdown --id 1rE20sLow9sT2ULpBOOWqw2SEnpIm16OZ\n",
        "    !mkdir data\n",
        "    !unzip ml-dataset-splitted.zip && mv ml-dataset-splitted data/ml-100k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7MOlLUPu6Mw"
      },
      "source": [
        "!ls data/ml-100k/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkH-eDE5x2JJ"
      },
      "source": [
        "The data sets `movielens.train.rating`, `movielens.test.rating` are the splits generated from `u.data` ( which contains the entire data). They follow the \"leave-one-out\" strategy for splitting the data in a way that will allow us to **evaluate ranking prediction**. \n",
        "\n",
        " &nbsp;\n",
        "\n",
        "Both files have the same tab-separated format:\n",
        "\n",
        "    user_id   movie_id   rating   timestamp\n",
        "\n",
        "where `user_id` is an integer between 1 and 943, `movie_id` is an integer between 1 and 1682, `rating` is an integer between 1 and 5 and `timestamp`  is an epoch-based integer.\n",
        "\n",
        "<div>\n",
        "<center><img src=\"https://files.realpython.com/media/movielens-head.0542b4c067c7.jpg\" width=\"300\"/></center>\n",
        "</div>\n",
        "\n",
        "\n",
        "However, in the provided preprocessed splits we also have changed all rating tags for binary labels in order to deal with an `implicit feedback` task. So, all data from the dataset will have positive labels (`1`) denoting any interaction with a film as a case of being interesed in the film (even the user did not like it in the end) and, for negative labels (`0`), we will perform negative sampling thus sampling interactions that did not actually occured between user and a given item. So, now we can observe the data:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qlc-a62ihSoU"
      },
      "source": [
        "# LOAD TRAINING DATA\n",
        "colnames = [\"user_id\", 'item_id', 'label', 'timestamp']\n",
        "data = pd.read_csv('data/ml-100k/movielens.train.rating', sep=\"\\t\", header=None, names=colnames)\n",
        "data.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-EHrPNNiMDQ"
      },
      "source": [
        "# Unique value for the label is 1 (we will need to manually sample negative data)\n",
        "data.nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpmX0iGBibHS"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X3IorkGm0Yl"
      },
      "source": [
        "assert 100000 - 99057 == 943 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ba0eE98yT4f"
      },
      "source": [
        "So, while we can observe for the training data many interactions for each user, we see below that for the testing (or validation) set we have just holded out one interaction for user, which will be used as ground-truth when evaluating the model outputing a ranking."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqMj1F92ihc-"
      },
      "source": [
        "# LOAD TESTING DATA\n",
        "colnames = [\"user_id\", 'item_id', 'label', 'timestamp']\n",
        "test_data = pd.read_csv('data/ml-100k/movielens.test.rating', sep=\"\\t\", header=None, names=colnames)\n",
        "test_data.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwPdFbAWimnf"
      },
      "source": [
        "test_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttM4SbE3iXb-"
      },
      "source": [
        "> Note that we need to preprocess the dataset by for example re-indexing the films or removing the timestamp, which is not useful for our task. We also need to build the adjacency matrix and perform negative sampling for training. Besides, we will need to build the test set thus aiming to evaluate in a ranking way by following *HR* and *NDCG* metrics seen in theory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqmuDc2083Sq"
      },
      "source": [
        "### **Preprocessing dataset ...**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3f5Qm5yxLtD"
      },
      "source": [
        "We will first show how to preprocess data for some individual examples in `1. Understanding how to process data` section and finally we will construct a *Pytorch Dataset class* which will allow us to preprocess and handle the whole data in order to forward it to the model (it is done in `2. Building dataset and preparing data for the model` section).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX5-Pvy-gOLp"
      },
      "source": [
        "#### **1. Understanding how to process data...**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_Y_CsR9ma98"
      },
      "source": [
        "##### *Pre-process Movielens-100k*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9ztgsTXeDpa"
      },
      "source": [
        "# userId,movieId,rating,timestamp\n",
        "data = data.to_numpy()\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03xEQjtgeOJh"
      },
      "source": [
        "items = data[:, :2].astype(np.int) - 1  # -1 because ID begins from 1\n",
        "items"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5ZgSNuke_ye"
      },
      "source": [
        "np.max(items, axis=0)[:2] + 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfvR59QefgU0"
      },
      "source": [
        "# We need each node to have a unique id\n",
        "reindex_items = items.copy()\n",
        "reindex_items[:, 1] = reindex_items[:, 1] + 943\n",
        "reindex_items"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHQRN4GJft-a"
      },
      "source": [
        "field_dims = np.max(reindex_items, axis=0) + 1\n",
        "field_dims"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8wVVlp4cOlY"
      },
      "source": [
        "def build_adj_mx(dims, interactions):\n",
        "    train_mat = sp.dok_matrix((dims, dims), dtype=np.float32)\n",
        "    for x in tqdm(interactions, desc=\"BUILDING ADJACENCY MATRIX...\"):\n",
        "        train_mat[x[0], x[1]] = 1.0\n",
        "        train_mat[x[1], x[0]] = 1.0\n",
        "\n",
        "    return train_mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip9jrpD3gLGj"
      },
      "source": [
        "train_mat = build_adj_mx(field_dims[-1], reindex_items.copy())\n",
        "train_mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxqwNJpxgpIA"
      },
      "source": [
        "# Check that we have (2*99057 = 198114) interactions...\n",
        "99057*2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjLXNJj-T2Cf"
      },
      "source": [
        "##### *Checking we have just positive data:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2lKQf92YTJA"
      },
      "source": [
        "targets = data[:, 2]\n",
        "targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnBLIE5en4zK"
      },
      "source": [
        "np.unique(targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMxOy4uSmngm"
      },
      "source": [
        "##### *Example on performing negative data for a training sample: (u, i, j)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3ZGNi5iXoFW"
      },
      "source": [
        "data = np.c_[(reindex_items, targets)].astype(int)\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7twzF5FYse6"
      },
      "source": [
        "field_dims[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhY4DiE4ZkwS"
      },
      "source": [
        "# EXAMPLE interaction number 988 : user 6 - item 1470\n",
        "x = data[988]\n",
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q57mTXqLZm-8"
      },
      "source": [
        "neg_triplet = np.array([0,0,0])\n",
        "neg_triplet[0] = x[0].copy()\n",
        "neg_triplet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2QLBkZWYHbd"
      },
      "source": [
        "# Example: We find item 1200 has no connection with user 6\n",
        "j = 1200\n",
        "neg_triplet[1] = j\n",
        "neg_triplet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3whzmXbmUjMc"
      },
      "source": [
        "##### *Define metrics:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMhyY-hVUjWX"
      },
      "source": [
        "import math\n",
        "\n",
        "def getHitRatio(recommend_list, gt_item):\n",
        "    if gt_item in recommend_list:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def getNDCG(recommend_list, gt_item):\n",
        "    idx = np.where(recommend_list == gt_item)[0]\n",
        "    if len(idx) > 0:\n",
        "        return math.log(2)/math.log(idx+2)\n",
        "    else:\n",
        "        return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2pw4pMTmr_d"
      },
      "source": [
        "##### *Build test dataset for evaluation*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELX6l9N-eifC"
      },
      "source": [
        "dataset_path = 'data/ml-100k/movielens'\n",
        "test_data = pd.read_csv(f'{dataset_path}.test.rating', sep='\\t',\n",
        "                        header=None, names=colnames).to_numpy()\n",
        "test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzfx621OiaRL"
      },
      "source": [
        "# Take number of users and items from reindex items from train set\n",
        "users, items = np.max(reindex_items, axis=0)[:2] + 1 # [ 943, 1682])\n",
        "print(users)\n",
        "print(items)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ9T2YGvcn2w"
      },
      "source": [
        "# Reindex test items and substract 1\n",
        "pairs_test = test_data[:, :2].astype(np.int) - 1    \n",
        "pairs_test[:, 1] = pairs_test[:, 1] + users \n",
        "pairs_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kptPm6E4ozIZ"
      },
      "source": [
        "assert 74 + 943 - 1 == 1016"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad4BNmcCiN7v"
      },
      "source": [
        "pair = pairs_test[0]\n",
        "pair"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddiimRXgkCLW"
      },
      "source": [
        "# GENERATE TEST SET WITH NEGATIVE EXAMPLES TO EVALUATE\n",
        "max_users, max_items = field_dims[:2] # number users (943), number items (2625)\n",
        "negatives = []\n",
        "for t in range(10):\n",
        "    j = np.random.randint(max_users, max_items)\n",
        "    while (pair[0], j) in train_mat or j == pair[1]:\n",
        "        j = np.random.randint(max_users, max_items)\n",
        "    negatives.append(j)\n",
        "negatives"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNs5YdPhxJeL"
      },
      "source": [
        "single_user_test_set = np.vstack([pair, ] * (len(negatives)+1))\n",
        "single_user_test_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJoln4vNwxyK"
      },
      "source": [
        "single_user_test_set[:, 1][1:] = negatives\n",
        "single_user_test_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz5TAC52gVPz"
      },
      "source": [
        "#### **2. Building dataset and preparing data for the model ...**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7IkNtA2LeR6"
      },
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.utils.data\n",
        "\n",
        "\n",
        "class MovieLens100kDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    MovieLens 100k Dataset\n",
        "\n",
        "    Data preparation\n",
        "        treat samples with a rating less than 3 as negative samples\n",
        "\n",
        "    :param dataset_path: MovieLens dataset path\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset_path, num_negatives_train=4, num_negatives_test=100, sep='\\t'):\n",
        "\n",
        "        colnames = [\"user_id\", 'item_id', 'label', 'timestamp']\n",
        "        data = pd.read_csv(f'{dataset_path}.train.rating', sep=sep, header=None, names=colnames).to_numpy()\n",
        "        test_data = pd.read_csv(f'{dataset_path}.test.rating', sep=sep, header=None, names=colnames).to_numpy()\n",
        "\n",
        "        # TAKE items, targets and test_items\n",
        "        self.targets = data[:, 2]\n",
        "        self.items = self.preprocess_items(data)\n",
        "\n",
        "        # Save dimensions of max users and items and build training matrix\n",
        "        self.field_dims = np.max(self.items, axis=0) + 1 # ([ 943, 2625])\n",
        "        self.train_mat = build_adj_mx(self.field_dims[-1], self.items.copy())\n",
        "\n",
        "        # Generate train interactions with 4 negative samples for each positive\n",
        "        self.negative_sampling(num_negatives=num_negatives_train)\n",
        "        \n",
        "        # Build test set by passing as input the test item interactions\n",
        "        self.test_set = self.build_test_set(self.preprocess_items(test_data),\n",
        "                                            num_neg_samples_test = num_negatives_test)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.targets.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.interactions[index]\n",
        "    \n",
        "    def preprocess_items(self, data, users=943):\n",
        "        reindexed_items = data[:, :2].astype(np.int) - 1  # -1 because ID begins from 1\n",
        "        #users, items = np.max(reindexed_items, axis=0)[:2] + 1 # [ 943, 1682])\n",
        "        # Reindex items (we need to have [users + items] nodes with unique idx)\n",
        "        reindexed_items[:, 1] = reindexed_items[:, 1] + users\n",
        "\n",
        "        return reindexed_items\n",
        "\n",
        "    def negative_sampling(self, num_negatives=4):\n",
        "        self.interactions = []\n",
        "        data = np.c_[(self.items, self.targets)].astype(int)\n",
        "        max_users, max_items = self.field_dims[:2] # number users (943), number items (2625)\n",
        "\n",
        "        for x in tqdm(data, desc=\"Performing negative sampling on test data...\"):  # x are triplets (u, i , 1) \n",
        "            # Append positive interaction\n",
        "            self.interactions.append(x)\n",
        "            # Copy user and maintain last position to 0. Now we will need to update neg_triplet[1] with j\n",
        "            neg_triplet = np.vstack([x, ] * (num_negatives))\n",
        "            neg_triplet[:, 2] = np.zeros(num_negatives)\n",
        "\n",
        "            # Generate num_negatives negative interactions\n",
        "            for idx in range(num_negatives):\n",
        "                j = np.random.randint(max_users, max_items)\n",
        "                # IDEA: Loop to exclude true interactions (set to 1 in adj_train) user - item\n",
        "                while (x[0], j) in self.train_mat:\n",
        "                    j = np.random.randint(max_users, max_items)\n",
        "                neg_triplet[:, 1][idx] = j\n",
        "            self.interactions.append(neg_triplet.copy())\n",
        "\n",
        "        self.interactions = np.vstack(self.interactions)\n",
        "    \n",
        "    def build_test_set(self, gt_test_interactions, num_neg_samples_test=99):\n",
        "        max_users, max_items = self.field_dims[:2] # number users (943), number items (2625)\n",
        "        test_set = []\n",
        "        for pair in tqdm(gt_test_interactions, desc=\"BUILDING TEST SET...\"):\n",
        "            negatives = []\n",
        "            for t in range(num_neg_samples_test):\n",
        "                j = np.random.randint(max_users, max_items)\n",
        "                while (pair[0], j) in self.train_mat or j == pair[1]:\n",
        "                    j = np.random.randint(max_users, max_items)\n",
        "                negatives.append(j)\n",
        "            #APPEND TEST SETS FOR SINGLE USER\n",
        "            single_user_test_set = np.vstack([pair, ] * (len(negatives)+1))\n",
        "            single_user_test_set[:, 1][1:] = negatives\n",
        "            test_set.append(single_user_test_set.copy())\n",
        "        return test_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Okf_CKu-hQH6"
      },
      "source": [
        "full_dataset= MovieLens100kDataset(dataset_path, num_negatives_train=4, num_negatives_test=99)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kVPuJr0htZi"
      },
      "source": [
        "# 90570 interactions with pairs of index that have interacted + 4*90570 negative\n",
        "full_dataset.interactions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDarK-6QT7BT"
      },
      "source": [
        "full_dataset.interactions[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1rrSiUnoEGu"
      },
      "source": [
        "## We had 99057 interactions in training_matrix --> now we have 99057 positive plus 4*99057 negative\n",
        "assert 5*99057 == full_dataset.interactions.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjTmCleF8HR5"
      },
      "source": [
        "# For test set, we keep the size (one interaction per user) but we append 99 negative samples for evaluation\n",
        "print(len(full_dataset.test_set))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLB3P3tnywKg"
      },
      "source": [
        "len(full_dataset.test_set[0]) # --> [gt_pair + 99_neg_samples]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmDTn5mzUIKG"
      },
      "source": [
        "full_dataset.test_set[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXEAH46wppTY"
      },
      "source": [
        "Sampling 4 negative samples for each positive, will also work as a type of normalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VROSRVsH9NNb"
      },
      "source": [
        "data_loader = DataLoader(full_dataset, batch_size=256, shuffle=True, num_workers=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTvD_ftK9NUe"
      },
      "source": [
        "for i, (interactions) in enumerate(data_loader):\n",
        "    if i == 0:\n",
        "        print(interactions.shape)\n",
        "    else:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNAKSGghsm7_"
      },
      "source": [
        "### **Building Factorization Machines model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdgA3E4Js4mx"
      },
      "source": [
        "\n",
        "Our training matrix is now even sparser: Of all 237,746,250 values (90,570*2,625), only 181,140 are non-zero (90,570*2). In other words, the matrix is 99.92% sparse. Storing this as a dense matrix would be a massive waste of both storage and computing power!\n",
        "To avoid this, let’s use a scipy.lil_matrix sparse matrix for samples and a numpy array for labels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSlwmfoK0mA-"
      },
      "source": [
        "\n",
        "<div>\n",
        "<center><img src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2019/04/03/sagemaker-factorization-1.gif\" width=\"400\"/></center>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMmrziB8rvya"
      },
      "source": [
        "##### **LAYERS:** Linear and FM part of the equation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSFpvgYTu4Xg"
      },
      "source": [
        "# EMBEDDING PYTORCH: https://pytorch.org/docs/stable/nn.html?highlight=embedding#torch.nn.Embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cyYo9pXlSEc"
      },
      "source": [
        "# Linear part of the equation\n",
        "class FeaturesLinear(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, field_dims, output_dim=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc = torch.nn.Embedding(field_dims, output_dim)\n",
        "        self.bias = torch.nn.Parameter(torch.zeros((output_dim,)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
        "        \"\"\"\n",
        "        # self.fc(x).shape --> [batch_size, num_fields, 1]\n",
        "        # torch.sum(self.fc(x), dim=1).shape --> ([batch_size, 1])\n",
        "        return torch.sum(self.fc(x), dim=1) + self.bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7ibkJzxyCdt"
      },
      "source": [
        "# FM part of the equation\n",
        "class FM_operation(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, reduce_sum=True):\n",
        "        super().__init__()\n",
        "        self.reduce_sum = reduce_sum\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
        "        \"\"\"\n",
        "        square_of_sum = torch.sum(x, dim=1) ** 2\n",
        "        sum_of_square = torch.sum(x ** 2, dim=1)\n",
        "        ix = square_of_sum - sum_of_square\n",
        "        if self.reduce_sum:\n",
        "            ix = torch.sum(ix, dim=1, keepdim=True)\n",
        "        return 0.5 * ix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw-jiLKCsCDk"
      },
      "source": [
        "##### MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agCg8QZaxyAt"
      },
      "source": [
        "class FactorizationMachineModel(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A pytorch implementation of Factorization Machine.\n",
        "\n",
        "    Reference:\n",
        "        S Rendle, Factorization Machines, 2010.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, field_dims, embed_dim):\n",
        "        super().__init__()\n",
        "        # field_dims == total of nodes (sum users + context)\n",
        "        # self.linear = torch.nn.Linear(field_dims, 1, bias=True)\n",
        "        self.linear = FeaturesLinear(field_dims)\n",
        "        self.embedding = torch.nn.Embedding(field_dims, embed_dim, sparse=False)\n",
        "        self.fm = FM_operation(reduce_sum=True)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
        "\n",
        "    def forward(self, interaction_pairs):\n",
        "        \"\"\"\n",
        "        :param interaction_pairs: Long tensor of size ``(batch_size, num_fields)``\n",
        "        \"\"\"\n",
        "        out = self.linear(interaction_pairs) + self.fm(self.embedding(interaction_pairs))\n",
        "        \n",
        "        return out.squeeze(1)\n",
        "        \n",
        "    def predict(self, interactions, device):\n",
        "        # return the score, inputs are numpy arrays, outputs are tensors\n",
        "        test_interactions = torch.from_numpy(interactions).to(dtype=torch.long, device=device)\n",
        "        output_scores = self.forward(test_interactions)\n",
        "        return output_scores\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpTBEpD3C-_P"
      },
      "source": [
        "### **Workflow for FM with usual embeddings ...**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7jtY86it__d"
      },
      "source": [
        "#### **Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUDsUfdUDCkJ"
      },
      "source": [
        "from statistics import mean\n",
        "\n",
        "def train_one_epoch(model, optimizer, data_loader, criterion, device, log_interval=100):\n",
        "    model.train()\n",
        "    total_loss = []\n",
        "\n",
        "    for i, (interactions) in enumerate(data_loader):\n",
        "        interactions = interactions.to(device)\n",
        "        targets = interactions[:,2]\n",
        "        predictions = model(interactions[:,:2])\n",
        "        \n",
        "        loss = criterion(predictions, targets.float())\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss.append(loss.item())\n",
        "\n",
        "    return mean(total_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGbK80gNuEhT"
      },
      "source": [
        "#### **Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmtwbJ9HzuBQ"
      },
      "source": [
        "##### **Understanding evaluation ...**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCCH3ay7vUsm"
      },
      "source": [
        "len(full_dataset.test_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgwLEHv_vjRi"
      },
      "source": [
        "user_test = full_dataset.test_set[0]\n",
        "user_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_4TUpWClpDo"
      },
      "source": [
        "user_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYS-E499vpmZ"
      },
      "source": [
        "gt_pair = user_test[0]\n",
        "neg_items = user_test[1:]\n",
        "print(f'gt_pair: {gt_pair}')\n",
        "print(f'lenght neg_items: {len(neg_items)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoM-OgFB3BeG"
      },
      "source": [
        "# DEFINE GT_ITEM\n",
        "gt_item = user_test[0][1]\n",
        "gt_item"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xszUI7bzarC"
      },
      "source": [
        "# Defining dummy model with 8 embedding dimensions\n",
        "dummy_model = FactorizationMachineModel(full_dataset.field_dims[-1], 8).to(device)\n",
        "out = dummy_model.predict(user_test, device)\n",
        "out.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h28_id_zh5_"
      },
      "source": [
        "# Print first 10 predictions, where 1st one is the one for the GT\n",
        "out[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUpRWF6F1gyd"
      },
      "source": [
        "values, indices = torch.topk(out, 10)\n",
        "print(values)\n",
        "print(indices.cpu().detach().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBpBD5Rp2pyu"
      },
      "source": [
        "user_test[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWiJr5OA1qjT"
      },
      "source": [
        "# RANKING LIST TO RECOMMEND\n",
        "recommend_list = user_test[indices.cpu().detach().numpy()][:, 1]\n",
        "recommend_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LZ0Iz_63gzF"
      },
      "source": [
        "gt_item in recommend_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlva2UiB6LkI"
      },
      "source": [
        "##### **Defining test function...**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T43Y3aM-RM-U"
      },
      "source": [
        "def test(model, full_dataset, device, topk=10):\n",
        "    # Test the HR and NDCG for the model @topK\n",
        "    model.eval()\n",
        "\n",
        "    HR, NDCG = [], []\n",
        "\n",
        "    for user_test in full_dataset.test_set:\n",
        "        gt_item = user_test[0][1]\n",
        "\n",
        "        predictions = model.predict(user_test, device)\n",
        "        _, indices = torch.topk(predictions, topk)\n",
        "        recommend_list = user_test[indices.cpu().detach().numpy()][:, 1]\n",
        "\n",
        "        HR.append(getHitRatio(recommend_list, gt_item))\n",
        "        NDCG.append(getNDCG(recommend_list, gt_item))\n",
        "    return mean(HR), mean(NDCG)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWkS4HJFuLyL"
      },
      "source": [
        "#### **Model, loss and optimizer definition**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byFW63h-Dkel"
      },
      "source": [
        "model = FactorizationMachineModel(full_dataset.field_dims[-1], 32).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MSFVW-yDCrJ"
      },
      "source": [
        "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gYl3_7Iph14"
      },
      "source": [
        "#### **Random evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKK4U2LPPlR0"
      },
      "source": [
        "topk = 10\n",
        "\n",
        "# Check Init performance\n",
        "hr, ndcg = test(model, full_dataset, device, topk=topk)\n",
        "print(\"initial HR: \", hr)\n",
        "print(\"initial NDCG: \", ndcg)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHbKyX8vpqOa"
      },
      "source": [
        "#### **Start training the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gs86M3iUXFM"
      },
      "source": [
        "# DO EPOCHS NOW\n",
        "tb = True\n",
        "topk = 10\n",
        "for epoch_i in range(20):\n",
        "    #data_loader.dataset.negative_sampling()\n",
        "    train_loss = train_one_epoch(model, optimizer, data_loader, criterion, device)\n",
        "    hr, ndcg = test(model, full_dataset, device, topk=topk)\n",
        "\n",
        "    print('\\n')\n",
        "\n",
        "    print(f'epoch {epoch_i}:')\n",
        "    print(f'training loss = {train_loss:.4f} | Eval: HR@{topk} = {hr:.4f}, NDCG@{topk} = {ndcg:.4f} ')\n",
        "    print('\\n')\n",
        "    if tb:\n",
        "        tb_fm.add_scalar('train/loss', train_loss, epoch_i)\n",
        "        tb_fm.add_scalar('eval/HR@{topk}', hr, epoch_i)\n",
        "        tb_fm.add_scalar('eval/NDCG@{topk}', ndcg, epoch_i)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMfW67UKWkfT"
      },
      "source": [
        "## **VISUALIZING RESULTS**\n",
        "\n",
        "Once we have trained both models (*fm with usual embbedding layers* vs *fm with embeddings from gcn*), we can observe both metrics and loss in the same graphic in order to compare:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbSDTbdNdH3o"
      },
      "source": [
        "%tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}