{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cudf\nimport pandas as pd\nimport numpy as np\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n#from tensorflow.keras import layers\n\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import *\n\n\n\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\nfrom os.path import exists\nimport cv2, matplotlib.pyplot as plt\nprint('RAPIDS version',cudf.__version__)\n\n\ntrain = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv',nrows =20000000)\ntrain['article_id'] = train.article_id.astype('int32')\nprint( train.shape )\ntrain.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-17T10:32:20.271566Z","iopub.execute_input":"2022-03-17T10:32:20.271908Z","iopub.status.idle":"2022-03-17T10:33:07.110753Z","shell.execute_reply.started":"2022-03-17T10:32:20.271853Z","shell.execute_reply":"2022-03-17T10:33:07.109902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##### import math\ndf_customers = train.groupby(by='customer_id').count().sort_values(by='t_dat',ascending=False)\nprint(\"# Unique customer: \", df_customers.shape[0])\nprint(\"#customer with only one entry: \", df_customers[df_customers.t_dat==1].shape[0],\"(\",\"{:.2f}\".format(100*df_customers[df_customers.t_dat==1].shape[0]/df_customers.shape[0]),\"%)\")\nprint(\"#customer with 1 or 2 entries: \", df_customers[df_customers.t_dat==1].shape[0],\"(\",\"{:.2f}\".format(100*df_customers[df_customers.t_dat<3].shape[0]/df_customers.shape[0]),\"%)\")\n\nprint()\ndf_articles = train.groupby(by='article_id').count().sort_values(by='t_dat',ascending=False)\nprint(\"# Unique articles: \", df_articles.shape[0])\nprint(\"#customer with only one entry: \", df_articles[df_articles.t_dat==1].shape[0],\"(\",\"{:.2f}\".format(100*df_articles[df_articles.t_dat==1].shape[0]/df_articles.shape[0]),\"%)\")\nprint(\"#customer with 1 or 2 entries: \", df_articles[df_articles.t_dat==1].shape[0],\"(\",\"{:.2f}\".format(100*df_articles[df_articles.t_dat<3].shape[0]/df_articles.shape[0]),\"%)\")\nprint()\nprint(\"TOP 5 products\")\ndf_articles.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:33:22.731643Z","iopub.execute_input":"2022-03-17T10:33:22.731939Z","iopub.status.idle":"2022-03-17T10:33:37.889588Z","shell.execute_reply.started":"2022-03-17T10:33:22.731898Z","shell.execute_reply":"2022-03-17T10:33:37.888853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"items = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/articles.csv')\nitems.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:33:37.892016Z","iopub.execute_input":"2022-03-17T10:33:37.892294Z","iopub.status.idle":"2022-03-17T10:33:38.757953Z","shell.execute_reply.started":"2022-03-17T10:33:37.892259Z","shell.execute_reply":"2022-03-17T10:33:38.757251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# REDUCE DATASET: delete those articles sold less than X times\nmin_times = 50\ndf_tmp = train.groupby(by='customer_id').count()\ndf_tmp = df_tmp[df_tmp.article_id>min_times][['article_id']].reset_index()\ndf = train.merge(df_tmp[['customer_id']],on='customer_id')\nprint(train.shape[0],df.shape[0],df_tmp.shape[0])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:33:38.759377Z","iopub.execute_input":"2022-03-17T10:33:38.75963Z","iopub.status.idle":"2022-03-17T10:33:55.302151Z","shell.execute_reply.started":"2022-03-17T10:33:38.759596Z","shell.execute_reply":"2022-03-17T10:33:55.301422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add output column\ndf['output'] = 1 ","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:33:55.327543Z","iopub.execute_input":"2022-03-17T10:33:55.327859Z","iopub.status.idle":"2022-03-17T10:33:55.362715Z","shell.execute_reply.started":"2022-03-17T10:33:55.327816Z","shell.execute_reply":"2022-03-17T10:33:55.361912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#prepare data\nuser_ids = df[\"customer_id\"].unique().tolist()\nuser2user_encoded = {x: i for i, x in enumerate(user_ids)}\nuser_encoded2user = {i: x for i, x in enumerate(user_ids)}\narticle_ids = df[\"article_id\"].unique().tolist()\narticle2article_encoded = {x: i for i, x in enumerate(article_ids)}\narticle_encoded2article = {i: x for i, x in enumerate(article_ids)}\ndf[\"uid\"] = df[\"customer_id\"].map(user2user_encoded)\ndf[\"iid\"] = df[\"article_id\"].map(article2article_encoded)\n\nnum_users = len(user2user_encoded)\nnum_articles = len(article_encoded2article)\n\n## ADD NEGATIVES\n# we create a copy of the positives ones and shuffle the user id\ndf_neg = df.copy()\ndf_neg['uid'] = df_neg['uid'].sample(frac=1).reset_index(drop=True)\ndf_neg['output'] = 0\ndf_final = pd.concat([df, df_neg], ignore_index=True)\ndf_final = df_final.sample(frac=1)\ndf_final.head()\ndel df\ndel df_neg","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:33:55.364171Z","iopub.execute_input":"2022-03-17T10:33:55.364429Z","iopub.status.idle":"2022-03-17T10:34:11.679513Z","shell.execute_reply.started":"2022-03-17T10:33:55.364392Z","shell.execute_reply":"2022-03-17T10:34:11.678611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:34:11.680975Z","iopub.execute_input":"2022-03-17T10:34:11.681269Z","iopub.status.idle":"2022-03-17T10:34:11.693446Z","shell.execute_reply.started":"2022-03-17T10:34:11.68123Z","shell.execute_reply":"2022-03-17T10:34:11.692374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test = train_test_split(df_final[[\"uid\", \"iid\",\"price\",\"sales_channel_id\",\"output\"]], test_size=0.05, random_state=42)\n\nprint(\"Training data_set has \"+ str(X_train.shape[0]) +\" entries\")\nprint(\"Test data set has \"+ str(X_test.shape[0]) +\" entries\")\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:34:11.695357Z","iopub.execute_input":"2022-03-17T10:34:11.695823Z","iopub.status.idle":"2022-03-17T10:34:14.647785Z","shell.execute_reply.started":"2022-03-17T10:34:11.695781Z","shell.execute_reply":"2022-03-17T10:34:14.647081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deep Factorization Machines\nImplementation of [Deep Factorization Machines](https://arxiv.org/abs/1703.04247) with keras  \n![](https://bangdasun.github.io/images/deepfm.PNG)","metadata":{}},{"cell_type":"markdown","source":"## (1) Define Input Layers","metadata":{}},{"cell_type":"code","source":"def define_input_layers():\n    # numerical features\n    fea3_input = Input((1,), name = 'input_fea3')\n    num_inputs = [fea3_input]\n    # single level categorical features\n    uid_input = Input((1,), name = 'input_uid') #user_id\n    mid_input = Input((1,), name= 'input_mid')  #movie_id\n    cat_sl_inputs = [uid_input, mid_input]\n\n    # multi level categorical features (with 3 genres at most)\n    #genre_input = Input((3,), name = 'input_genre')\n    #cat_ml_inputs = [genre_input]\n\n    inputs = num_inputs + cat_sl_inputs #+ cat_ml_inputs\n    \n    return inputs\n\ninputs = define_input_layers()\ninputs","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:34:14.649313Z","iopub.execute_input":"2022-03-17T10:34:14.649806Z","iopub.status.idle":"2022-03-17T10:34:14.690667Z","shell.execute_reply.started":"2022-03-17T10:34:14.649768Z","shell.execute_reply":"2022-03-17T10:34:14.689953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (2) 1st order factorization machines","metadata":{}},{"cell_type":"code","source":"def Tensor_Mean_Pooling(name = 'mean_pooling', keepdims = False):\n    return Lambda(lambda x: K.mean(x, axis = 1, keepdims=keepdims), name = name)\n\ndef fm_1d(inputs, n_uid, n_mid):\n    \n    # user feat3 + user embedding + movie embedding + genre embedding\n    fea3_input, uid_input, mid_input = inputs\n    \n    # all tensors are reshape to (None, 1)\n    num_dense_1d = [Dense(1, name = 'num_dense_1d_fea4')(fea3_input)]\n    cat_sl_embed_1d = [Embedding(n_uid + 1, 1, name = 'cat_embed_1d_uid')(uid_input),\n                        Embedding(n_mid + 1, 1, name = 'cat_embed_1d_mid')(mid_input)]\n\n    cat_sl_embed_1d = [Reshape((1,))(i) for i in cat_sl_embed_1d]\n    \n    # add all tensors\n    y_fm_1d = Add(name = 'fm_1d_output')(num_dense_1d + cat_sl_embed_1d )\n    \n    return y_fm_1d\n\ny_1d = fm_1d(inputs, 10, 10)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:34:14.692909Z","iopub.execute_input":"2022-03-17T10:34:14.693476Z","iopub.status.idle":"2022-03-17T10:34:18.660399Z","shell.execute_reply.started":"2022-03-17T10:34:14.693435Z","shell.execute_reply":"2022-03-17T10:34:18.6596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (3) 2nd order factorization machines\n\nIn 2nd order FM, each feature is map to shape (None, 1, k) and then stack to concat_embed_2d layer with shape (None, p, k).\nk - matrix factorization latent dimension, p is feature dimension.\n\nthe calculation of interaction terms can be simplified, using\n\\begin{equation*} \\sum{x_ix_j} = \\frac{1}{2} \\left((\\sum{x})^2 - \\sum({x}^2)\\right) \\end{equation*}\n\nHence, the sum of 2nd order interactions = square of sum of concat_embed_2d - sum of squared concat_embed_2d in p dimension, the resulting tensor will have a shape (None, k)","metadata":{}},{"cell_type":"code","source":"def fm_2d(inputs, n_uid, n_mid, k):\n    \n    fea3_input, uid_input, mid_input = inputs\n    \n    num_dense_2d = [Dense(k, name = 'num_dense_2d_fea3')(fea3_input)] # shape (None, k)\n    num_dense_2d = [Reshape((1,k))(i) for i in num_dense_2d] # shape (None, 1, k)\n\n    cat_sl_embed_2d = [Embedding(n_uid + 1, k, name = 'cat_embed_2d_uid')(uid_input), \n                       Embedding(n_mid + 1, k, name = 'cat_embed_2d_mid')(mid_input)] # shape (None, 1, k)\n    \n   \n    # concatenate all 2d embed layers => (None, ?, k)\n    embed_2d = Concatenate(axis=1, name = 'concat_embed_2d')(num_dense_2d + cat_sl_embed_2d )\n\n    # calcuate the interactions by simplication\n    # sum of (x1*x2) = sum of (0.5*[(xi)^2 - (xi^2)])\n    tensor_sum = Lambda(lambda x: K.sum(x, axis = 1), name = 'sum_of_tensors')\n    tensor_square = Lambda(lambda x: K.square(x), name = 'square_of_tensors')\n\n    sum_of_embed = tensor_sum(embed_2d)\n    square_of_embed = tensor_square(embed_2d)\n\n    square_of_sum = Multiply()([sum_of_embed, sum_of_embed])\n    sum_of_square = tensor_sum(square_of_embed)\n\n    sub = Subtract()([square_of_sum, sum_of_square])\n    sub = Lambda(lambda x: x*0.5)(sub)\n    y_fm_2d = Reshape((1,), name = 'fm_2d_output')(tensor_sum(sub))\n    \n    return y_fm_2d, embed_2d\n\ny_fm2_d, embed_2d = fm_2d(inputs, 10, 10, 5)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:34:18.663372Z","iopub.execute_input":"2022-03-17T10:34:18.664017Z","iopub.status.idle":"2022-03-17T10:34:18.720098Z","shell.execute_reply.started":"2022-03-17T10:34:18.663969Z","shell.execute_reply":"2022-03-17T10:34:18.719344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (4) deep part","metadata":{}},{"cell_type":"code","source":"def deep_part(embed_2d, dnn_dim, dnn_dr):\n    \n    # flat embed layers from 3D to 2D tensors\n    y_dnn = Flatten(name = 'flat_embed_2d')(embed_2d)\n    for h in dnn_dim:\n        y_dnn = Dropout(dnn_dr)(y_dnn)\n        y_dnn = Dense(h, activation='relu')(y_dnn)\n    y_dnn = Dense(1, activation='linear', name = 'deep_output')(y_dnn)\n    \n    return y_dnn\n\ny_dnn = deep_part(embed_2d, [16, 16], 0.5)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:34:18.721466Z","iopub.execute_input":"2022-03-17T10:34:18.721733Z","iopub.status.idle":"2022-03-17T10:34:18.75627Z","shell.execute_reply.started":"2022-03-17T10:34:18.72169Z","shell.execute_reply":"2022-03-17T10:34:18.755499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (5) Put all together","metadata":{}},{"cell_type":"code","source":"def deep_fm_model(n_uid, n_mid, k, dnn_dim, dnn_dr):\n    \n    inputs = define_input_layers()\n    \n    y_fm_1d = fm_1d(inputs, n_uid, n_mid)\n    y_fm_2d, embed_2d = fm_2d(inputs, n_uid, n_mid, k)\n    y_dnn = deep_part(embed_2d, dnn_dim, dnn_dr)\n    \n    # combinded deep and fm parts\n    y = Concatenate()([y_fm_1d, y_fm_2d, y_dnn])\n    y = Dense(1, activation='sigmoid', name = 'deepfm_output')(y)\n    \n    fm_model_1d = Model(inputs, y_fm_1d)\n    fm_model_2d = Model(inputs, y_fm_2d)\n    deep_model = Model(inputs, y_dnn)\n    deep_fm_model = Model(inputs, y)\n    \n    return fm_model_1d, fm_model_2d, deep_model, deep_fm_model","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:34:18.757525Z","iopub.execute_input":"2022-03-17T10:34:18.757869Z","iopub.status.idle":"2022-03-17T10:34:18.765189Z","shell.execute_reply.started":"2022-03-17T10:34:18.757828Z","shell.execute_reply":"2022-03-17T10:34:18.764054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def df2xy(df):\n    x = [df.price.values, \n         df.uid.values, \n         df.iid.values]\n    y = df.output.values\n    return x,y\n\ntrain_x, train_y = df2xy(X_train)\nvalid_x, valid_y = df2xy(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:34:18.766851Z","iopub.execute_input":"2022-03-17T10:34:18.767401Z","iopub.status.idle":"2022-03-17T10:34:18.778176Z","shell.execute_reply.started":"2022-03-17T10:34:18.767361Z","shell.execute_reply":"2022-03-17T10:34:18.777347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    'n_uid': df_final.uid.max(),\n    'n_mid': df_final.iid.max(),\n    'k':20,\n    'dnn_dim':[32,32],\n    'dnn_dr': 0.1\n}\nprint(params)\nfm_model_1d, fm_model_2d, deep_model, deep_fm_model = deep_fm_model(**params)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:34:18.77965Z","iopub.execute_input":"2022-03-17T10:34:18.779938Z","iopub.status.idle":"2022-03-17T10:34:18.936742Z","shell.execute_reply.started":"2022-03-17T10:34:18.779901Z","shell.execute_reply":"2022-03-17T10:34:18.936009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import  EarlyStopping, ModelCheckpoint\n# train  model\ndeep_fm_model.compile(\n    loss=tf.keras.losses.BinaryCrossentropy(),       \n    metrics = ['accuracy'],\n    optimizer=keras.optimizers.Adam(learning_rate=0.01)\n)\nearly_stop = EarlyStopping(monitor='val_loss', patience=5)\nmodel_ckp = ModelCheckpoint(filepath='deepfm_weights.h5', \n                            monitor='val_loss',\n                            save_weights_only=True, \n                            save_best_only=True)\ncallbacks = [model_ckp,early_stop]\ntrain_history = deep_fm_model.fit(train_x, train_y, \n                                  epochs=30, batch_size=4096,\n                                  validation_data=(valid_x, valid_y),\n                                  #callbacks = callbacks\n                                 )","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:34:18.93809Z","iopub.execute_input":"2022-03-17T10:34:18.938519Z","iopub.status.idle":"2022-03-17T10:48:42.997615Z","shell.execute_reply.started":"2022-03-17T10:34:18.938482Z","shell.execute_reply":"2022-03-17T10:48:42.996744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_train_val_losses(history):\n    plt.plot(history.history[\"loss\"])\n    plt.plot(history.history[\"val_loss\"])\n    plt.title(\"model loss\")\n    plt.ylabel(\"loss\")\n    plt.xlabel(\"epoch\")\n    plt.legend([\"train\", \"test\"], loc=\"upper left\")\n    plt.axis([0,len(history.history[\"loss\"]),np.min(history.history[\"loss\"]),np.max(history.history[\"val_loss\"])])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:48:42.999548Z","iopub.execute_input":"2022-03-17T10:48:42.999816Z","iopub.status.idle":"2022-03-17T10:48:43.007262Z","shell.execute_reply.started":"2022-03-17T10:48:42.999785Z","shell.execute_reply":"2022-03-17T10:48:43.005232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_train_val_losses(train_history)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:58:17.983003Z","iopub.execute_input":"2022-03-17T10:58:17.983771Z","iopub.status.idle":"2022-03-17T10:58:18.171832Z","shell.execute_reply.started":"2022-03-17T10:58:17.98373Z","shell.execute_reply":"2022-03-17T10:58:18.171174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}