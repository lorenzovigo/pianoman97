{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate Inference\n",
    "\n",
    "In this notebook, you firstly will understand the steps of the algorithms of \n",
    "Forward Sampling, Likelihood-Weighted FS, and Gibbs Sampling (for Bayesian networks). We will use <b>pgmpy</b> for the construction of the model.\n",
    "\n",
    "First of all, we need to import the necessary functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.factors.discrete import TabularCPD, DiscreteFactor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our model\n",
    "\n",
    "We will use the classical (enlarged) students example, which has the following graph structure:\n",
    "\n",
    "<img src=\"images/students_bn.png\" style=\"width:200px\" />\n",
    "\n",
    "We need to codify the graph structure and the Conditional Probability Distribution families\n",
    "\n",
    "### Set the structure\n",
    "\n",
    "First of all, we need to specify that we are constructing a Bayesian network and the set of directed edges as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = ['C', 'D', 'I', 'G', 'S', 'L', 'J', 'H']\n",
    "G = BayesianModel([('D', 'G'), ('I', 'G'), ('I', 'S'), ('G', 'L'),\n",
    "                  ('C','D'), ('G','H'), ('J','H'), ('S','J'), ('L','J')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Conditional Probability Distribution families\n",
    "\n",
    "Once the structure has been defined, we codify the respective CPDs as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_cpd = TabularCPD('C', 2, [[0.2], [0.8]])\n",
    "d_cpd = TabularCPD('D', 2, [[0.3, 0.4], \n",
    "                            [0.7, 0.6]],\n",
    "                   evidence=['C'], evidence_card=[2])\n",
    "i_cpd = TabularCPD('I', 3, [[0.5], [0.3], [0.2]])\n",
    "g_cpd = TabularCPD('G', 3, [[0.1, 0.2, 0.1, 0.1, 0.2, 0.3],\n",
    "                            [0.1, 0.3, 0.3, 0.2, 0.2, 0.3],\n",
    "                            [0.8, 0.5, 0.6, 0.7, 0.6, 0.4]],\n",
    "                   evidence=['D', 'I'], evidence_card=[2, 3])\n",
    "s_cpd = TabularCPD('S', 2, [[0.1, 0.2, 0.7],\n",
    "                            [0.9, 0.8, 0.3]],\n",
    "                   evidence=['I'], evidence_card=[3])\n",
    "l_cpd = TabularCPD('L', 2, [[0.1, 0.4, 0.8],\n",
    "                            [0.9, 0.6, 0.2]],\n",
    "                   evidence=['G'], evidence_card=[3])\n",
    "j_cpd = TabularCPD('J', 2, [[0.1, 0.5, 0.4, 0.6],\n",
    "                            [0.9, 0.5, 0.6, 0.4]],\n",
    "                   evidence=['L', 'S'], evidence_card=[2, 2])\n",
    "h_cpd = TabularCPD('H', 3, [[0.7, 0.3, 0.5, 0.3, 0.2, 0.4],\n",
    "                            [0.1, 0.3, 0.4, 0.4, 0.6, 0.3],\n",
    "                            [0.2, 0.4, 0.1, 0.3, 0.2, 0.3]],\n",
    "                   evidence=['G', 'J'], evidence_card=[3, 2])\n",
    "\n",
    "G.add_cpds(c_cpd, d_cpd, i_cpd, g_cpd, s_cpd, l_cpd, j_cpd, h_cpd)\n",
    "print('Is the model right?',G.check_model())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Sampling\n",
    "\n",
    "Forward sampling takes advantage of an **ancestral ordering** of the Bayesian network to sample each CPD one-by-one. Following that order, once you need to sample any variable $X_i$, all their parents $\\mathbf{PA}_i$ will have been sampled before (i.e., $\\mathbf{pa}_i$ is known) and we just need to sample the corresponding distribution, $P(X_i|\\mathbf{pa}_i)$.\n",
    "\n",
    "Thus, the first step will be to obtain an ancestral ordering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the ancestral ordering of a BN model\n",
    "\"\"\"\n",
    "def ancestral_ordering(model):\n",
    "    nodes = model.nodes()\n",
    "    ancestors = {}\n",
    "    ordering = [] # the ordering will be a list\n",
    "    # go through all the nodes\n",
    "    for n in nodes:\n",
    "        n_ancestors = model._get_ancestors_of(n) # returns ancestors + node n\n",
    "        # if the node has no parent, we can introduce it directly into the ordering\n",
    "        # if the node does have parent(s), we save the set of parents\n",
    "        ############################\n",
    "        ###### YOUR CODE HERE ######\n",
    "        ############################\n",
    "    while len(ancestors) >= 1: # whereas there exist nodes to be introduced into the ordering\n",
    "        act_dict = ancestors.copy()\n",
    "        for n,ancs in act_dict.items():\n",
    "            # if all the parents of a node are already in the ordering, we can introduce it too\n",
    "            if len(ancs) == len(ancs.intersection(set(ordering))):\n",
    "                ordering.append(n)\n",
    "                del ancestors[n]\n",
    "                \n",
    "    return ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use this function to get an ancestral ordering for our model $G$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestral_ordering(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check it yourself!\n",
    "\n",
    "Following the ancestral ordering, we just need to sample distributions. Let's implement a function for this process of sampling a single distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_cpd(probs):\n",
    "    rval = np.random.random()\n",
    "    ############################\n",
    "    ###### YOUR CODE HERE ######\n",
    "    ############################\n",
    "    x = # index i where rval < accumulated sum of vector probs\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function easily returns a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sample_cpd([0.1,0.4,0.3,0.2]) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just need to carry out the process of sampling the CPD of each node, one at a time. Following the ancestral ordering, for each node, we select the correct distribution within the CPD to sample and get a sample using the previous function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_sampling(model,aord=None):\n",
    "    if aord is None:\n",
    "        aord = ancestral_ordering(model) # Find the ancestral ordering, if not given\n",
    "    sampled_vals = {}\n",
    "    for n in aord: # for each node (CPD), following the ancestral ord.\n",
    "        cpd = G.get_cpds(n).copy()\n",
    "        cpd_vars = cpd.variables\n",
    "        if len(cpd_vars) > 1: # has node 'n' any parent? If so, identify the correct distribution to sample\n",
    "            red_vals = []\n",
    "            for v in cpd_vars:\n",
    "                if v != n:\n",
    "                    red_vals.append((v,sampled_vals[v]))\n",
    "            cpd.reduce(red_vals)\n",
    "        probs = np.ravel(cpd.get_values())\n",
    "        sampled_vals[n] = sample_cpd(probs) # sample the CPD of node 'n'\n",
    "    return sampled_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    print(forward_sampling(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood-weighted Sampling\n",
    "\n",
    "As you know, Forward sampling has a few limitations when answering conditional probability queries (we know the value of a few variables). One might be tempted of just sampling, in the same way as before, the rest of variables. However, we need to account for the probability of observing those given values (evidence) together with the rest of sampled values. \n",
    "\n",
    "Thus, LW-sampling is similar to FS in the sense that it samples, one by one, and following an ancestral ordering, the CDPs of all the unobserved variables $X_i$. The difference now is that, alongside with sampling, we will calculate the probability of that evidence happening within the configuration sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_weighted_sampling(model, evidence={}, aord=None):\n",
    "    if aord is None:\n",
    "        aord = ancestral_ordering(model) # Find the ancestral ordering, if not given\n",
    "    wei = 1 # we accumulate here the weight (probability)\n",
    "    sampled_vals = evidence.copy()\n",
    "    for n in aord: # for each node (CPD), following the ancestral ord.\n",
    "        cpd = G.get_cpds(n).copy()\n",
    "        cpd_vars = cpd.variables\n",
    "        if len(cpd_vars) > 1: # has node 'n' any parent? If so, identify the correct distribution to sample\n",
    "            red_vals = []\n",
    "            for v in cpd_vars:\n",
    "                if v != n:\n",
    "                    red_vals.append((v,sampled_vals[v]))\n",
    "            cpd.reduce(red_vals)\n",
    "        probs = np.ravel(cpd.get_values())\n",
    "        # if the node 'n' is already observed (evidence), just update the weight of the sample\n",
    "        # otherwise, sample the CPD of node 'n'\n",
    "        ############################\n",
    "        ###### YOUR CODE HERE ######\n",
    "        ############################\n",
    "    return sampled_vals, wei\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    print(likelihood_weighted_sampling(G, {\"D\":0,\"I\":1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gibbs Sampling\n",
    "\n",
    "Gibbs sampling is based on the more general Markov Chain Monte Carlo (MCMC) approach. There are a set of state among which you transition given a probabilistic transition matrix. In the long term, you are guaranteed, if a few basic conditions are met, that the samples that you obtain (the states that you visit) could be considered as samples from a probability distribution of interest.\n",
    "\n",
    "In the specific case of Bayesian networks, you work with a complete sample (whole assignment of values to all the variables). At each step, you just remove the current value of a variable and sample it again. In this case, where all the variables are observed (only the one to be sampled is assumed to be unobserved), only the variables in the Markov Blanket need to be taken into account. In the case of BNs, we only need to consider the CDP of the variable to sample, as well as those where it appears as a parent.\n",
    "\n",
    "In this specific case, we code it so that we obtain a number of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gibbs Sampling from Bayesian networks\n",
    "\"\"\"\n",
    "def GibbsSampling(model, nsample=1):\n",
    "    initial_state = {}\n",
    "    for n in model.nodes(): # We start from a randomly chosen state\n",
    "        initial_state[n] = np.random.choice(model.get_cardinality(n),size=1)[0]\n",
    "    new_state = initial_state.copy()\n",
    "    \n",
    "    sample = []\n",
    "    count = 0\n",
    "    while count < nsample: # For each of the samples that we want...\n",
    "        for n,val in initial_state.items(): # for each of the nodes (order don't matter)\n",
    "            # we obtain the product of all the CPDs that consider the node 'n'\n",
    "            phi = model.get_cpds(n).to_factor().copy()\n",
    "            for cn in model.get_children(n):\n",
    "                phi.product(model.get_cpds(cn), inplace=True)\n",
    "\n",
    "            phi_vars = phi.variables\n",
    "            if len(phi_vars) > 1: # if there are several variables in the resulting factor\n",
    "                red_vals = []\n",
    "                for v in phi_vars:\n",
    "                    if v != n:\n",
    "                        red_vals.append((v,new_state[v]))\n",
    "                # identify the correct distribution to sample\n",
    "                ############################\n",
    "                ###### YOUR CODE HERE ######\n",
    "                ############################\n",
    "\n",
    "            phi.normalize() # (it is a factor, we need to normalize!)\n",
    "            new_state[n] = sample_cpd(np.ravel(phi.values)) # sample the distribution\n",
    "        \n",
    "        count+=1\n",
    "        initial_state = new_state.copy() # the sample that we just generated will be, next, the 'previous sample'\n",
    "        sample.append(new_state.copy())\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GibbsSampling(G,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGMPY native functions for sampling\n",
    "All these functions are already implemented in `pgmpy`.\n",
    "\n",
    "Let's have a look on how to use them. We can start again with Forward Sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.sampling import BayesianModelSampling\n",
    "\n",
    "inference = BayesianModelSampling(G)\n",
    "inference.forward_sample(size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm of likelihood weighted sampling is also in the same `BayesianModelSampling` class, and uses the object `State` to receive the evidence of the CPQ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.factors.discrete import State\n",
    "evidence = [State('D', 0), State('J', 1)]\n",
    "inference.likelihood_weighted_sample(evidence=evidence, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use also MCMC-based algorithms such as Gibbs Sampling. There is a class, `GibbsSampling`, for this method. We can use the functions two different functions to obtain a sample in different formats. Firstly,  `sample` returns it as a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.sampling import GibbsSampling\n",
    "gibbs_sampler = GibbsSampling(G)\n",
    "gibbs_sampler.sample(size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And `generate_sample` returns the sample as a genetator object, that includes a list of samples, each of which use the object `State` to denote the assignment of value to the variables of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = gibbs_sampler.generate_sample(size=10)\n",
    "[inst for inst in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
