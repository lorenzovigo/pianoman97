{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametric learning\n",
    "\n",
    "In this notebook you will understand the basic concepts of parametric learning of discrete Bayesian networks.\n",
    "\n",
    "Remember that a Bayesian network factorizes as the product of conditional probability distributions (CPDs), one for each variable. Our job is, then, to learn the CPT of each CPD.\n",
    "\n",
    "Let's start loading the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.sampling import BayesianModelSampling\n",
    "from pgmpy.utils import get_example_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with a basic BN with only two variables:\n",
    "$$P(X,Y)=P(X|Y)P(Y)$$\n",
    "Imagine that we are given the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'X': [\"l\", \"l\", \"l\", \"m\", \"l\", \"l\", \"l\", \"m\", \"l\", \"l\", \"m\", \"l\", \"l\", \"m\", \"m\"], \n",
    "                   'Y': [\"y\", \"n\", \"y\", \"y\", \"d\", \"y\", \"y\", \"d\", \"y\", \"d\", \"y\", \"y\", \"d\", \"n\", \"n\"] })\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint probability distribution $p(X,Y)$ can be estimated directly from this dataset as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"X\", \"Y\"]).size() / df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we are interested in the factorization, so let's learn the CPT of $P(Y)$ and $P(X|Y)$. The marginal probability distribution of $Y$ is just the proportion of examples with each value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pos_vals = np.unique(df[\"Y\"])\n",
    "print(\"For these possible values of variable Y:\",y_pos_vals)\n",
    "pY = np.array([#### YOUR CODE HERE ####\n",
    "               for val in y_pos_vals])/df.shape[0]\n",
    "print(pY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is basically the product of computing the following estimator:\n",
    "$$\\theta_{Y=y}=\\frac{n_y}{n}$$\n",
    "where $n_y$ is the number of cases where value $Y=y$ is observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the CPT of $X|Y$, we need to account for the distribution of the values of $X$ for each value of $Y$ separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pos_vals = np.unique(df[\"Y\"])\n",
    "x_pos_vals = np.unique(df[\"X\"])\n",
    "print(\"For these possible values of variable X:\",x_pos_vals)\n",
    "\n",
    "pXY =np.transpose(np.array([ [np.sum(np.logical_and(df[\"X\"] == val_x, df[\"Y\"] == val_y)) \n",
    "                              for val_x in x_pos_vals] / np.sum(df[\"Y\"] == val_y)\n",
    "                            for val_y in y_pos_vals]))\n",
    "print(pXY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is indeed the maximum-likelihood estimator of a categorical distribution, that is the type of distributions that we are dealing with in this case. This is basically the product of computing the following estimator:\n",
    "$$\\theta_{X=x|Y=y}=\\frac{n_{xy}}{n_y}$$\n",
    "where $n_{xy}$ is the number of cases where the combination of values $X=x$ and $Y=y$ is observed.\n",
    "\n",
    "You might have heard of the Laplace smoothing, a technique to prevent numerical problems produced by ML estimators in case of sparse data or rare events (which produce 0 probability values in the CPDs). This procedure consists of adding a constant uniform value to all the counts:\n",
    "\n",
    "$$\\theta_{X=x|Y=y}=\\frac{n_{xy}+l}{n_y+l\\cdot|\\mathcal{X}|}$$\n",
    "where $|\\mathcal{X}|$ is the number of possible values of variable $X$ (in our case, $|\\mathcal{X}|=2$), and \n",
    "$$\\theta_{Y=y}=\\frac{n_y+l}{n+l\\cdot|\\mathcal{Y}|}$$\n",
    "where  $|\\mathcal{Y}|=3$ is the number of possible values of $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=1 # Laplace smoothing value\n",
    "y_pos_vals = np.unique(df[\"Y\"])\n",
    "x_pos_vals = np.unique(df[\"X\"])\n",
    "print(\"For these possible values of variable Y:\",y_pos_vals)\n",
    "pY = np.array([#### YOUR CODE HERE ####\n",
    "               for val in y_pos_vals])/(df.shape[0]+l*len(y_pos_vals))\n",
    "print(\"p(Y) with Laplace smoothing (l=\"+str(l)+\"):\")\n",
    "print(pY)\n",
    "\n",
    "pXY =np.transpose(np.array([ [np.sum(np.logical_and(df[\"X\"] == val_x, df[\"Y\"] == val_y))+l\n",
    "                              for val_x in x_pos_vals] / (np.sum(df[\"Y\"] == val_y)+l*len(x_pos_vals))\n",
    "                            for val_y in y_pos_vals]))\n",
    "print(\"p(X|Y) with Laplace smoothing (l=\"+str(l)+\"):\")\n",
    "print(pXY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice trial! Now, we know how to compute MLE parameters from data. But what happens in more realistic scenarios?\n",
    "\n",
    "## Playing with a real example\n",
    "\n",
    "To do so, let's create a dataset from a real Bayesian network: <a href=\"https://www.bnlearn.com/bnrepository/discrete-small.html#survey\">survey</a>.\n",
    "\n",
    "<img src=\"https://www.bnlearn.com/bnrepository/survey/survey.png\" width=\"300\" />\n",
    "We will learn from the generated data and compare with the real parameters. We will use pgmpy to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model = get_example_model('survey')\n",
    "n_samples = 1000\n",
    "\n",
    "samples = BayesianModelSampling(gen_model).forward_sample(size=n_samples)\n",
    "samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are just dealing with parametric learning, so let's assume that we know the real structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model_struct = BayesianModel(ebunch=gen_model.edges())\n",
    "gen_model_struct.nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use standard pgmpy methods to learn the MLE parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "\n",
    "mle = MaximumLikelihoodEstimator(model=gen_model_struct, data=samples)\n",
    "\n",
    "print(gen_model.get_cpds('A'))\n",
    "print(mle.estimate_cpd(node='A'))\n",
    "print(gen_model.get_cpds('E'))\n",
    "print(mle.estimate_cpd(node='E'))\n",
    "\n",
    "mle.get_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This library `pgmpy` allows us to carry out all these operations at one if we call the learning function `fit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnt_model = BayesianModel(ebunch=gen_model.edges())\n",
    "learnt_model.fit(data=samples, estimator=MaximumLikelihoodEstimator)\n",
    "print(learnt_model.get_cpds('E'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the ability to recover the original parameters, we need to define a measure of error. In this case, we just measure the root mean square error between all the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_error(gen_model, learnt_model):\n",
    "    diff = []\n",
    "    for v in gen_model.nodes():\n",
    "        real_cpd=gen_model.get_cpds(v)\n",
    "        est_cpd=learnt_model.get_cpds(v)\n",
    "        act_states = est_cpd.state_names\n",
    "        for tpl in it.product(*act_states.values()):\n",
    "            diff.append(real_cpd.get_value(**dict(zip(act_states.keys(),tpl)))-\n",
    "                         est_cpd.get_value(**dict(zip(act_states.keys(),tpl))))\n",
    "    return np.sqrt(np.mean(np.array(diff)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can generate datasets of different size to understand how, as the sample size increases, the error is reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "errors=[]\n",
    "sample_sizes = np.arange(100,2001,100)\n",
    "for n_samples in sample_sizes:\n",
    "    samples = BayesianModelSampling(gen_model).forward_sample(size=n_samples)\n",
    "    learnt_model = BayesianModel(ebunch=gen_model.edges())\n",
    "    learnt_model.fit(data=samples, estimator=MaximumLikelihoodEstimator)\n",
    "    errors.append(estimate_error(#### YOUR CODE HERE ####))\n",
    "plt.plot(sample_sizes,errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Bayesian estimators\n",
    "\n",
    "All the things that we have seen so far can be done if we use a Bayesian estimator (such as BDeu) instead of MLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import BayesianEstimator\n",
    "\n",
    "n_samples = 1000\n",
    "samples = BayesianModelSampling(gen_model).forward_sample(size=n_samples)\n",
    "\n",
    "be = BayesianEstimator(model=gen_model_struct, data=samples)\n",
    "\n",
    "print(be.estimate_cpd(node='A', prior_type=\"BDeu\", equivalent_sample_size=1000))\n",
    "be.get_parameters(prior_type=\"BDeu\", equivalent_sample_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can call the `fit` function to directly learn with a Bayesian estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnt_model = BayesianModel(ebunch=gen_model.edges())\n",
    "learnt_model.fit(data=samples, estimator=BayesianEstimator, prior_type='BDeu', equivalent_sample_size=100)\n",
    "print(learnt_model.get_cpds('E'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
