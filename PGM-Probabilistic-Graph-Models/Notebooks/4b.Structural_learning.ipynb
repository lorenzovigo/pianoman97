{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structural learning\n",
    "\n",
    "In this notebook you will understand the basic concepts of structural learning of discrete Bayesian networks.\n",
    "\n",
    "Remember that a Bayesian network is based on a Directed Acyclic Graph (DAG). It is precisely this graph structure  what we want to learn from data in this case.\n",
    "\n",
    "Let's start loading the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.sampling import BayesianModelSampling\n",
    "from pgmpy.utils import get_example_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating data from a known BN\n",
    "\n",
    "Let's play with a real BN. Thus, we can test if we are able to recover the real structure.\n",
    "\n",
    "To do so, we will use again the <a href=\"https://www.bnlearn.com/bnrepository/discrete-small.html#survey\">survey</a> BN.\n",
    "\n",
    "<img src=\"https://www.bnlearn.com/bnrepository/survey/survey.png\" width=\"300\" />\n",
    "We will learn from the generated data and compare with the real structure. We will use pgmpy to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "gen_model = get_example_model('survey')\n",
    "nx.draw(gen_model, with_labels=True, arrowsize=30, node_size=800, alpha=0.3, font_weight='bold')\n",
    "plt.show()\n",
    "\n",
    "n_samples = 1000\n",
    "\n",
    "samples = BayesianModelSampling(gen_model).forward_sample(size=n_samples)\n",
    "samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual information\n",
    "To learn the structure we need to find dependencies between variables to know where introduce an edge.\n",
    "\n",
    "We can use, to this end, mutual information which, as we explained at class, is related to maximum likelihood estimators in its basic form. Let's calculate it as:\n",
    "$$MI(X,Y)=\\sum_{x_i,y_j} \\frac{N_{x_i,y_j}}{N} \\log \\frac{N\\cdot N_{x_i,y_j}}{N_{x_i}\\cdot N_{y_j}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "def mutual_information(x,y,smooth=0.1): \n",
    "    # first, we calculate the contingency table between both variables\n",
    "    contingency = contingency_matrix(x,y)+smooth # we use smoothing to avoid problems with 0 probabilities\n",
    "    # we obtain per-variable counts\n",
    "    N = np.sum(contingency)\n",
    "    Ni = contingency.sum(axis=1, keepdims=True)\n",
    "    Nj = #### YOUR CODE HERE ####\n",
    "\n",
    "    inner = np.log(contingency / Ni / Nj * N)\n",
    "    outer = contingency / N\n",
    "    return np.sum(inner*outer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate it, now, for the second and third variables as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_information(samples.values[:,1],samples.values[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditional mutual information is defined similarly, but with estimates of the conditional distributions on a third variable:\n",
    "$$CMI(X,Y;Z)=\\sum_{z_k} \\sum_{x_i,y_j} \\frac{N_{x_i,y_j,z_k}}{N} \\log \\frac{N_{z_k}\\cdot N_{x_i,y_j}}{N_{x_i,z_k}\\cdot N_{y_j,z_k}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cond_mutual_information(x,y,cond_z):\n",
    "    z_vals = np.unique(cond_z)\n",
    "    N = len(x)\n",
    "    res = 0\n",
    "    for i_z, z in enumerate(z_vals):\n",
    "        ind_zs = np.where(cond_z == z)[0]\n",
    "        contingency = contingency_matrix(x[ind_zs],y[ind_zs])\n",
    "        Nz = np.sum(contingency)\n",
    "        Niz = contingency.sum(axis=1, keepdims=True)\n",
    "        Njz = #### YOUR CODE HERE ####\n",
    "\n",
    "        inner = np.log(contingency / Niz / Njz * Nz)\n",
    "        outer = contingency / N\n",
    "        res += np.sum(inner*outer)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous case, if we condition on the fourth variable, we obtain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_mutual_information(samples.values[:,1],samples.values[:,2],samples.values[:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might need to calculate the mutual information between sets of variables. To do so, we transform each set of variables into a new single variable as the cartesian product of the original variables. Then, we can calculate MI as in the regular case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information_sets(x,y,cards_x,cards_y):\n",
    "    if x.shape[1] > 1:\n",
    "        cols=[]\n",
    "        for j in np.arange(x.shape[1]):\n",
    "            cols.append(np.unique(x[:,j], return_inverse=True)[1])\n",
    "        x = np.ravel_multi_index(cols, cards_x)\n",
    "    if y.shape[1] > 1:\n",
    "        cols=[]\n",
    "        for j in np.arange(y.shape[1]):\n",
    "            cols.append(np.unique(y[:,j], return_inverse=True)[1])\n",
    "        y = np.ravel_multi_index(cols, cards_y)\n",
    "    return mutual_information(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, to be able to calculate the cartesian product, we need to know the cardinality of the involved variables. This is how we calculate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the cardinalities\n",
    "cards = [len(np.unique(samples.values[:,j])) for j in np.arange(samples.shape[1])]\n",
    "# calculate MI(X_1,X_2 ; X_3,X_4)\n",
    "mutual_information_sets(samples.values[:100,:2].astype(str),\n",
    "                        samples.values[:100,2:4].astype(str),\n",
    "                        cards[:2],cards[2:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning a BN tree\n",
    "Now we have all we need to start learning a network. We can start by calculating the mutual information between all the pairs of variables, if that is feasible. We are going to order them by decreasing value too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = {}\n",
    "for i in np.arange(samples.shape[1]-1):\n",
    "    for j in np.arange(i+1,samples.shape[1]):\n",
    "        edges[(i,j)] = mutual_information(samples.values[:,i],samples.values[:,j])\n",
    "sorted_edges = dict(sorted(edges.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we create a network trying to reach all the nodes (so that none is disconnected) and avoiding loops, we could do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.Graph()\n",
    "G.add_nodes_from(np.arange(samples.values.shape[1]))\n",
    "\n",
    "n = samples.values.shape[1] - 1\n",
    "for k in sorted_edges:\n",
    "    if not nx.has_path(G,k[0],k[1]):\n",
    "        G.add_edge(k[0],k[1])\n",
    "        n-=1\n",
    "        if (n == 0):\n",
    "            break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we introduce only $n-1$ edges (where $n$ is the number of nodes). Try it yourself, it is the maximum number if we don't want cycles.\n",
    "\n",
    "Let's plot the graph that we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {k:v for k,v in zip(np.arange(samples.values.shape[1]),np.array(samples.columns,dtype=str))}\n",
    "Gp = nx.relabel_nodes(G, mapping)\n",
    "nx.draw_circular(Gp, with_labels=True, arrowsize=30, node_size=800, alpha=0.3, font_weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at it! It's a **tree**! You probably have already noticed that we are just applying <a href=\"https://en.wikipedia.org/wiki/Kruskal%27s_algorithm\">Kruskal's algorithm</a> for finding maximum spanning trees. \n",
    "\n",
    "The <a href=\"https://en.wikipedia.org/wiki/Chow%E2%80%93Liu_tree\">Chow-Liu algorithm</a> to learn a BN tree might be understood as using Kruskal's method and, then, directing all the edges following this procedure:\n",
    "\n",
    "- 1) Select a node as a root\n",
    "- 2) Direct all the edges related with it as out-edges\n",
    "- 3) Identify all the reached nodes, and follow the same procedure as in Step 2 for all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H=nx.DiGraph()\n",
    "H.add_nodes_from(np.arange(samples.values.shape[1]))\n",
    "\n",
    "root_node = 1 # we choose this, as a random choice\n",
    "# we use these lists to identify the set of currently processing nodes and the previously processed ones\n",
    "next_list = []\n",
    "act_list = [root_node]\n",
    "prev_list = []\n",
    "while len(act_list)>0:\n",
    "    for n in act_list:\n",
    "        nbs_n = nx.neighbors(G,n) # for each processing node, find all neighbors\n",
    "        for nei in nbs_n:\n",
    "            if nei not in prev_list: # for each neighbor, if it is not a previously processed node, add directed edge\n",
    "                H.add_edge(n,nei)\n",
    "                next_list.append(nei)\n",
    "        prev_list.append(n)\n",
    "    act_list = next_list\n",
    "    next_list=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can draw it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {k:v for k,v in zip(np.arange(samples.values.shape[1]),np.array(samples.columns,dtype=str))}\n",
    "Hp = nx.relabel_nodes(H, mapping)\n",
    "nx.draw_circular(Hp, with_labels=True, arrowsize=30, node_size=800, alpha=0.3, font_weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have our DAG structure!! We would only need to learn the parameters, after this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pgmpy for learning a tree\n",
    "We can use pgmpy to do all this. To use Chow-Liu's algorithm, we need to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import TreeSearch\n",
    "\n",
    "# let's just search for a tree. Remember that in this case, the root is not that important\n",
    "est = TreeSearch(samples, root_node=\"A\")\n",
    "# estimate the DAG using Chow-Liu's algorithm\n",
    "cl_dag = est.estimate(estimator_type=\"chow-liu\")\n",
    "\n",
    "nx.draw_circular(cl_dag, with_labels=True, arrowsize=30, node_size=800, alpha=0.3, font_weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can learn the parameters associated with this structure similarly as we did in the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "\n",
    "# we will stick to Maximum Likelihood (but you know about the existence of Bayesian estimators...)\n",
    "cl_model = BayesianModel(cl_dag.edges())\n",
    "cl_model.fit(data=samples, estimator=MaximumLikelihoodEstimator)\n",
    "cl_model.get_cpds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to score somehow the structure that we just obtained. Sticking to the same criteria, we can calculate the likelihood of the model given the sample. \n",
    "\n",
    "We need to calculate the probability of the samples (which, surprisingly, cannot be done in pgmpy!!) given that specific model.\n",
    "\n",
    "We are going to use log probabilities to avoid numerical issues!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_probability_of_samples(G, data):\n",
    "    res = np.zeros(data.shape[0])\n",
    "    lcpds = G.get_cpds()\n",
    "    for i in np.arange(len(res)):\n",
    "        smp_d = dict(data.iloc[i,:])\n",
    "        for cpd in lcpds:\n",
    "            evidence = { k: smp_d[k] for k in cpd.scope() }\n",
    "            prob = cpd.get_value(**evidence)\n",
    "            #### YOUR CODE HERE ####\n",
    "    return res\n",
    "\n",
    "def log_likelihood(G, data):\n",
    "    probs = log_probability_of_samples(G, data)\n",
    "    probs = probs[np.isfinite(probs)] # sanity check!\n",
    "    return np.sum(probs)\n",
    "\n",
    "def likelihood(G, data):\n",
    "    return np.exp(log_likelihood(G, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the likelihood of our model with that of the original one (the one we used to generate the data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_likelihood(gen_model, samples))\n",
    "print(log_likelihood(cl_model, samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe that the likelihood of the learnt structure is larger than that of the generative model!! Remember what we mention at class about the tendency of MLE to overfitting??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning a general DAG\n",
    "Now, we want to learn an unrestricted DAG (not a tree, as before). We will use a Hill-Climbing greedy (local search) method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that we need to implement, to take full advantage of the process, is a score that decomposes locally within nodes/CPDs. We use the likelihood, which reduces to the MI of each node (the main variable with the parent variables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_score(dag, act_score, data, vars_to_update):\n",
    "    new_score = copy.deepcopy(act_score)\n",
    "    for var in vars_to_update:\n",
    "        edges_from_par = dag.in_edges(var)\n",
    "        evid_var = [e for e,_ in edges_from_par]\n",
    "        if len(evid_var) > 0:\n",
    "            new_score[var] = mutual_information_sets(data[[var]].values.astype(str),\n",
    "                                                     data[evid_var].values.astype(str),\n",
    "                                                     [cards[var]],[cards[k] for k in evid_var])\n",
    "    return new_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we implement a few useful functions:\n",
    "- one that finds all the possible local changes (swap, removal, addition) for a given structure\n",
    "- another one that implements such a change into a given DAG\n",
    "- a function that calculates the score of a possible change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rev(arc):\n",
    "    return (arc[1],arc[0])\n",
    "\n",
    "def find_all_changes(dag, anc_ordering):\n",
    "    changes = list(dag.edges()) # we can remove these edges\n",
    "    \n",
    "    # we consider a new potential parent all the previous nodes in the ordering\n",
    "    for v in anc_ordering:\n",
    "        for prev_v in anc_ordering:\n",
    "            if prev_v == v:\n",
    "                continue;\n",
    "            if (prev_v, v) in dag.edges(): # if the edge is already in the DAG\n",
    "                continue;\n",
    "            if nx.has_path(dag, v, prev_v): # if the new edge would introduce a loop\n",
    "                continue;\n",
    "            changes.append((prev_v, v))\n",
    "    \n",
    "    for edge in dag.edges():\n",
    "        prov_dag = dag.copy()\n",
    "        prov_dag.remove_edge(edge[0], edge[1])\n",
    "        rev_edge = rev(edge)\n",
    "        if not nx.has_path(prov_dag,edge[0], edge[1]):\n",
    "            changes.append(rev(edge))\n",
    "    return changes\n",
    "\n",
    "def apply_change(dag, change):\n",
    "    if change in dag.edges():\n",
    "        # remove\n",
    "        dag.remove_edge(change[0], change[1])\n",
    "    elif rev(change) in dag.edges():\n",
    "        revedge = rev(change)\n",
    "        dag.remove_edge(revedge[0], revedge[1])\n",
    "        dag.add_edge(change[0],change[1])\n",
    "    else:\n",
    "        # add\n",
    "        dag.add_edge(change[0],change[1])\n",
    "\n",
    "def score_of_possible_change(dag, act_score, data, change):\n",
    "    prov_dag = dag.copy()\n",
    "    if change in dag.edges():\n",
    "        # remove\n",
    "        prov_dag.remove_edge(change[0], change[1])\n",
    "        return local_score(prov_dag, act_score, data, [change[1]])\n",
    "    elif rev(change) in dag.edges():\n",
    "        revedge = rev(change)\n",
    "        prov_dag.remove_edge(revedge[0], revedge[1])\n",
    "        prov_dag.add_edge(change[0],change[1])\n",
    "        return local_score(prov_dag, act_score, data, [change[0], change[1]])\n",
    "    else:\n",
    "        # add\n",
    "        prov_dag.add_edge(change[0],change[1])\n",
    "        return local_score(prov_dag, act_score, data, [change[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the algorithm is just a greedy method that selects, in each step, the best possible change, until no change can improve the current structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hill_climbing(dag, anc_ordering, data):\n",
    "    lh_increases = True\n",
    "    prev_score = 0\n",
    "    prev_dcmp_score = {c:0 for c in data.columns}\n",
    "    while lh_increases:\n",
    "        all_posible_changes = find_all_changes(dag, anc_ordering)\n",
    "        act_best_score = 0\n",
    "        act_dcmp_score = None\n",
    "        act_change = None\n",
    "        for change in all_posible_changes:\n",
    "            local_lh = score_of_possible_change(dag, prev_dcmp_score, data, change)\n",
    "            ch_sc = np.sum(list(local_lh.values())) #\n",
    "            print(change,ch_sc)\n",
    "            if #### YOUR CODE HERE ####: \n",
    "                act_best_score = ch_sc\n",
    "                act_dcmp_score = local_lh\n",
    "                act_change = change\n",
    "        if act_best_score > prev_score:\n",
    "            prev_score = act_best_score\n",
    "            prev_dcmp_score = act_dcmp_score\n",
    "            apply_change(dag, act_change)\n",
    "        else:\n",
    "            lh_increases = False\n",
    "    return dag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now all we need!! Let's try it. \n",
    "\n",
    "We start by creating an empty BN (no edges), which will be our initial point (it could be another one! E.g., the tree learnt with Chow-Liu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_bn = BayesianModel() \n",
    "initial_bn.add_nodes_from(samples.columns)\n",
    "initial_bn.fit(data=samples, estimator=MaximumLikelihoodEstimator)\n",
    "ordering = list(samples.columns) # In this algorithm, we really don't care about the ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to need the cardinality of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards = {c:len(np.unique(samples[c].values)) for c in samples.columns}\n",
    "cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we just run our local search algorithm and, for the obtained DAG, we learn the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_bn = hill_climbing(initial_bn, ordering, samples)\n",
    "final_bn.fit(data=samples, estimator=MaximumLikelihoodEstimator)\n",
    "final_bn.get_cpds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the likelihood of this model, and how does it look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_likelihood(final_bn, samples))\n",
    "nx.draw_circular(final_bn, with_labels=True, arrowsize=30, node_size=800, alpha=0.3, font_weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It resembles so much the complete graph, isn't it?? Remember that tendency of MLE to overfeating?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning a general DAG with pgmpy\n",
    "\n",
    "We now are going to do this using pgmpy methods but, let's take the opportunity to use a more robust score: <a href=\"https://en.wikipedia.org/wiki/Bayesian_information_criterion\">BIC</a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import BicScore,HillClimbSearch\n",
    "\n",
    "scoring_method = BicScore(data=samples)\n",
    "est = HillClimbSearch(data=samples)\n",
    "hc_dag = est.estimate(scoring_method=scoring_method, max_indegree=4, max_iter=int(1e4))\n",
    "\n",
    "# we will stick to Maximum Likelihood (but you know the existence of Bayesian estimators...)\n",
    "hc_model = BayesianModel(hc_dag.edges())\n",
    "hc_model.fit(data=samples, estimator=MaximumLikelihoodEstimator)\n",
    "hc_model.get_cpds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the likelihood of this model, and how does it look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_likelihood(hc_model, samples))\n",
    "nx.draw_circular(hc_model, with_labels=True, arrowsize=30, node_size=800, alpha=0.3, font_weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
