{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1W6fkIxZikF"
      },
      "source": [
        "# **The Problem: Automatic Apparent Age Estimation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9RcmMxfZikH"
      },
      "source": [
        "# Pre-requisites:\n",
        "Installing tensorflow-gpu (GPU) and OpenCv.\n",
        "Check GPU usage instructions [here](https://research.google.com/colaboratory/faq.html#gpu-availability)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH-lt981ZikH",
        "outputId": "3c002a13-c45a-4094-88d0-5d636e5134db"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-gpu==2.4.0\n",
        "!pip install opencv-python\n",
        "!pip install h5py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUQaSbl5ZikI"
      },
      "source": [
        "# Downloading and decompressing the Appa-Real Age Dataset [(source)](http://chalearnlap.cvc.uab.es/challenge/13/track/13/description/)\n",
        "- As default, RGB images (cropped faces) are in the range of [0, 255], and labels are in the range of ~0.9 to ~90 (years old).\n",
        "- The data is divided in train, validation and test set. \n",
        "- Matadata is also provided\n",
        "  - gender: male / female \n",
        "  - ethnicity: asian / afroamerican / caucasian\n",
        "  - facial expression: neutral / slightlyhappy / happy / oth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "02mUGbfEZikJ"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "tTI3ROwMZikJ",
        "outputId": "c32bee13-6120-4cb0-88d9-efb20f72e406"
      },
      "outputs": [],
      "source": [
        "# downloading the data\n",
        "!wget https://data.chalearnlap.cvc.uab.cat/Colab_2021/app_data.zip\n",
        "\n",
        "with ZipFile('app_data.zip','r') as zip:\n",
        "   zip.extractall()\n",
        "   print('Data decompressed successfully')\n",
        "\n",
        "# removing the .zip file after extraction to clean space\n",
        "!rm app_data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-DOYviUZikJ"
      },
      "source": [
        "# Loading the train/validation data, and re-scaling the labels to [0..1]\n",
        "Perform the split and the metadata extraction:\n",
        "- X_[train,valid,test] = Face images\n",
        "- Y_[train,valid,test] = Ground truth \n",
        "- M_[train,valid,test] = Metadata (gender, ethnicicy, facial expression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmLwc6JGZikK",
        "outputId": "159a91c7-cb27-4cd6-d27f-d2694decffb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data size and shape (4065, 224, 224, 3)\n",
            "Train labels size and shape (4065,)\n",
            "Train metadata size and shape (4065, 3)\n",
            "----\n",
            "Valid data size and shape (1482, 224, 224, 3)\n",
            "Valid labels size and shape (1482,)\n",
            "Valid metadata size and shape (1482, 3)\n",
            "----\n",
            "Test data size and shape (1978, 224, 224, 3)\n",
            "Test labels size and shape (1978,)\n",
            "Test metadata size and shape (1978, 3)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# loading the train data\n",
        "X_train = np.load('./data/data_train.npy')\n",
        "Y_train = np.load('./data/labels_train.npy')\n",
        "M_train = np.load('./data/meta_data_train.npy')\n",
        "\n",
        "# loading the validation data\n",
        "X_valid = np.load('./data/data_valid.npy')\n",
        "Y_valid = np.load('./data/labels_valid.npy')\n",
        "M_valid = np.load('./data/meta_data_valid.npy')\n",
        "\n",
        "# loading the test data\n",
        "X_test = np.load('./data/data_test.npy')\n",
        "Y_test = np.load('./data/labels_test.npy')\n",
        "M_test = np.load('./data/meta_data_test.npy')\n",
        "\n",
        "# train labels are real numbers, ranging from ~0.9 to ~89 (years old);\n",
        "# we will re-scale the labels to [0,1] by using a normalization factor of 100,\n",
        "# assuming there is no sample with age > 100.\n",
        "Y_train = Y_train/100\n",
        "Y_valid = Y_valid/100\n",
        "# Y_test = Y_test/100 # -> we don't normalize the test labels as we will evaluate \n",
        "                      # them using the raw data, i.e., the apparent age values\n",
        "\n",
        "print('Train data size and shape', X_train.shape)\n",
        "print('Train labels size and shape', Y_train.shape)\n",
        "print('Train metadata size and shape', M_train.shape)\n",
        "print('----')\n",
        "print('Valid data size and shape', X_valid.shape)\n",
        "print('Valid labels size and shape', Y_valid.shape)\n",
        "print('Valid metadata size and shape', M_valid.shape)\n",
        "print('----')\n",
        "print('Test data size and shape', X_test.shape)\n",
        "print('Test labels size and shape', Y_test.shape)\n",
        "print('Test metadata size and shape', M_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3cpR0doZikL"
      },
      "source": [
        "# Visualizations\n",
        "\n",
        "Can be checked in the original baseline notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRt5xr7eZikL"
      },
      "source": [
        "# Preprocessing the data (face images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_anQxeUKZikL"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "\n",
        "# train\n",
        "for i in range(0,X_train.shape[0]):\n",
        "  x = X_train[i,:,:,:]\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  X_train[i,] = preprocess_input(x)\n",
        "\n",
        "# validation\n",
        "for i in range(0,X_valid.shape[0]):\n",
        "  x = X_valid[i,:,:,:]\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  X_valid[i,] = preprocess_input(x)  \n",
        "\n",
        "# test\n",
        "for i in range(0,X_test.shape[0]):\n",
        "  x = X_test[i,:,:,:]\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  X_test[i,] = preprocess_input(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Skm4EM3JZikM"
      },
      "source": [
        "# Downloading the ResNet50 model pre-trained on Faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAGJLw45ZikM",
        "outputId": "fff18d06-1c85-469a-a52a-be93e84f100a"
      },
      "outputs": [],
      "source": [
        "# downloading the data\n",
        "!wget https://data.chalearnlap.cvc.uab.cat/Colab_2021/model.zip\n",
        "\n",
        "# decompressing the data\n",
        "with ZipFile('model.zip','r') as zip:\n",
        "   zip.extractall()\n",
        "   print('Model decompressed successfully')\n",
        "\n",
        "# removing the .zip file after extraction  to clean space\n",
        "!rm model.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMHGVNwlZikM"
      },
      "source": [
        "# Loading the pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrCzFx0oZikM",
        "outputId": "09de3411-baec-4a86-ef8a-ddfa395131a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "base_input (InputLayer)         [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1/7x7_s2 (Conv2D)           (None, 112, 112, 64) 9408        base_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1/7x7_s2/bn (BatchNormaliza (None, 112, 112, 64) 256         conv1/7x7_s2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 112, 112, 64) 0           conv1/7x7_s2/bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 55, 55, 64)   0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1_1x1_reduce (Conv2D)     (None, 55, 55, 64)   4096        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1_1x1_reduce/bn (BatchNor (None, 55, 55, 64)   256         conv2_1_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 55, 55, 64)   0           conv2_1_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1_3x3 (Conv2D)            (None, 55, 55, 64)   36864       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1_3x3/bn (BatchNormalizat (None, 55, 55, 64)   256         conv2_1_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 55, 55, 64)   0           conv2_1_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1_1x1_increase (Conv2D)   (None, 55, 55, 256)  16384       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1_1x1_proj (Conv2D)       (None, 55, 55, 256)  16384       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1_1x1_increase/bn (BatchN (None, 55, 55, 256)  1024        conv2_1_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1_1x1_proj/bn (BatchNorma (None, 55, 55, 256)  1024        conv2_1_1x1_proj[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 55, 55, 256)  0           conv2_1_1x1_increase/bn[0][0]    \n",
            "                                                                 conv2_1_1x1_proj/bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 55, 55, 256)  0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2_2_1x1_reduce (Conv2D)     (None, 55, 55, 64)   16384       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2_2_1x1_reduce/bn (BatchNor (None, 55, 55, 64)   256         conv2_2_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 55, 55, 64)   0           conv2_2_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2_2_3x3 (Conv2D)            (None, 55, 55, 64)   36864       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2_2_3x3/bn (BatchNormalizat (None, 55, 55, 64)   256         conv2_2_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 55, 55, 64)   0           conv2_2_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2_2_1x1_increase (Conv2D)   (None, 55, 55, 256)  16384       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2_2_1x1_increase/bn (BatchN (None, 55, 55, 256)  1024        conv2_2_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 55, 55, 256)  0           conv2_2_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 55, 55, 256)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2_3_1x1_reduce (Conv2D)     (None, 55, 55, 64)   16384       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2_3_1x1_reduce/bn (BatchNor (None, 55, 55, 64)   256         conv2_3_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 55, 55, 64)   0           conv2_3_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2_3_3x3 (Conv2D)            (None, 55, 55, 64)   36864       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2_3_3x3/bn (BatchNormalizat (None, 55, 55, 64)   256         conv2_3_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 55, 55, 64)   0           conv2_3_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2_3_1x1_increase (Conv2D)   (None, 55, 55, 256)  16384       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2_3_1x1_increase/bn (BatchN (None, 55, 55, 256)  1024        conv2_3_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 55, 55, 256)  0           conv2_3_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 55, 55, 256)  0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1_1x1_reduce (Conv2D)     (None, 28, 28, 128)  32768       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1_1x1_reduce/bn (BatchNor (None, 28, 28, 128)  512         conv3_1_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 28, 28, 128)  0           conv3_1_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1_3x3 (Conv2D)            (None, 28, 28, 128)  147456      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1_3x3/bn (BatchNormalizat (None, 28, 28, 128)  512         conv3_1_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 28, 28, 128)  0           conv3_1_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1_1x1_increase (Conv2D)   (None, 28, 28, 512)  65536       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1_1x1_proj (Conv2D)       (None, 28, 28, 512)  131072      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1_1x1_increase/bn (BatchN (None, 28, 28, 512)  2048        conv3_1_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1_1x1_proj/bn (BatchNorma (None, 28, 28, 512)  2048        conv3_1_1x1_proj[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 28, 28, 512)  0           conv3_1_1x1_increase/bn[0][0]    \n",
            "                                                                 conv3_1_1x1_proj/bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 28, 28, 512)  0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv3_2_1x1_reduce (Conv2D)     (None, 28, 28, 128)  65536       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_2_1x1_reduce/bn (BatchNor (None, 28, 28, 128)  512         conv3_2_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 28, 28, 128)  0           conv3_2_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv3_2_3x3 (Conv2D)            (None, 28, 28, 128)  147456      activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_2_3x3/bn (BatchNormalizat (None, 28, 28, 128)  512         conv3_2_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 28, 28, 128)  0           conv3_2_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv3_2_1x1_increase (Conv2D)   (None, 28, 28, 512)  65536       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_2_1x1_increase/bn (BatchN (None, 28, 28, 512)  2048        conv3_2_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 28, 28, 512)  0           conv3_2_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 28, 28, 512)  0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv3_3_1x1_reduce (Conv2D)     (None, 28, 28, 128)  65536       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_3_1x1_reduce/bn (BatchNor (None, 28, 28, 128)  512         conv3_3_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 28, 28, 128)  0           conv3_3_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv3_3_3x3 (Conv2D)            (None, 28, 28, 128)  147456      activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_3_3x3/bn (BatchNormalizat (None, 28, 28, 128)  512         conv3_3_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 28, 28, 128)  0           conv3_3_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv3_3_1x1_increase (Conv2D)   (None, 28, 28, 512)  65536       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_3_1x1_increase/bn (BatchN (None, 28, 28, 512)  2048        conv3_3_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 28, 28, 512)  0           conv3_3_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 28, 28, 512)  0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv3_4_1x1_reduce (Conv2D)     (None, 28, 28, 128)  65536       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_4_1x1_reduce/bn (BatchNor (None, 28, 28, 128)  512         conv3_4_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 28, 28, 128)  0           conv3_4_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv3_4_3x3 (Conv2D)            (None, 28, 28, 128)  147456      activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_4_3x3/bn (BatchNormalizat (None, 28, 28, 128)  512         conv3_4_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 28, 28, 128)  0           conv3_4_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv3_4_1x1_increase (Conv2D)   (None, 28, 28, 512)  65536       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_4_1x1_increase/bn (BatchN (None, 28, 28, 512)  2048        conv3_4_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 28, 28, 512)  0           conv3_4_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 28, 28, 512)  0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv4_1_1x1_reduce (Conv2D)     (None, 14, 14, 256)  131072      activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_1_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_1_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 14, 14, 256)  0           conv4_1_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv4_1_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_1_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_1_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 14, 14, 256)  0           conv4_1_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv4_1_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_1_1x1_proj (Conv2D)       (None, 14, 14, 1024) 524288      activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_1_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_1_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_1_1x1_proj/bn (BatchNorma (None, 14, 14, 1024) 4096        conv4_1_1x1_proj[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 14, 14, 1024) 0           conv4_1_1x1_increase/bn[0][0]    \n",
            "                                                                 conv4_1_1x1_proj/bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 14, 14, 1024) 0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv4_2_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_2_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_2_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 14, 14, 256)  0           conv4_2_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv4_2_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_2_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_2_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 14, 14, 256)  0           conv4_2_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv4_2_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_2_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_2_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 14, 14, 1024) 0           conv4_2_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 14, 14, 1024) 0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv4_3_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_3_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_3_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 14, 14, 256)  0           conv4_3_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv4_3_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_3_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_3_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 14, 14, 256)  0           conv4_3_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv4_3_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_3_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_3_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 14, 14, 1024) 0           conv4_3_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 14, 14, 1024) 0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_4_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_4_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_4_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 14, 14, 256)  0           conv4_4_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv4_4_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_4_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_4_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 14, 14, 256)  0           conv4_4_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv4_4_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_4_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_4_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 14, 14, 1024) 0           conv4_4_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 14, 14, 1024) 0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_5_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_5_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_5_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 14, 14, 256)  0           conv4_5_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv4_5_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_5_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_5_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 14, 14, 256)  0           conv4_5_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv4_5_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_5_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_5_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 14, 14, 1024) 0           conv4_5_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 14, 14, 1024) 0           add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_6_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_6_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_6_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 14, 14, 256)  0           conv4_6_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv4_6_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_6_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_6_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 14, 14, 256)  0           conv4_6_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv4_6_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_6_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_6_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 14, 14, 1024) 0           conv4_6_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 14, 14, 1024) 0           add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_1_1x1_reduce (Conv2D)     (None, 7, 7, 512)    524288      activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_1_1x1_reduce/bn (BatchNor (None, 7, 7, 512)    2048        conv5_1_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 7, 7, 512)    0           conv5_1_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv5_1_3x3 (Conv2D)            (None, 7, 7, 512)    2359296     activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_1_3x3/bn (BatchNormalizat (None, 7, 7, 512)    2048        conv5_1_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 7, 7, 512)    0           conv5_1_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv5_1_1x1_increase (Conv2D)   (None, 7, 7, 2048)   1048576     activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_1_1x1_proj (Conv2D)       (None, 7, 7, 2048)   2097152     activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_1_1x1_increase/bn (BatchN (None, 7, 7, 2048)   8192        conv5_1_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_1_1x1_proj/bn (BatchNorma (None, 7, 7, 2048)   8192        conv5_1_1x1_proj[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 7, 7, 2048)   0           conv5_1_1x1_increase/bn[0][0]    \n",
            "                                                                 conv5_1_1x1_proj/bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 7, 7, 2048)   0           add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_2_1x1_reduce (Conv2D)     (None, 7, 7, 512)    1048576     activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_2_1x1_reduce/bn (BatchNor (None, 7, 7, 512)    2048        conv5_2_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 7, 7, 512)    0           conv5_2_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv5_2_3x3 (Conv2D)            (None, 7, 7, 512)    2359296     activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_2_3x3/bn (BatchNormalizat (None, 7, 7, 512)    2048        conv5_2_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 7, 7, 512)    0           conv5_2_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv5_2_1x1_increase (Conv2D)   (None, 7, 7, 2048)   1048576     activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_2_1x1_increase/bn (BatchN (None, 7, 7, 2048)   8192        conv5_2_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 7, 7, 2048)   0           conv5_2_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 7, 7, 2048)   0           add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_3_1x1_reduce (Conv2D)     (None, 7, 7, 512)    1048576     activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_3_1x1_reduce/bn (BatchNor (None, 7, 7, 512)    2048        conv5_3_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 7, 7, 512)    0           conv5_3_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv5_3_3x3 (Conv2D)            (None, 7, 7, 512)    2359296     activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_3_3x3/bn (BatchNormalizat (None, 7, 7, 512)    2048        conv5_3_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 7, 7, 512)    0           conv5_3_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv5_3_1x1_increase (Conv2D)   (None, 7, 7, 2048)   1048576     activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_3_1x1_increase/bn (BatchN (None, 7, 7, 2048)   8192        conv5_3_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 7, 7, 2048)   0           conv5_3_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 7, 7, 2048)   0           add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool (AveragePooling2D)     (None, 1, 1, 2048)   0           activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 2048)         0           avg_pool[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dim_proj (Dense)                (None, 512)          1049088     flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "classifier_low_dim (Dense)      (None, 8631)         4419072     dim_proj[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 29,029,312\n",
            "Trainable params: 28,976,192\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# loading the pretrained model\n",
        "model = tf.keras.models.load_model('./model/weights.h5')\n",
        "\n",
        "# print the model summary\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk01xYa2ZikM"
      },
      "source": [
        "# Adapting the model to our needs\n",
        "- In summary, we will ignore the last layer 'classifier_low_dim' and will include a few other layers on top of our backbone. Here, we also define the activation function we are going to use as output of the last FC layer (Sigmoid, in the case)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czf0FlHXZikN",
        "outputId": "06500ab2-6ddf-44a0-95fa-955f4b89f3d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "base_input (InputLayer)         [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1/7x7_s2 (Conv2D)           (None, 112, 112, 64) 9408        base_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1/7x7_s2/bn (BatchNormaliza (None, 112, 112, 64) 256         conv1/7x7_s2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 112, 112, 64) 0           conv1/7x7_s2/bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 55, 55, 64)   0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1_1x1_reduce (Conv2D)     (None, 55, 55, 64)   4096        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1_1x1_reduce/bn (BatchNor (None, 55, 55, 64)   256         conv2_1_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 55, 55, 64)   0           conv2_1_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1_3x3 (Conv2D)            (None, 55, 55, 64)   36864       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1_3x3/bn (BatchNormalizat (None, 55, 55, 64)   256         conv2_1_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 55, 55, 64)   0           conv2_1_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1_1x1_increase (Conv2D)   (None, 55, 55, 256)  16384       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1_1x1_proj (Conv2D)       (None, 55, 55, 256)  16384       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1_1x1_increase/bn (BatchN (None, 55, 55, 256)  1024        conv2_1_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1_1x1_proj/bn (BatchNorma (None, 55, 55, 256)  1024        conv2_1_1x1_proj[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 55, 55, 256)  0           conv2_1_1x1_increase/bn[0][0]    \n",
            "                                                                 conv2_1_1x1_proj/bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 55, 55, 256)  0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2_2_1x1_reduce (Conv2D)     (None, 55, 55, 64)   16384       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2_2_1x1_reduce/bn (BatchNor (None, 55, 55, 64)   256         conv2_2_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 55, 55, 64)   0           conv2_2_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2_2_3x3 (Conv2D)            (None, 55, 55, 64)   36864       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2_2_3x3/bn (BatchNormalizat (None, 55, 55, 64)   256         conv2_2_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 55, 55, 64)   0           conv2_2_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2_2_1x1_increase (Conv2D)   (None, 55, 55, 256)  16384       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2_2_1x1_increase/bn (BatchN (None, 55, 55, 256)  1024        conv2_2_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 55, 55, 256)  0           conv2_2_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 55, 55, 256)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2_3_1x1_reduce (Conv2D)     (None, 55, 55, 64)   16384       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2_3_1x1_reduce/bn (BatchNor (None, 55, 55, 64)   256         conv2_3_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 55, 55, 64)   0           conv2_3_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2_3_3x3 (Conv2D)            (None, 55, 55, 64)   36864       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2_3_3x3/bn (BatchNormalizat (None, 55, 55, 64)   256         conv2_3_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 55, 55, 64)   0           conv2_3_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2_3_1x1_increase (Conv2D)   (None, 55, 55, 256)  16384       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2_3_1x1_increase/bn (BatchN (None, 55, 55, 256)  1024        conv2_3_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 55, 55, 256)  0           conv2_3_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 55, 55, 256)  0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1_1x1_reduce (Conv2D)     (None, 28, 28, 128)  32768       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1_1x1_reduce/bn (BatchNor (None, 28, 28, 128)  512         conv3_1_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 28, 28, 128)  0           conv3_1_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1_3x3 (Conv2D)            (None, 28, 28, 128)  147456      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1_3x3/bn (BatchNormalizat (None, 28, 28, 128)  512         conv3_1_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 28, 28, 128)  0           conv3_1_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1_1x1_increase (Conv2D)   (None, 28, 28, 512)  65536       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1_1x1_proj (Conv2D)       (None, 28, 28, 512)  131072      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1_1x1_increase/bn (BatchN (None, 28, 28, 512)  2048        conv3_1_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1_1x1_proj/bn (BatchNorma (None, 28, 28, 512)  2048        conv3_1_1x1_proj[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 28, 28, 512)  0           conv3_1_1x1_increase/bn[0][0]    \n",
            "                                                                 conv3_1_1x1_proj/bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 28, 28, 512)  0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv3_2_1x1_reduce (Conv2D)     (None, 28, 28, 128)  65536       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_2_1x1_reduce/bn (BatchNor (None, 28, 28, 128)  512         conv3_2_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 28, 28, 128)  0           conv3_2_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv3_2_3x3 (Conv2D)            (None, 28, 28, 128)  147456      activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_2_3x3/bn (BatchNormalizat (None, 28, 28, 128)  512         conv3_2_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 28, 28, 128)  0           conv3_2_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv3_2_1x1_increase (Conv2D)   (None, 28, 28, 512)  65536       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_2_1x1_increase/bn (BatchN (None, 28, 28, 512)  2048        conv3_2_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 28, 28, 512)  0           conv3_2_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 28, 28, 512)  0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv3_3_1x1_reduce (Conv2D)     (None, 28, 28, 128)  65536       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_3_1x1_reduce/bn (BatchNor (None, 28, 28, 128)  512         conv3_3_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 28, 28, 128)  0           conv3_3_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv3_3_3x3 (Conv2D)            (None, 28, 28, 128)  147456      activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_3_3x3/bn (BatchNormalizat (None, 28, 28, 128)  512         conv3_3_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 28, 28, 128)  0           conv3_3_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv3_3_1x1_increase (Conv2D)   (None, 28, 28, 512)  65536       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_3_1x1_increase/bn (BatchN (None, 28, 28, 512)  2048        conv3_3_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 28, 28, 512)  0           conv3_3_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 28, 28, 512)  0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv3_4_1x1_reduce (Conv2D)     (None, 28, 28, 128)  65536       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_4_1x1_reduce/bn (BatchNor (None, 28, 28, 128)  512         conv3_4_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 28, 28, 128)  0           conv3_4_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv3_4_3x3 (Conv2D)            (None, 28, 28, 128)  147456      activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_4_3x3/bn (BatchNormalizat (None, 28, 28, 128)  512         conv3_4_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 28, 28, 128)  0           conv3_4_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv3_4_1x1_increase (Conv2D)   (None, 28, 28, 512)  65536       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_4_1x1_increase/bn (BatchN (None, 28, 28, 512)  2048        conv3_4_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 28, 28, 512)  0           conv3_4_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 28, 28, 512)  0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv4_1_1x1_reduce (Conv2D)     (None, 14, 14, 256)  131072      activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_1_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_1_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 14, 14, 256)  0           conv4_1_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv4_1_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_1_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_1_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 14, 14, 256)  0           conv4_1_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv4_1_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_1_1x1_proj (Conv2D)       (None, 14, 14, 1024) 524288      activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_1_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_1_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_1_1x1_proj/bn (BatchNorma (None, 14, 14, 1024) 4096        conv4_1_1x1_proj[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 14, 14, 1024) 0           conv4_1_1x1_increase/bn[0][0]    \n",
            "                                                                 conv4_1_1x1_proj/bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 14, 14, 1024) 0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv4_2_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_2_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_2_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 14, 14, 256)  0           conv4_2_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv4_2_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_2_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_2_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 14, 14, 256)  0           conv4_2_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv4_2_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_2_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_2_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 14, 14, 1024) 0           conv4_2_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 14, 14, 1024) 0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv4_3_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_3_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_3_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 14, 14, 256)  0           conv4_3_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv4_3_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_3_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_3_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 14, 14, 256)  0           conv4_3_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv4_3_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_3_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_3_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 14, 14, 1024) 0           conv4_3_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 14, 14, 1024) 0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_4_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_4_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_4_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 14, 14, 256)  0           conv4_4_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv4_4_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_4_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_4_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 14, 14, 256)  0           conv4_4_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv4_4_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_4_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_4_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 14, 14, 1024) 0           conv4_4_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 14, 14, 1024) 0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_5_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_5_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_5_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 14, 14, 256)  0           conv4_5_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv4_5_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_5_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_5_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 14, 14, 256)  0           conv4_5_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv4_5_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_5_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_5_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 14, 14, 1024) 0           conv4_5_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 14, 14, 1024) 0           add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_6_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_6_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_6_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 14, 14, 256)  0           conv4_6_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv4_6_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_6_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_6_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 14, 14, 256)  0           conv4_6_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv4_6_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_6_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_6_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 14, 14, 1024) 0           conv4_6_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 14, 14, 1024) 0           add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_1_1x1_reduce (Conv2D)     (None, 7, 7, 512)    524288      activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_1_1x1_reduce/bn (BatchNor (None, 7, 7, 512)    2048        conv5_1_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 7, 7, 512)    0           conv5_1_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv5_1_3x3 (Conv2D)            (None, 7, 7, 512)    2359296     activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_1_3x3/bn (BatchNormalizat (None, 7, 7, 512)    2048        conv5_1_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 7, 7, 512)    0           conv5_1_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv5_1_1x1_increase (Conv2D)   (None, 7, 7, 2048)   1048576     activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_1_1x1_proj (Conv2D)       (None, 7, 7, 2048)   2097152     activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_1_1x1_increase/bn (BatchN (None, 7, 7, 2048)   8192        conv5_1_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_1_1x1_proj/bn (BatchNorma (None, 7, 7, 2048)   8192        conv5_1_1x1_proj[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 7, 7, 2048)   0           conv5_1_1x1_increase/bn[0][0]    \n",
            "                                                                 conv5_1_1x1_proj/bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 7, 7, 2048)   0           add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_2_1x1_reduce (Conv2D)     (None, 7, 7, 512)    1048576     activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_2_1x1_reduce/bn (BatchNor (None, 7, 7, 512)    2048        conv5_2_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 7, 7, 512)    0           conv5_2_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv5_2_3x3 (Conv2D)            (None, 7, 7, 512)    2359296     activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_2_3x3/bn (BatchNormalizat (None, 7, 7, 512)    2048        conv5_2_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 7, 7, 512)    0           conv5_2_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv5_2_1x1_increase (Conv2D)   (None, 7, 7, 2048)   1048576     activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_2_1x1_increase/bn (BatchN (None, 7, 7, 2048)   8192        conv5_2_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 7, 7, 2048)   0           conv5_2_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 7, 7, 2048)   0           add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_3_1x1_reduce (Conv2D)     (None, 7, 7, 512)    1048576     activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_3_1x1_reduce/bn (BatchNor (None, 7, 7, 512)    2048        conv5_3_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 7, 7, 512)    0           conv5_3_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv5_3_3x3 (Conv2D)            (None, 7, 7, 512)    2359296     activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_3_3x3/bn (BatchNormalizat (None, 7, 7, 512)    2048        conv5_3_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 7, 7, 512)    0           conv5_3_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv5_3_1x1_increase (Conv2D)   (None, 7, 7, 2048)   1048576     activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_3_1x1_increase/bn (BatchN (None, 7, 7, 2048)   8192        conv5_3_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 7, 7, 2048)   0           conv5_3_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 7, 7, 2048)   0           add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool (AveragePooling2D)     (None, 1, 1, 2048)   0           activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 2048)         0           avg_pool[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dim_proj (Dense)                (None, 512)          1049088     flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 512)          0           dim_proj[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "f_128 (Dense)                   (None, 128)          65664       dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "f_32 (Dense)                    (None, 32)           4128        f_128[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 32)           0           f_32[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "predict (Dense)                 (None, 1)            33          dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 24,680,065\n",
            "Trainable params: 24,626,945\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Using the FC layer before the 'classifier_low_dim' layer as feature vector\n",
        "fc_512 = model.get_layer('dim_proj').output\n",
        "\n",
        "# adding a dropout layer to minimize overfiting problems\n",
        "dp_layer_1 = Dropout(0.5)(fc_512)\n",
        "\n",
        "# adding a few hidden FC layers to learn hidden representations\n",
        "fc_128 = Dense(128, activation='relu', name='f_128')(dp_layer_1)\n",
        "fc_32 = Dense(32, activation='relu', name='f_32')(fc_128)\n",
        "\n",
        "# adding a dropout layer to minimize overfiting problems\n",
        "dp_layer_2 = Dropout(0.5)(fc_32)\n",
        "\n",
        "# Includint an additional FC layer with sigmoid activation, used to regress\n",
        "# the apparent age\n",
        "output = Dense(1, activation='sigmoid', name='predict')(dp_layer_2)\n",
        "\n",
        "# building and pringing the final model\n",
        "model = Model(inputs=model.get_layer('base_input').output,outputs=output)\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1bz0DtwZikN"
      },
      "source": [
        "# Freezing the first layers to allow the fine-tuning of the last FC layers (only)\n",
        "- Next, we set some layer to be trainable or not, and print if layers are set to trainable = True or False.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hy9Cdb9DZikN",
        "outputId": "c3f8d43d-b05c-4943-87a3-94a2fac136ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 base_input False\n",
            "1 conv1/7x7_s2 False\n",
            "2 conv1/7x7_s2/bn False\n",
            "3 activation_1 False\n",
            "4 max_pooling2d_1 False\n",
            "5 conv2_1_1x1_reduce False\n",
            "6 conv2_1_1x1_reduce/bn False\n",
            "7 activation_2 False\n",
            "8 conv2_1_3x3 False\n",
            "9 conv2_1_3x3/bn False\n",
            "10 activation_3 False\n",
            "11 conv2_1_1x1_increase False\n",
            "12 conv2_1_1x1_proj False\n",
            "13 conv2_1_1x1_increase/bn False\n",
            "14 conv2_1_1x1_proj/bn False\n",
            "15 add_1 False\n",
            "16 activation_4 False\n",
            "17 conv2_2_1x1_reduce False\n",
            "18 conv2_2_1x1_reduce/bn False\n",
            "19 activation_5 False\n",
            "20 conv2_2_3x3 False\n",
            "21 conv2_2_3x3/bn False\n",
            "22 activation_6 False\n",
            "23 conv2_2_1x1_increase False\n",
            "24 conv2_2_1x1_increase/bn False\n",
            "25 add_2 False\n",
            "26 activation_7 False\n",
            "27 conv2_3_1x1_reduce False\n",
            "28 conv2_3_1x1_reduce/bn False\n",
            "29 activation_8 False\n",
            "30 conv2_3_3x3 False\n",
            "31 conv2_3_3x3/bn False\n",
            "32 activation_9 False\n",
            "33 conv2_3_1x1_increase False\n",
            "34 conv2_3_1x1_increase/bn False\n",
            "35 add_3 False\n",
            "36 activation_10 False\n",
            "37 conv3_1_1x1_reduce False\n",
            "38 conv3_1_1x1_reduce/bn False\n",
            "39 activation_11 False\n",
            "40 conv3_1_3x3 False\n",
            "41 conv3_1_3x3/bn False\n",
            "42 activation_12 False\n",
            "43 conv3_1_1x1_increase False\n",
            "44 conv3_1_1x1_proj False\n",
            "45 conv3_1_1x1_increase/bn False\n",
            "46 conv3_1_1x1_proj/bn False\n",
            "47 add_4 False\n",
            "48 activation_13 False\n",
            "49 conv3_2_1x1_reduce False\n",
            "50 conv3_2_1x1_reduce/bn False\n",
            "51 activation_14 False\n",
            "52 conv3_2_3x3 False\n",
            "53 conv3_2_3x3/bn False\n",
            "54 activation_15 False\n",
            "55 conv3_2_1x1_increase False\n",
            "56 conv3_2_1x1_increase/bn False\n",
            "57 add_5 False\n",
            "58 activation_16 False\n",
            "59 conv3_3_1x1_reduce False\n",
            "60 conv3_3_1x1_reduce/bn False\n",
            "61 activation_17 False\n",
            "62 conv3_3_3x3 False\n",
            "63 conv3_3_3x3/bn False\n",
            "64 activation_18 False\n",
            "65 conv3_3_1x1_increase False\n",
            "66 conv3_3_1x1_increase/bn False\n",
            "67 add_6 False\n",
            "68 activation_19 False\n",
            "69 conv3_4_1x1_reduce False\n",
            "70 conv3_4_1x1_reduce/bn False\n",
            "71 activation_20 False\n",
            "72 conv3_4_3x3 False\n",
            "73 conv3_4_3x3/bn False\n",
            "74 activation_21 False\n",
            "75 conv3_4_1x1_increase False\n",
            "76 conv3_4_1x1_increase/bn False\n",
            "77 add_7 False\n",
            "78 activation_22 False\n",
            "79 conv4_1_1x1_reduce False\n",
            "80 conv4_1_1x1_reduce/bn False\n",
            "81 activation_23 False\n",
            "82 conv4_1_3x3 False\n",
            "83 conv4_1_3x3/bn False\n",
            "84 activation_24 False\n",
            "85 conv4_1_1x1_increase False\n",
            "86 conv4_1_1x1_proj False\n",
            "87 conv4_1_1x1_increase/bn False\n",
            "88 conv4_1_1x1_proj/bn False\n",
            "89 add_8 False\n",
            "90 activation_25 False\n",
            "91 conv4_2_1x1_reduce False\n",
            "92 conv4_2_1x1_reduce/bn False\n",
            "93 activation_26 False\n",
            "94 conv4_2_3x3 False\n",
            "95 conv4_2_3x3/bn False\n",
            "96 activation_27 False\n",
            "97 conv4_2_1x1_increase False\n",
            "98 conv4_2_1x1_increase/bn False\n",
            "99 add_9 False\n",
            "100 activation_28 False\n",
            "101 conv4_3_1x1_reduce False\n",
            "102 conv4_3_1x1_reduce/bn False\n",
            "103 activation_29 False\n",
            "104 conv4_3_3x3 False\n",
            "105 conv4_3_3x3/bn False\n",
            "106 activation_30 False\n",
            "107 conv4_3_1x1_increase False\n",
            "108 conv4_3_1x1_increase/bn False\n",
            "109 add_10 False\n",
            "110 activation_31 False\n",
            "111 conv4_4_1x1_reduce False\n",
            "112 conv4_4_1x1_reduce/bn False\n",
            "113 activation_32 False\n",
            "114 conv4_4_3x3 False\n",
            "115 conv4_4_3x3/bn False\n",
            "116 activation_33 False\n",
            "117 conv4_4_1x1_increase False\n",
            "118 conv4_4_1x1_increase/bn False\n",
            "119 add_11 False\n",
            "120 activation_34 False\n",
            "121 conv4_5_1x1_reduce False\n",
            "122 conv4_5_1x1_reduce/bn False\n",
            "123 activation_35 False\n",
            "124 conv4_5_3x3 False\n",
            "125 conv4_5_3x3/bn False\n",
            "126 activation_36 False\n",
            "127 conv4_5_1x1_increase False\n",
            "128 conv4_5_1x1_increase/bn False\n",
            "129 add_12 False\n",
            "130 activation_37 False\n",
            "131 conv4_6_1x1_reduce False\n",
            "132 conv4_6_1x1_reduce/bn False\n",
            "133 activation_38 False\n",
            "134 conv4_6_3x3 False\n",
            "135 conv4_6_3x3/bn False\n",
            "136 activation_39 False\n",
            "137 conv4_6_1x1_increase False\n",
            "138 conv4_6_1x1_increase/bn False\n",
            "139 add_13 False\n",
            "140 activation_40 False\n",
            "141 conv5_1_1x1_reduce False\n",
            "142 conv5_1_1x1_reduce/bn False\n",
            "143 activation_41 False\n",
            "144 conv5_1_3x3 False\n",
            "145 conv5_1_3x3/bn False\n",
            "146 activation_42 False\n",
            "147 conv5_1_1x1_increase False\n",
            "148 conv5_1_1x1_proj False\n",
            "149 conv5_1_1x1_increase/bn False\n",
            "150 conv5_1_1x1_proj/bn False\n",
            "151 add_14 False\n",
            "152 activation_43 False\n",
            "153 conv5_2_1x1_reduce False\n",
            "154 conv5_2_1x1_reduce/bn False\n",
            "155 activation_44 False\n",
            "156 conv5_2_3x3 False\n",
            "157 conv5_2_3x3/bn False\n",
            "158 activation_45 False\n",
            "159 conv5_2_1x1_increase False\n",
            "160 conv5_2_1x1_increase/bn False\n",
            "161 add_15 False\n",
            "162 activation_46 False\n",
            "163 conv5_3_1x1_reduce False\n",
            "164 conv5_3_1x1_reduce/bn False\n",
            "165 activation_47 False\n",
            "166 conv5_3_3x3 False\n",
            "167 conv5_3_3x3/bn False\n",
            "168 activation_48 False\n",
            "169 conv5_3_1x1_increase False\n",
            "170 conv5_3_1x1_increase/bn False\n",
            "171 add_16 False\n",
            "172 activation_49 False\n",
            "173 avg_pool False\n",
            "174 flatten_1 False\n",
            "175 dim_proj True\n",
            "176 dropout True\n",
            "177 f_128 True\n",
            "178 f_32 True\n",
            "179 dropout_1 True\n",
            "180 predict True\n"
          ]
        }
      ],
      "source": [
        "counter = 0\n",
        "for layer in model.layers:\n",
        "  if counter <= 174: \n",
        "    layer.trainable = False\n",
        "  else:\n",
        "    layer.trainable = True\n",
        "  print(counter, layer.name, layer.trainable)\n",
        "  counter +=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwHpRmaXZikN"
      },
      "source": [
        "# Analyze bias functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdCmnWoLZikN"
      },
      "source": [
        "---\n",
        "# Age Bias ($B_a$) \n",
        "\n",
        "- Evaluates (on the TEST set) how accurate the model is with respect to different age ranges.\n",
        "  - group 1: age < 20\n",
        "  - group 2: 20 <= age < 40\n",
        "  - group 3: 40 <= age < 60\n",
        "  - group 4: 60 <= age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XG3As5ZzZikO"
      },
      "outputs": [],
      "source": [
        "def age_bias(predictions, gt):\n",
        "  error_g1 = []\n",
        "  error_g2 = []\n",
        "  error_g3 = []\n",
        "  error_g4 = []\n",
        "  for i in range(0,len(gt)):\n",
        "    if(gt[i]<20):\n",
        "      error_g1.append(abs(predictions[i]-gt[i]))\n",
        "    if(gt[i]>=20 and gt[i]<40):\n",
        "      error_g2.append(abs(predictions[i]-gt[i]))\n",
        "    if(gt[i]>=40 and gt[i]<60):\n",
        "      error_g3.append(abs(predictions[i]-gt[i]))\n",
        "    if(gt[i]>=60):\n",
        "      error_g4.append(abs(predictions[i]-gt[i]))\n",
        "\n",
        "  print('=============================')\n",
        "  print('Age analysis:')\n",
        "  print('Size group 1 = %d, MAE = %f' %(len(error_g1), np.mean(error_g1)))\n",
        "  print('Size group 2 = %d, MAE = %f' %(len(error_g2), np.mean(error_g2)))\n",
        "  print('Size group 3 = %d, MAE = %f' %(len(error_g3), np.mean(error_g3)))\n",
        "  print('Size group 4 = %d, MAE = %f' %(len(error_g4), np.mean(error_g4)))\n",
        "\n",
        "  age_bias = (abs(np.mean(error_g1)-np.mean(error_g2)) +\n",
        "            abs(np.mean(error_g1)-np.mean(error_g3)) +\n",
        "            abs(np.mean(error_g1)-np.mean(error_g4)) +\n",
        "            abs(np.mean(error_g2)-np.mean(error_g3)) +\n",
        "            abs(np.mean(error_g2)-np.mean(error_g4)) +\n",
        "            abs(np.mean(error_g3)-np.mean(error_g4)))/6\n",
        "\n",
        "  print('---------')\n",
        "  print('Age bias (Ba) = ', age_bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHhn0zKYZikO"
      },
      "source": [
        "# Gender Bias ($B_g$) \n",
        "- Evaluates (on the test set) how accurate the model is with respect to different gender.\n",
        "  - group 1: male\n",
        "  - group 2: female"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zyUFwbhLZikO"
      },
      "outputs": [],
      "source": [
        "def gender_bias(predictions, gt, metadata):\n",
        "  error_m = []\n",
        "  error_f = []\n",
        "  for i in range(0,len(gt)):\n",
        "    if(metadata[i][0] == 'female'):\n",
        "      error_f.append(abs(predictions[i]-gt[i]))\n",
        "    else:\n",
        "      error_m.append(abs(predictions[i]-gt[i]))\n",
        "\n",
        "  print('=============================')\n",
        "  print('Gender analysis:')\n",
        "  print('Size group female = %d, MAE = %f' %(len(error_f), np.mean(error_f)))\n",
        "  print('Size group male = %d, MAE = %f' %(len(error_m), np.mean(error_m)))\n",
        "\n",
        "  gender_bias = abs(np.mean(error_f)-np.mean(error_m))\n",
        "\n",
        "  print('---------')\n",
        "  print('Gender bias (Bg) = ', gender_bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCyA92BjZikO"
      },
      "source": [
        "# Ethnicity Bias ($B_e$)\n",
        "- Evaluates (on the test set) how accurate the model is with respect to different ethnicity categories.\n",
        "  - group 1: asian\n",
        "  - group 2: afroamerican\n",
        "  - group 3: caucasian\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "W2lGrA0pZikO"
      },
      "outputs": [],
      "source": [
        "def ethnicity_bias(predictions, gt, metadata):\n",
        "  error_as = []\n",
        "  error_af = []\n",
        "  error_ca = []\n",
        "  for i in range(0,len(gt)):\n",
        "    if(metadata[i][1] == 'asian'):\n",
        "      error_as.append(abs(predictions[i]-gt[i]))\n",
        "    if(metadata[i][1] == 'afroamerican'):\n",
        "      error_af.append(abs(predictions[i]-gt[i]))\n",
        "    if(metadata[i][1] == 'caucasian'):\n",
        "      error_ca.append(abs(predictions[i]-gt[i]))\n",
        "\n",
        "  print('=============================')\n",
        "  print('Ethnicity Analysis:')\n",
        "  print('Size group asian = %d, MAE = %f' %(len(error_as), np.mean(error_as)))\n",
        "  print('Size group afroamerican = %d, MAE = %f' %(len(error_af), np.mean(error_af)))\n",
        "  print('Size group caucasian = %d, MAE = %f' %(len(error_ca), np.mean(error_ca)))\n",
        "  \n",
        "  ethnicity_bias = (abs(np.mean(error_as)-np.mean(error_af)) +\n",
        "                   abs(np.mean(error_as)-np.mean(error_ca)) +\n",
        "                   abs(np.mean(error_af)-np.mean(error_ca)))/3\n",
        "\n",
        "  print('---------')\n",
        "  print('Ethnicity bias (Be) = ', ethnicity_bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIOT9uo6ZikO"
      },
      "source": [
        "# Face expression bias ($B_f$)\n",
        "- Evaluates (on the test set) how accurate the model is with respect to different face expression categories.\n",
        "  - group 1: neutral\n",
        "  - group 2: slightlyhappy\n",
        "  - group 3: happy\n",
        "  - group 4: other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cBJ48WrNZikO"
      },
      "outputs": [],
      "source": [
        "def face_expression_bias(predictions, gt, metadata):\n",
        "  error_h = []\n",
        "  error_s = []\n",
        "  error_n = []\n",
        "  error_o = []\n",
        "  for i in range(0,len(gt)):\n",
        "    if(metadata[i][2]=='happy'):\n",
        "      error_h.append(abs(predictions[i]-gt[i]))\n",
        "    if(metadata[i][2]=='slightlyhappy'):\n",
        "      error_s.append(abs(predictions[i]-gt[i]))\n",
        "    if(metadata[i][2]=='neutral'):\n",
        "      error_n.append(abs(predictions[i]-gt[i]))\n",
        "    if(metadata[i][2]=='other'):\n",
        "      error_o.append(abs(predictions[i]-gt[i]))\n",
        "\n",
        "  print('=============================')\n",
        "  print('Face experession Analysis:')\n",
        "  print('Size group happy = %d, MAE = %f' %(len(error_h), np.mean(error_h)))\n",
        "  print('Size group slightlyhappy = %d, MAE = %f' %(len(error_s), np.mean(error_s)))\n",
        "  print('Size group neutral = %d, MAE = %f' %(len(error_n), np.mean(error_n)))\n",
        "  print('Size group other = %d, MAE = %f' %(len(error_o), np.mean(error_o)))\n",
        "\n",
        "  face_bias = (abs(np.mean(error_h)-np.mean(error_s)) +\n",
        "              abs(np.mean(error_h)-np.mean(error_n)) +\n",
        "              abs(np.mean(error_h)-np.mean(error_o)) +\n",
        "              abs(np.mean(error_s)-np.mean(error_n)) +\n",
        "              abs(np.mean(error_s)-np.mean(error_o)) +\n",
        "              abs(np.mean(error_n)-np.mean(error_o)))/6\n",
        "\n",
        "  print('---------')\n",
        "  print('Face Expression bias (Bf) = ', face_bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWH5-W1RZikP"
      },
      "source": [
        "# Data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaCU9KqBZikP"
      },
      "source": [
        "First, let's check the number of samples per each age group of the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6TT4aukAZikP"
      },
      "outputs": [],
      "source": [
        "num_samples = len(Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM_dZxaYZikP",
        "outputId": "f5f4fd66-d9d6-4126-a01e-adfe1a21d30f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num samples group1 796 (0.1958179581795818%)\n",
            "Num samples group2 2389 (0.58769987699877%)\n",
            "Num samples group3 732 (0.18007380073800738%)\n",
            "Num samples group4 148 (0.036408364083640836%)\n"
          ]
        }
      ],
      "source": [
        "group1 = Y_train[Y_train*100 < 20]\n",
        "group2 = Y_train[(Y_train*100 >= 20) & (Y_train*100 < 40)]\n",
        "group3 = Y_train[(Y_train*100 >= 40) & (Y_train*100 < 60)]\n",
        "group4 = Y_train[Y_train*100 >= 60]\n",
        "\n",
        "print('Num samples group1', len(group1), '(' + str(len(group1) / num_samples) + '%)')\n",
        "print('Num samples group2', len(group2), '(' + str(len(group2) / num_samples) + '%)')\n",
        "print('Num samples group3', len(group3), '(' + str(len(group3) / num_samples) + '%)')\n",
        "print('Num samples group4', len(group4), '(' + str(len(group4) / num_samples) + '%)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpyvvBNGZikP"
      },
      "source": [
        "As expected the number of samples of the last group (>=60) is the one with the least number of samples. The following cells performed, at first, 4 augmentations for that group increasing the number of samples to 592. This helps to reduce the bias but it's not enough, the group is not reaching the number of samples of the other groups with less ones (700-800).\n",
        "\n",
        "So, from our point of view, each group should have a similar number of samples in order to reduce that bias. Then, we should generate ~2.3k samples for each group (the second group already has them). To do that, we can use one or more data augmentation techniques to reach that goal, let's compute how much techniques we should apply to the dataset to reach that numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB9E4qJ1ZikP",
        "outputId": "160ce891-1582-4170-a8e2-6a3d4eff009f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For group 1: 2 augmentations. Final number of samples: 2388\n",
            "For group 3: 2 augmentations. Final number of samples: 2196\n",
            "For group 4: 15 augmentations. Final number of samples: 2368\n"
          ]
        }
      ],
      "source": [
        "print('For group 1:', len(group2) // len(group1) - 1, 'augmentations. Final number of samples:', len(group1) * (len(group2) // len(group1)))\n",
        "print('For group 3:', len(group2) // len(group3) - 1, 'augmentations. Final number of samples:', len(group3) * (len(group2) // len(group3)))\n",
        "print('For group 4:', len(group2) // len(group4) - 1, 'augmentations. Final number of samples:', len(group4) * (len(group2) // len(group4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ab6-y1ZmZikP"
      },
      "source": [
        "In order to avoid repeating or transforming way too many times the same sample, we will reduce proportionally the number of transformations for each group. They will not have the same number of samples as the largest group, but they will be close enough: the bias will be mitigated and the largest group will not be very damaged in this sense.\n",
        "- 1 augmentation for group 1\n",
        "- 1 augmentation for group 3\n",
        "- 8 augmentations for group 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQrFAjDgZikP",
        "outputId": "9751137a-7be9-4ed8-8e69-287fb34b0464"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set before augmentation =  (4065, 224, 224, 3)\n",
            "Train set after augmentation =  (6629, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "# aux variables\n",
        "X_train_augmented = []\n",
        "Y_train_augmented = []\n",
        "M_train_augmented = []\n",
        "\n",
        "# loading the train data and labels\n",
        "X_train = np.load('./data/data_train.npy')\n",
        "Y_train = np.load('./data/labels_train.npy')\n",
        "Y_train = Y_train/100\n",
        "print('Train set before augmentation = ', np.array(X_train).shape)\n",
        "\n",
        "# augmenting the data\n",
        "for i in range(0,len(X_train)):\n",
        "  #### MY CODE\n",
        "  if Y_train[i]*100 < 20:\n",
        "    # First group, do one augmentation, e.g. flip\n",
        "    X_train_augmented.append(cv2.flip(X_train[i], 1))\n",
        "    Y_train_augmented.append(Y_train[i]) \n",
        "    M_train_augmented.append(M_train[i]) \n",
        "  if Y_train[i]*100 >= 40 and Y_train[i]*100 < 60:\n",
        "    # Third group, do one augmentation, e.g. flip\n",
        "    X_train_augmented.append(cv2.flip(X_train[i], 1))\n",
        "    Y_train_augmented.append(Y_train[i])\n",
        "    M_train_augmented.append(M_train[i]) \n",
        "\n",
        "  ####\n",
        "  \n",
        "  # check if image is in the group 'age >= 60'\n",
        "  if Y_train[i]*100>=60: # here labels are multiplied by 100 as they were normalized to be between [0,1]\n",
        "    # flip\n",
        "    x_flipped = cv2.flip(X_train[i], 1)\n",
        "    X_train_augmented.append(x_flipped)\n",
        "    Y_train_augmented.append(Y_train[i])\n",
        "    M_train_augmented.append(M_train[i]) \n",
        "\n",
        "    # changing brightness\n",
        "    x_aux = cv2.cvtColor(X_train[i],cv2.COLOR_RGB2HSV)\n",
        "    x_aux[:,:,2] = x_aux[:,:,2]*.5+np.random.uniform()\n",
        "    X_train_augmented.append(cv2.cvtColor(x_aux,cv2.COLOR_HSV2RGB))\n",
        "    Y_train_augmented.append(Y_train[i])\n",
        "    M_train_augmented.append(M_train[i]) \n",
        "\n",
        "    # blur\n",
        "    X_train_augmented.append(cv2.GaussianBlur(X_train[i],(5,5),1.0))\n",
        "    Y_train_augmented.append(Y_train[i])\n",
        "    M_train_augmented.append(M_train[i]) \n",
        "   \n",
        "    # translation\n",
        "    rows, cols ,c= X_train[i].shape\n",
        "    M = np.float32([[1, 0, random.randint(-25, 25)], [0, 1, random.randint(-25, 25)]])\n",
        "    X_train_augmented.append(cv2.warpAffine(X_train[i], M, (cols, rows)))\n",
        "    Y_train_augmented.append(Y_train[i])\n",
        "    M_train_augmented.append(M_train[i]) \n",
        "\n",
        "    #### MY CODE\n",
        "    # Fourth group, add 3 augmentations\n",
        "    # change brightness to flipped image\n",
        "    x_aux = cv2.cvtColor(x_flipped,cv2.COLOR_RGB2HSV)\n",
        "    x_aux[:,:,2] = x_aux[:,:,2]*.5+np.random.uniform()\n",
        "    X_train_augmented.append(cv2.cvtColor(x_aux,cv2.COLOR_HSV2RGB))\n",
        "    Y_train_augmented.append(Y_train[i])\n",
        "    M_train_augmented.append(M_train[i]) \n",
        "\n",
        "    # gaussian blur to flipped image\n",
        "    X_train_augmented.append(cv2.GaussianBlur(x_flipped,(5,5),1.0))\n",
        "    Y_train_augmented.append(Y_train[i])\n",
        "    M_train_augmented.append(M_train[i]) \n",
        "\n",
        "    # translation to flipped image\n",
        "    rows, cols ,c= x_flipped.shape\n",
        "    M = np.float32([[1, 0, random.randint(-25, 25)], [0, 1, random.randint(-25, 25)]])\n",
        "    X_train_augmented.append(cv2.warpAffine(x_flipped, M, (cols, rows)))\n",
        "    Y_train_augmented.append(Y_train[i])\n",
        "    M_train_augmented.append(M_train[i]) \n",
        "\n",
        "    ####\n",
        "\n",
        "# adding the augmented images to the train set\n",
        "X_train = np.concatenate((X_train, X_train_augmented))\n",
        "Y_train = np.concatenate((Y_train, Y_train_augmented))\n",
        "M_train = np.concatenate((M_train, M_train_augmented))\n",
        "print('Train set after augmentation = ', np.array(X_train).shape)\n",
        "\n",
        "\n",
        "# post-processing the train data with respect to ResNet-50 Inputs.\n",
        "for i in range(0,X_train.shape[0]):\n",
        "  x = X_train[i,:,:,:]\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  X_train[i,] = preprocess_input(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, something similar has to be done for the rest of biases, without breaking the balancing we have already done for age groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Ya5qmlZfD-9S"
      },
      "outputs": [],
      "source": [
        "## Convert the Y_train age labels for the age group the samples are included in. (E.g.: if item at index i in Y_train has a value of 25, its value will be 2 in binning)\n",
        "import bisect\n",
        "\n",
        "score_ranges =  [0, 0.2, 0.4, 0.6, 2]\n",
        "binning = []\n",
        "\n",
        "for a in Y_train:\n",
        "    binning.append(bisect.bisect_left(score_ranges, a))\n",
        "\n",
        "binning = np.array(binning)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To do that, we should first count how many items of each kind are available. A kind of sample is any possible combination of race and emotion. We are ignoring the gender variable because we found ourselves with a pretty well balanced scenario in this feature.\n",
        "\n",
        "Also, we will get the age distribution within each sample class.\n",
        "\n",
        "With this information, we will retrieve the number of transformations that all samples of a sample class should go through in order for all classes to have approximately the same amount of samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJ81-QhTssZX",
        "outputId": "644c2a4a-8022-41e6-faf5-f2c6c25467d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[caucasian, neutral]: 2215 samples.\n",
            "      Age distribution:  {1: 577, 2: 618, 3: 468, 4: 552}\n",
            "      Needs 0 augmentations. Final samples: 2215\n",
            "[caucasian, happy]: 927 samples.\n",
            "      Age distribution:  {1: 168, 2: 393, 3: 238, 4: 128}\n",
            "      Needs 1 augmentations. Final samples: 1854\n",
            "[caucasian, slightlyhappy]: 2443 samples.\n",
            "      Age distribution:  {1: 528, 2: 937, 3: 618, 4: 360}\n",
            "      Needs 0 augmentations. Final samples: 2443\n",
            "[caucasian, other]: 267 samples.\n",
            "      Age distribution:  {1: 78, 2: 79, 3: 38, 4: 72}\n",
            "[asian, neutral]: 238 samples.\n",
            "      Age distribution:  {1: 86, 2: 86, 3: 42, 4: 24}\n",
            "      Needs 9 augmentations. Final samples: 2380\n",
            "[asian, happy]: 80 samples.\n",
            "      Age distribution:  {1: 22, 2: 54, 3: 4}\n",
            "      Needs 29 augmentations. Final samples: 2400\n",
            "[asian, slightlyhappy]: 273 samples.\n",
            "      Age distribution:  {1: 78, 2: 141, 3: 22, 4: 32}\n",
            "      Needs 7 augmentations. Final samples: 2184\n",
            "[asian, other]: 22 samples.\n",
            "      Age distribution:  {1: 10, 2: 2, 3: 2, 4: 8}\n",
            "[afroamerican, neutral]: 56 samples.\n",
            "      Age distribution:  {1: 12, 2: 26, 3: 18}\n",
            "      Needs 42 augmentations. Final samples: 2408\n",
            "[afroamerican, happy]: 45 samples.\n",
            "      Age distribution:  {1: 20, 2: 21, 3: 4}\n",
            "      Needs 53 augmentations. Final samples: 2430\n",
            "[afroamerican, slightlyhappy]: 40 samples.\n",
            "      Age distribution:  {1: 10, 2: 28, 3: 2}\n",
            "      Needs 60 augmentations. Final samples: 2440\n",
            "[afroamerican, other]: 23 samples.\n",
            "      Age distribution:  {1: 8, 2: 5, 3: 2, 4: 8}\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "\n",
        "for race in ['caucasian', 'asian', 'afroamerican']:\n",
        "  for emotion in ['neutral', 'happy', 'slightlyhappy', 'other']:\n",
        "    filtered = np.all(M_train[:, 1:] == [race, emotion], axis=1)\n",
        "    num_samples = len(filtered[filtered])\n",
        "    ages = dict(sorted(dict(collections.Counter(binning[filtered])).items()))\n",
        "    print('[' + race + ', ' + emotion + ']: ' + str(num_samples) + ' samples.')\n",
        "    print('      Age distribution: ', ages)\n",
        "    if emotion != 'other':\n",
        "      print('      Needs ' + str(2443//num_samples - 1) + ' augmentations. Final samples: ' + str(2443//num_samples * num_samples))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we did for the age groups, we will reduce the number of transformations respecting proportions as well as possible. We are going to ignore the 'other' emotion, as it is residual. The number of transformations we will apply is returned by the following method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "TEFRF7hHBCOs"
      },
      "outputs": [],
      "source": [
        "def get_number_of_needed_transformations(sample_metadata):\n",
        "  if sample_metadata[1] == 'caucasian':\n",
        "    return 0\n",
        "  elif sample_metadata[1] == 'asian':\n",
        "    if sample_metadata[2] == 'happy':\n",
        "      return 5\n",
        "    else:\n",
        "      return 2\n",
        "  else:\n",
        "    if sample_metadata[2] == 'neutral':\n",
        "      return 8\n",
        "    else:\n",
        "      return 11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, in order to mantain the balance in the age groups: we will distribute the number of transformations within a class taking into account how many of those samples fall into each age group. In that sense, group 2 samples will generally suffer less transformations, while need more samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "8xlKhjPPg2M4"
      },
      "outputs": [],
      "source": [
        "# Util method\n",
        "def add_all(augX, augY, augZ, X, Y, Z, rep, bin):\n",
        "    for j in range(rep[bin]):\n",
        "      augX.append(X)\n",
        "      augY.append(Y)\n",
        "      augZ.append(Z)\n",
        "\n",
        "    return augX, augY, augZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvFPwyNsAxdf",
        "outputId": "f79723b2-a3b0-4f15-99d8-a0ffe40e77ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set after augmentation =  (10100, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "X_train_augmented = []\n",
        "Y_train_augmented = []\n",
        "M_train_augmented = []\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "  X_actual = X_train[i]\n",
        "  bin_actual = binning[i] - 1\n",
        "  Y_actual = Y_train[i]\n",
        "  M_actual = M_train[i]\n",
        "\n",
        "  if M_actual[1] == 'caucasian' and M_actual[2] == 'happy':\n",
        "    repetitions = [2, 0, 1, 2]\n",
        "  elif M_actual[1] == 'asian' and M_actual[2] == 'neutral':\n",
        "    repetitions = [1, 1, 3, 6]\n",
        "  elif M_actual[1] == 'asian' and M_actual[2] == 'happy':\n",
        "    repetitions = [10, 4, 18, 0]\n",
        "  elif M_actual[1] == 'asian' and M_actual[2] == 'slightlyhappy':\n",
        "    repetitions = [2, 1, 5, 5]\n",
        "  elif M_actual[1] == 'afroamerican' and M_actual[2] == 'neutral':\n",
        "    repetitions = [10, 8, 6, 0]\n",
        "  elif M_actual[1] == 'afroamerican' and M_actual[2] == 'happy':\n",
        "    repetitions = [8, 8, 20, 0]\n",
        "  elif M_actual[1] == 'afroamerican' and M_actual[2] == 'slightlyhappy':\n",
        "    repetitions = [10, 5, 20, 0]\n",
        "  else:\n",
        "    continue\n",
        "    \n",
        "  X_train_augmented, Y_train_augmented, M_train_augmented = add_all(X_train_augmented, Y_train_augmented, M_train_augmented, X_actual, Y_actual, M_actual, repetitions, bin_actual)\n",
        "\n",
        "X_train = np.concatenate((X_train, X_train_augmented))\n",
        "Y_train = np.concatenate((Y_train, Y_train_augmented))\n",
        "M_train = np.concatenate((M_train, M_train_augmented))\n",
        "print('Train set after augmentation = ', np.array(X_train).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the dataset should be augmented enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfgkfB8kZikQ"
      },
      "source": [
        "# Training the Model / or downloading a model already trained\n",
        "- As default, the code below will load a pre-trained model, obtained using the same code if LOAD_BEST_MODEL_ST1 is set to False.\n",
        "- Later, you can set LOAD_BEST_MODEL_ST1 to False to perfom the training.\n",
        "  - The code below uses Early stopping (es) with patience = 5 (that is, the training will stop if no improvement on valid_loss is observed on the last 5 epochs).\n",
        "  - It uses the Mean Squared Error (MSE) as loss function ('loss=tf.keras.losses.MeanSquaredError()'). The code also evaluates the Mean Absolute Error (MAE) during training ('metrics=['mae']'). Learning rate is set to 'learning_rate=1e-5', batch size = 32, and the model will be trained for 50 epochs (if Colab allows it based to the time budget)\n",
        "  - The model callback (mc) is set to save the best model based on valid_loss (that is, if validation loss decreases from one epoch to another, a new model is saved on the path you specify).\n",
        "  - Other hyperparameters you can play with are: defining another optimizer, loss function, learning rate, batch size, num of epochs.\n",
        "  \n",
        "\n",
        "- Note: in case you want to save your model, stop training, and resume training, check the end of this file **\"II) illustrating how to train + save + stop training + RESUME TRAINING\"** where we provide a more detailed example about this procedure. Recommendation: first train your model for a few epochs to avoid the need of resume training. This way, you will get used with the code and the general pipeline. Later, you can play with that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCyH0iSlZikQ",
        "outputId": "9425da0b-2fb9-4cbb-8d5f-2ef242b3be48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "158/158 [==============================] - 35s 177ms/step - loss: 0.3600 - mae: 0.3649 - val_loss: 0.2924 - val_mae: 0.3030\n",
            "Epoch 2/50\n",
            "158/158 [==============================] - 26s 163ms/step - loss: 0.3375 - mae: 0.3416 - val_loss: 0.2298 - val_mae: 0.1881\n",
            "Epoch 3/50\n",
            "158/158 [==============================] - 26s 165ms/step - loss: 0.2663 - mae: 0.2400 - val_loss: 0.2090 - val_mae: 0.1459\n",
            "Epoch 4/50\n",
            "158/158 [==============================] - 26s 167ms/step - loss: 0.2259 - mae: 0.1803 - val_loss: 0.2006 - val_mae: 0.1205\n",
            "Epoch 5/50\n",
            "158/158 [==============================] - 27s 170ms/step - loss: 0.2196 - mae: 0.1670 - val_loss: 0.1989 - val_mae: 0.1150\n",
            "Epoch 6/50\n",
            "158/158 [==============================] - 27s 172ms/step - loss: 0.2156 - mae: 0.1589 - val_loss: 0.1978 - val_mae: 0.1115\n",
            "Epoch 7/50\n",
            "158/158 [==============================] - 27s 174ms/step - loss: 0.2140 - mae: 0.1543 - val_loss: 0.1973 - val_mae: 0.1096\n",
            "Epoch 8/50\n",
            "158/158 [==============================] - 27s 173ms/step - loss: 0.2129 - mae: 0.1515 - val_loss: 0.1971 - val_mae: 0.1086\n",
            "Epoch 9/50\n",
            "158/158 [==============================] - 28s 174ms/step - loss: 0.2114 - mae: 0.1484 - val_loss: 0.1962 - val_mae: 0.1051\n",
            "Epoch 10/50\n",
            "158/158 [==============================] - 28s 175ms/step - loss: 0.2101 - mae: 0.1448 - val_loss: 0.1956 - val_mae: 0.1031\n",
            "Epoch 11/50\n",
            "158/158 [==============================] - 28s 177ms/step - loss: 0.2101 - mae: 0.1445 - val_loss: 0.1952 - val_mae: 0.1015\n",
            "Epoch 12/50\n",
            "158/158 [==============================] - 28s 178ms/step - loss: 0.2087 - mae: 0.1409 - val_loss: 0.1949 - val_mae: 0.1003\n",
            "Epoch 13/50\n",
            "158/158 [==============================] - 28s 178ms/step - loss: 0.2080 - mae: 0.1393 - val_loss: 0.1955 - val_mae: 0.1026\n",
            "Epoch 14/50\n",
            "158/158 [==============================] - 28s 178ms/step - loss: 0.2081 - mae: 0.1403 - val_loss: 0.1946 - val_mae: 0.0988\n",
            "Epoch 15/50\n",
            "158/158 [==============================] - 28s 179ms/step - loss: 0.2077 - mae: 0.1369 - val_loss: 0.1941 - val_mae: 0.0968\n",
            "Epoch 16/50\n",
            "158/158 [==============================] - 28s 179ms/step - loss: 0.2065 - mae: 0.1347 - val_loss: 0.1944 - val_mae: 0.0981\n",
            "Epoch 17/50\n",
            "158/158 [==============================] - 28s 179ms/step - loss: 0.2053 - mae: 0.1314 - val_loss: 0.1945 - val_mae: 0.0987\n",
            "Epoch 18/50\n",
            "158/158 [==============================] - 28s 179ms/step - loss: 0.2055 - mae: 0.1322 - val_loss: 0.1941 - val_mae: 0.0973\n",
            "Epoch 19/50\n",
            "158/158 [==============================] - 28s 180ms/step - loss: 0.2046 - mae: 0.1295 - val_loss: 0.1942 - val_mae: 0.0973\n",
            "Epoch 20/50\n",
            "158/158 [==============================] - 29s 181ms/step - loss: 0.2049 - mae: 0.1294 - val_loss: 0.1938 - val_mae: 0.0960\n",
            "Epoch 21/50\n",
            "158/158 [==============================] - 28s 180ms/step - loss: 0.2044 - mae: 0.1288 - val_loss: 0.1941 - val_mae: 0.0970\n",
            "Epoch 22/50\n",
            "158/158 [==============================] - 28s 180ms/step - loss: 0.2037 - mae: 0.1265 - val_loss: 0.1938 - val_mae: 0.0956\n",
            "Epoch 23/50\n",
            "158/158 [==============================] - 28s 180ms/step - loss: 0.2029 - mae: 0.1244 - val_loss: 0.1931 - val_mae: 0.0928\n",
            "Epoch 24/50\n",
            "158/158 [==============================] - 28s 180ms/step - loss: 0.2019 - mae: 0.1219 - val_loss: 0.1934 - val_mae: 0.0942\n",
            "Epoch 25/50\n",
            "158/158 [==============================] - 29s 181ms/step - loss: 0.2023 - mae: 0.1217 - val_loss: 0.1937 - val_mae: 0.0952\n",
            "Epoch 26/50\n",
            "158/158 [==============================] - 29s 181ms/step - loss: 0.2016 - mae: 0.1200 - val_loss: 0.1931 - val_mae: 0.0926\n",
            "Epoch 27/50\n",
            "158/158 [==============================] - 28s 180ms/step - loss: 0.2017 - mae: 0.1207 - val_loss: 0.1933 - val_mae: 0.0934\n",
            "Epoch 28/50\n",
            "158/158 [==============================] - 28s 180ms/step - loss: 0.2019 - mae: 0.1204 - val_loss: 0.1932 - val_mae: 0.0930\n",
            "Epoch 29/50\n",
            "158/158 [==============================] - 28s 181ms/step - loss: 0.2014 - mae: 0.1188 - val_loss: 0.1932 - val_mae: 0.0930\n",
            "Epoch 30/50\n",
            "158/158 [==============================] - 29s 181ms/step - loss: 0.2005 - mae: 0.1165 - val_loss: 0.1926 - val_mae: 0.0908\n",
            "Epoch 31/50\n",
            "158/158 [==============================] - 28s 181ms/step - loss: 0.1996 - mae: 0.1141 - val_loss: 0.1931 - val_mae: 0.0917\n",
            "Epoch 32/50\n",
            "158/158 [==============================] - 29s 181ms/step - loss: 0.1995 - mae: 0.1136 - val_loss: 0.1925 - val_mae: 0.0899\n",
            "Epoch 33/50\n",
            "158/158 [==============================] - 28s 180ms/step - loss: 0.1998 - mae: 0.1143 - val_loss: 0.1926 - val_mae: 0.0904\n",
            "Epoch 34/50\n",
            "158/158 [==============================] - 28s 181ms/step - loss: 0.1997 - mae: 0.1134 - val_loss: 0.1919 - val_mae: 0.0883\n",
            "Epoch 35/50\n",
            "158/158 [==============================] - 28s 181ms/step - loss: 0.1987 - mae: 0.1108 - val_loss: 0.1924 - val_mae: 0.0897\n",
            "Epoch 36/50\n",
            "158/158 [==============================] - 28s 180ms/step - loss: 0.1983 - mae: 0.1086 - val_loss: 0.1928 - val_mae: 0.0914\n",
            "Epoch 37/50\n",
            "158/158 [==============================] - 28s 180ms/step - loss: 0.1981 - mae: 0.1094 - val_loss: 0.1922 - val_mae: 0.0888\n",
            "Epoch 38/50\n",
            "158/158 [==============================] - 28s 180ms/step - loss: 0.1975 - mae: 0.1061 - val_loss: 0.1923 - val_mae: 0.0886\n",
            "Epoch 39/50\n",
            "158/158 [==============================] - 28s 180ms/step - loss: 0.1973 - mae: 0.1066 - val_loss: 0.1921 - val_mae: 0.0879\n",
            "Epoch 00039: early stopping\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import pickle\n",
        "\n",
        "# load a model and train history (defined and trained\n",
        "# as below, trained for 38 epochs)\n",
        "#--------------------------\n",
        "LOAD_BEST_MODEL_ST1 = False # (training only the last FC layers)\n",
        "#--------------------------\n",
        "\n",
        "\n",
        "if(LOAD_BEST_MODEL_ST1==True):\n",
        "  # downloading the trained model\n",
        "  !wget https://data.chalearnlap.cvc.uab.cat/Colab_2021/best_model_st1.zip\n",
        "  # decompressing the data\n",
        "  with ZipFile('best_model_st1.zip','r') as zip:\n",
        "    zip.extractall()\n",
        "    print('Model decompressed successfully')\n",
        "  # removing the .zip file after extraction  to clean space\n",
        "  !rm best_model_st1.zip\n",
        "  \n",
        "else:\n",
        "  # defining the early stop criteria\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "  # saving the best model based on val_loss\n",
        "  mc = ModelCheckpoint('/content/gdrive/MyDrive/temp/best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
        "\n",
        "  # defining the optimizer\n",
        "  model.compile(tf.keras.optimizers.Adam(learning_rate=1e-4),loss=tf.keras.losses.MeanSquaredError(),metrics=['mae'])\n",
        "\n",
        "  # training the model\n",
        "  history = model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid), batch_size=64, epochs=50, shuffle=True, verbose=1, callbacks=[es,mc])\n",
        "\n",
        "  # saving training history (for future visualization)\n",
        "  with open('/content/gdrive/MyDrive/temp/train_history.pkl', 'wb') as handle:\n",
        "    pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgFV1qJiZikQ"
      },
      "source": [
        "# Visualizing train history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "Aa7r1rYlZikQ",
        "outputId": "c896f6df-15e5-41f1-c02e-1ee700b9e629"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f4a05ba6d10>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1080x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# here, it loads the history of the model we have already trained, or loads the \n",
        "# history of the model you defined and trained\n",
        "if(LOAD_BEST_MODEL_ST1==True):\n",
        "  train_hist = pickle.load(open(\"train_history.pkl\",\"rb\"))\n",
        "else:\n",
        "  train_hist = pickle.load(open(\"/content/gdrive/MyDrive/temp/train_history.pkl\",\"rb\"))\n",
        "\n",
        "# we plot both, the LOSS and MAE\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
        "fig.suptitle('Training history (stage 1)', fontsize=14, fontweight='bold')\n",
        "\n",
        "ax1.plot(train_hist['loss'])\n",
        "ax1.plot(train_hist['val_loss'])\n",
        "ax1.set(xlabel='epoch', ylabel='LOSS')\n",
        "ax1.legend(['train', 'valid'], loc='upper right')\n",
        "\n",
        "ax2.plot(train_hist['mae'])\n",
        "ax2.plot(train_hist['val_mae'])\n",
        "ax2.set(xlabel='epoch', ylabel='MAE')\n",
        "ax2.legend(['train', 'valid'], loc='upper right')  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIvPeEOeZikQ"
      },
      "source": [
        "# Evaluate on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCgDHFhWZikR",
        "outputId": "23f2a6bc-6343-4bad-b087-681779d67319"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "62/62 [==============================] - 7s 87ms/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "#--------------------------\n",
        "ENABLE_EVALUATION_ST1 = True\n",
        "#--------------------------\n",
        "\n",
        "# loading the saved model\n",
        "if(LOAD_BEST_MODEL_ST1==True):\n",
        "  saved_model = load_model('best_model.h5')\n",
        "else:\n",
        "  saved_model = load_model('/content/gdrive/MyDrive/temp/best_model.h5')\n",
        "\n",
        "if(ENABLE_EVALUATION_ST1==True):\n",
        "  # predict on the test data\n",
        "  predictions_st1 = saved_model.predict(X_test, batch_size=32, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BurD8YsoZikR",
        "outputId": "f8ca00d4-5373-405f-c3b2-0a56f3ff8c7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE = 9.73926248\n"
          ]
        }
      ],
      "source": [
        "if(ENABLE_EVALUATION_ST1==True):\n",
        "  # re-scaling the output predictions (from [0,1] to age range) using the\n",
        "  # the normalization factor mentioned before\n",
        "  predictions_st1_f = predictions_st1*100\n",
        "\n",
        "  # evaluating on test data\n",
        "  error = []\n",
        "  for i in range(0,len(Y_test)):\n",
        "    error.append(abs(np.subtract(predictions_st1_f[i][0],Y_test[i])))\n",
        "\n",
        "  print('MAE = %.8f' %(np.mean(error)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN9BKUb7ZikR"
      },
      "source": [
        "---\n",
        "# Performing a 2nd Stage of training, where ALL Layers are set to \"trainable\"\n",
        "- Up to here, we have just trained the last FC layers of our model. Now, we will load the model we have trained (referred to it as 1st stage), set all layers to TRAINABLE, and train the whole model. Training will take more time, but we expect to get better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjNdbUoUZikR",
        "outputId": "0a1dc160-ffdb-4325-fff7-86d12d2e5a1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 base_input True\n",
            "1 conv1/7x7_s2 True\n",
            "2 conv1/7x7_s2/bn True\n",
            "3 activation_1 True\n",
            "4 max_pooling2d_1 True\n",
            "5 conv2_1_1x1_reduce True\n",
            "6 conv2_1_1x1_reduce/bn True\n",
            "7 activation_2 True\n",
            "8 conv2_1_3x3 True\n",
            "9 conv2_1_3x3/bn True\n",
            "10 activation_3 True\n",
            "11 conv2_1_1x1_increase True\n",
            "12 conv2_1_1x1_proj True\n",
            "13 conv2_1_1x1_increase/bn True\n",
            "14 conv2_1_1x1_proj/bn True\n",
            "15 add_1 True\n",
            "16 activation_4 True\n",
            "17 conv2_2_1x1_reduce True\n",
            "18 conv2_2_1x1_reduce/bn True\n",
            "19 activation_5 True\n",
            "20 conv2_2_3x3 True\n",
            "21 conv2_2_3x3/bn True\n",
            "22 activation_6 True\n",
            "23 conv2_2_1x1_increase True\n",
            "24 conv2_2_1x1_increase/bn True\n",
            "25 add_2 True\n",
            "26 activation_7 True\n",
            "27 conv2_3_1x1_reduce True\n",
            "28 conv2_3_1x1_reduce/bn True\n",
            "29 activation_8 True\n",
            "30 conv2_3_3x3 True\n",
            "31 conv2_3_3x3/bn True\n",
            "32 activation_9 True\n",
            "33 conv2_3_1x1_increase True\n",
            "34 conv2_3_1x1_increase/bn True\n",
            "35 add_3 True\n",
            "36 activation_10 True\n",
            "37 conv3_1_1x1_reduce True\n",
            "38 conv3_1_1x1_reduce/bn True\n",
            "39 activation_11 True\n",
            "40 conv3_1_3x3 True\n",
            "41 conv3_1_3x3/bn True\n",
            "42 activation_12 True\n",
            "43 conv3_1_1x1_increase True\n",
            "44 conv3_1_1x1_proj True\n",
            "45 conv3_1_1x1_increase/bn True\n",
            "46 conv3_1_1x1_proj/bn True\n",
            "47 add_4 True\n",
            "48 activation_13 True\n",
            "49 conv3_2_1x1_reduce True\n",
            "50 conv3_2_1x1_reduce/bn True\n",
            "51 activation_14 True\n",
            "52 conv3_2_3x3 True\n",
            "53 conv3_2_3x3/bn True\n",
            "54 activation_15 True\n",
            "55 conv3_2_1x1_increase True\n",
            "56 conv3_2_1x1_increase/bn True\n",
            "57 add_5 True\n",
            "58 activation_16 True\n",
            "59 conv3_3_1x1_reduce True\n",
            "60 conv3_3_1x1_reduce/bn True\n",
            "61 activation_17 True\n",
            "62 conv3_3_3x3 True\n",
            "63 conv3_3_3x3/bn True\n",
            "64 activation_18 True\n",
            "65 conv3_3_1x1_increase True\n",
            "66 conv3_3_1x1_increase/bn True\n",
            "67 add_6 True\n",
            "68 activation_19 True\n",
            "69 conv3_4_1x1_reduce True\n",
            "70 conv3_4_1x1_reduce/bn True\n",
            "71 activation_20 True\n",
            "72 conv3_4_3x3 True\n",
            "73 conv3_4_3x3/bn True\n",
            "74 activation_21 True\n",
            "75 conv3_4_1x1_increase True\n",
            "76 conv3_4_1x1_increase/bn True\n",
            "77 add_7 True\n",
            "78 activation_22 True\n",
            "79 conv4_1_1x1_reduce True\n",
            "80 conv4_1_1x1_reduce/bn True\n",
            "81 activation_23 True\n",
            "82 conv4_1_3x3 True\n",
            "83 conv4_1_3x3/bn True\n",
            "84 activation_24 True\n",
            "85 conv4_1_1x1_increase True\n",
            "86 conv4_1_1x1_proj True\n",
            "87 conv4_1_1x1_increase/bn True\n",
            "88 conv4_1_1x1_proj/bn True\n",
            "89 add_8 True\n",
            "90 activation_25 True\n",
            "91 conv4_2_1x1_reduce True\n",
            "92 conv4_2_1x1_reduce/bn True\n",
            "93 activation_26 True\n",
            "94 conv4_2_3x3 True\n",
            "95 conv4_2_3x3/bn True\n",
            "96 activation_27 True\n",
            "97 conv4_2_1x1_increase True\n",
            "98 conv4_2_1x1_increase/bn True\n",
            "99 add_9 True\n",
            "100 activation_28 True\n",
            "101 conv4_3_1x1_reduce True\n",
            "102 conv4_3_1x1_reduce/bn True\n",
            "103 activation_29 True\n",
            "104 conv4_3_3x3 True\n",
            "105 conv4_3_3x3/bn True\n",
            "106 activation_30 True\n",
            "107 conv4_3_1x1_increase True\n",
            "108 conv4_3_1x1_increase/bn True\n",
            "109 add_10 True\n",
            "110 activation_31 True\n",
            "111 conv4_4_1x1_reduce True\n",
            "112 conv4_4_1x1_reduce/bn True\n",
            "113 activation_32 True\n",
            "114 conv4_4_3x3 True\n",
            "115 conv4_4_3x3/bn True\n",
            "116 activation_33 True\n",
            "117 conv4_4_1x1_increase True\n",
            "118 conv4_4_1x1_increase/bn True\n",
            "119 add_11 True\n",
            "120 activation_34 True\n",
            "121 conv4_5_1x1_reduce True\n",
            "122 conv4_5_1x1_reduce/bn True\n",
            "123 activation_35 True\n",
            "124 conv4_5_3x3 True\n",
            "125 conv4_5_3x3/bn True\n",
            "126 activation_36 True\n",
            "127 conv4_5_1x1_increase True\n",
            "128 conv4_5_1x1_increase/bn True\n",
            "129 add_12 True\n",
            "130 activation_37 True\n",
            "131 conv4_6_1x1_reduce True\n",
            "132 conv4_6_1x1_reduce/bn True\n",
            "133 activation_38 True\n",
            "134 conv4_6_3x3 True\n",
            "135 conv4_6_3x3/bn True\n",
            "136 activation_39 True\n",
            "137 conv4_6_1x1_increase True\n",
            "138 conv4_6_1x1_increase/bn True\n",
            "139 add_13 True\n",
            "140 activation_40 True\n",
            "141 conv5_1_1x1_reduce True\n",
            "142 conv5_1_1x1_reduce/bn True\n",
            "143 activation_41 True\n",
            "144 conv5_1_3x3 True\n",
            "145 conv5_1_3x3/bn True\n",
            "146 activation_42 True\n",
            "147 conv5_1_1x1_increase True\n",
            "148 conv5_1_1x1_proj True\n",
            "149 conv5_1_1x1_increase/bn True\n",
            "150 conv5_1_1x1_proj/bn True\n",
            "151 add_14 True\n",
            "152 activation_43 True\n",
            "153 conv5_2_1x1_reduce True\n",
            "154 conv5_2_1x1_reduce/bn True\n",
            "155 activation_44 True\n",
            "156 conv5_2_3x3 True\n",
            "157 conv5_2_3x3/bn True\n",
            "158 activation_45 True\n",
            "159 conv5_2_1x1_increase True\n",
            "160 conv5_2_1x1_increase/bn True\n",
            "161 add_15 True\n",
            "162 activation_46 True\n",
            "163 conv5_3_1x1_reduce True\n",
            "164 conv5_3_1x1_reduce/bn True\n",
            "165 activation_47 True\n",
            "166 conv5_3_3x3 True\n",
            "167 conv5_3_3x3/bn True\n",
            "168 activation_48 True\n",
            "169 conv5_3_1x1_increase True\n",
            "170 conv5_3_1x1_increase/bn True\n",
            "171 add_16 True\n",
            "172 activation_49 True\n",
            "173 avg_pool True\n",
            "174 flatten_1 True\n",
            "175 dim_proj True\n",
            "176 dropout True\n",
            "177 f_128 True\n",
            "178 f_32 True\n",
            "179 dropout_1 True\n",
            "180 predict True\n"
          ]
        }
      ],
      "source": [
        "# setting all layers of the model to trainable\n",
        "saved_model.trainable = True\n",
        "\n",
        "counter = 0\n",
        "for layer in saved_model.layers:\n",
        "  print(counter, layer.name, layer.trainable)\n",
        "  counter +=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1LIjKQnZikR"
      },
      "source": [
        "# Train the whole model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGEfs5QaZikR",
        "outputId": "582632ff-4c34-4fa2-d601-d586124b189e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "632/632 [==============================] - 117s 173ms/step - loss: 0.2035 - mae: 0.1270 - val_loss: 0.1878 - val_mae: 0.0830\n",
            "Epoch 2/12\n",
            "632/632 [==============================] - 108s 171ms/step - loss: 0.1925 - mae: 0.1008 - val_loss: 0.1836 - val_mae: 0.0745\n",
            "Epoch 3/12\n",
            "632/632 [==============================] - 108s 171ms/step - loss: 0.1880 - mae: 0.0929 - val_loss: 0.1806 - val_mae: 0.0730\n",
            "Epoch 4/12\n",
            "632/632 [==============================] - 108s 171ms/step - loss: 0.1843 - mae: 0.0897 - val_loss: 0.1771 - val_mae: 0.0688\n",
            "Epoch 5/12\n",
            "632/632 [==============================] - 108s 171ms/step - loss: 0.1809 - mae: 0.0854 - val_loss: 0.1742 - val_mae: 0.0682\n",
            "Epoch 6/12\n",
            "632/632 [==============================] - 108s 171ms/step - loss: 0.1768 - mae: 0.0813 - val_loss: 0.1704 - val_mae: 0.0659\n",
            "Epoch 7/12\n",
            "632/632 [==============================] - 108s 171ms/step - loss: 0.1737 - mae: 0.0816 - val_loss: 0.1677 - val_mae: 0.0662\n",
            "Epoch 8/12\n",
            "632/632 [==============================] - 108s 171ms/step - loss: 0.1701 - mae: 0.0796 - val_loss: 0.1642 - val_mae: 0.0642\n",
            "Epoch 9/12\n",
            "632/632 [==============================] - 108s 171ms/step - loss: 0.1676 - mae: 0.0813 - val_loss: 0.1611 - val_mae: 0.0650\n",
            "Epoch 10/12\n",
            "632/632 [==============================] - 109s 172ms/step - loss: 0.1636 - mae: 0.0788 - val_loss: 0.1581 - val_mae: 0.0684\n",
            "Epoch 11/12\n",
            "632/632 [==============================] - 109s 172ms/step - loss: 0.1593 - mae: 0.0754 - val_loss: 0.1538 - val_mae: 0.0649\n",
            "Epoch 12/12\n",
            "632/632 [==============================] - 109s 172ms/step - loss: 0.1558 - mae: 0.0759 - val_loss: 0.1498 - val_mae: 0.0656\n"
          ]
        }
      ],
      "source": [
        "# Loding a pretrained model or train\n",
        "#--------------------------\n",
        "LOAD_BEST_MODEL_ST2 = False\n",
        "#--------------------------\n",
        "\n",
        "if(LOAD_BEST_MODEL_ST2==True):\n",
        "  # downloading the trained model\n",
        "  !wget https://data.chalearnlap.cvc.uab.cat/Colab_2021/best_model_st2.zip\n",
        "  # decompressing the data\n",
        "  with ZipFile('best_model_st2.zip','r') as zip:\n",
        "    zip.extractall()\n",
        "    print('Model decompressed successfully')\n",
        "  # removing the .zip file after extraction  to clean space\n",
        "  !rm best_model_st2.zip\n",
        "\n",
        "else:\n",
        "  \n",
        "  # training all layers (2nd stage), given the model saved on stage 1\n",
        "  saved_model.compile(tf.keras.optimizers.Adam(learning_rate=1e-5),loss=tf.keras.losses.MeanSquaredError(),metrics=['mae'])\n",
        "\n",
        "  # defining the early stop criteria\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "  # saving the best model (2nd stage) based on val_loss with a different name\n",
        "  mc = ModelCheckpoint('/content/gdrive/MyDrive/temp/best_model_2nd_stage.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
        "\n",
        "  history = saved_model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid), batch_size=16, epochs=12, shuffle=True, verbose=1, callbacks=[es,mc])\n",
        "\n",
        "  # saving training history\n",
        "  with open('/content/gdrive/MyDrive/temp/train_history_2nd_stage.pkl', 'wb') as handle:\n",
        "    pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g3Se9BFZikR"
      },
      "source": [
        "# Visualizing the Training history of both stages (1st stage and 2nd stage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "VjY3sS1TZikR",
        "outputId": "aed37a7e-061b-4913-8255-1f95a633312d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f4a06d27ed0>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1080x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "if(LOAD_BEST_MODEL_ST1==True and LOAD_BEST_MODEL_ST2==True):\n",
        "    train_hist = pickle.load(open(\"train_history.pkl\",\"rb\"))\n",
        "    train_hist_2nd = pickle.load(open(\"train_history_2nd_stage.pkl\",\"rb\"))\n",
        "else:\n",
        "    train_hist = pickle.load(open(\"/content/gdrive/MyDrive/temp/train_history.pkl\",\"rb\"))\n",
        "    train_hist_2nd = pickle.load(open(\"/content/gdrive/MyDrive/temp/train_history_2nd_stage.pkl\",\"rb\"))\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
        "fig.suptitle('Training history (Stage 1 and Stage 2)', fontsize=14, fontweight='bold')\n",
        "\n",
        "ax1.plot(train_hist['loss']+train_hist_2nd['loss'])\n",
        "ax1.plot(train_hist['val_loss']+train_hist_2nd['val_loss'])\n",
        "ax1.axvline(42, 0, 1, ls='--', color='r')\n",
        "ax1.set(xlabel='epoch', ylabel='Loss')\n",
        "ax1.legend(['train', 'valid'], loc='upper right')\n",
        "\n",
        "ax2.plot(train_hist['mae']+train_hist_2nd['mae'])\n",
        "ax2.plot(train_hist['val_mae']+train_hist_2nd['val_mae'])\n",
        "ax2.axvline(42, 0, 1, ls='--', color='r')\n",
        "ax2.set(xlabel='epoch', ylabel='MAE')\n",
        "ax2.legend(['train', 'valid'], loc='upper right')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl0H1KEGZikR"
      },
      "source": [
        "# Evaluate on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1TbWhYrZikS",
        "outputId": "c888d4b5-8ebf-4a57-f0ee-89b78e223023"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "62/62 [==============================] - 6s 78ms/step\n"
          ]
        }
      ],
      "source": [
        "#--------------------------\n",
        "ENABLE_EVALUATION_ST2 = True\n",
        "#--------------------------\n",
        "\n",
        "# loading the saved model\n",
        "if(LOAD_BEST_MODEL_ST2 == True):\n",
        "  saved_model_2nd = load_model('best_model_2nd_stage.h5')\n",
        "else:\n",
        "  saved_model_2nd = load_model('/content/gdrive/MyDrive/temp/best_model_2nd_stage.h5')\n",
        "\n",
        "\n",
        "if(ENABLE_EVALUATION_ST2==True):\n",
        "  # predict on the test data\n",
        "  predictions_2nd = saved_model_2nd.predict(X_test, batch_size=32, verbose=1)\n",
        "\n",
        "  # re-scaling the output predictions (from [0,1] to age range) using the\n",
        "  # the normalization factor mentioned before\n",
        "  predictions_2nd_f = predictions_2nd*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cShp1aMZZikS",
        "outputId": "962110f9-f7b8-49a9-e30c-07bcd2cdfab4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE = 7.46678216\n"
          ]
        }
      ],
      "source": [
        "if(ENABLE_EVALUATION_ST2==True):\n",
        "  # evaluating on test data\n",
        "  error = []\n",
        "  for i in range(0,len(Y_test)):\n",
        "    error.append(abs(np.subtract(predictions_2nd_f[i][0],Y_test[i])))\n",
        "\n",
        "  print('MAE = %.8f' %(np.mean(error)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXedcOgDZikS"
      },
      "source": [
        "# Check the biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IN3cCqMQZikS",
        "outputId": "2c765aca-1ae1-4c34-8361-bb9871294211"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=============================\n",
            "Age analysis:\n",
            "Size group 1 = 369, MAE = 7.372414\n",
            "Size group 2 = 1044, MAE = 6.207960\n",
            "Size group 3 = 390, MAE = 6.687294\n",
            "Size group 4 = 175, MAE = 16.912683\n",
            "---------\n",
            "Age bias (Ba) =  5.4665482838948565\n",
            "=============================\n",
            "Gender analysis:\n",
            "Size group female = 1020, MAE = 7.623360\n",
            "Size group male = 958, MAE = 7.300071\n",
            "---------\n",
            "Gender bias (Bg) =  0.3232894\n",
            "=============================\n",
            "Ethnicity Analysis:\n",
            "Size group asian = 129, MAE = 7.273325\n",
            "Size group afroamerican = 56, MAE = 7.300735\n",
            "Size group caucasian = 1793, MAE = 7.485887\n",
            "---------\n",
            "Ethnicity bias (Be) =  0.1417077382405599\n",
            "=============================\n",
            "Face experession Analysis:\n",
            "Size group happy = 589, MAE = 7.656462\n",
            "Size group slightlyhappy = 505, MAE = 7.573052\n",
            "Size group neutral = 756, MAE = 7.264775\n",
            "Size group other = 128, MAE = 7.367793\n",
            "---------\n",
            "Face Expression bias (Bf) =  0.23005358378092447\n"
          ]
        }
      ],
      "source": [
        "if(ENABLE_EVALUATION_ST2==True):\n",
        "  # computing the age bias (model_stage_2)\n",
        "  age_bias(predictions_2nd_f,Y_test)\n",
        "\n",
        "  # computing the gender bias (model_stage_2)\n",
        "  gender_bias(predictions_2nd_f,Y_test,M_test)\n",
        "\n",
        "  # computing the ethnicity bias (model_stage_2)\n",
        "  ethnicity_bias(predictions_2nd_f,Y_test,M_test)\n",
        "\n",
        "  # computing the face bias (model_stage_2)\n",
        "  face_expression_bias(predictions_2nd_f,Y_test,M_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KauF50ZvZikS"
      },
      "source": [
        "# Saving the predicted values (on Test set) to be uploaded on Codalab Competition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hM_ptIWRZikS",
        "outputId": "7773ed57-b25b-4fb5-e17a-488dde270e4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "updating: predictions.csv (deflated 55%)\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "if(ENABLE_EVALUATION_ST2==True):\n",
        "  # saving the predictions as a csv file\n",
        "  with open('predictions.csv', 'w') as csvFile:\n",
        "    writer = csv.writer(csvFile)\n",
        "    writer.writerows(predictions_2nd_f)\n",
        "  csvFile.close()\n",
        "\n",
        "  # compressing the csv file (to be submitted to codalab as prediction)\n",
        "  ! zip predictions.zip predictions.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "pBK3CBo672TP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CV_assignment1_multiplebiases&modeltune.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "92db24907b656f1a4d8cb5a7c2bba7505f1b45bd873b05b28fbc79e8b65fcf6d"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('cv-env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
