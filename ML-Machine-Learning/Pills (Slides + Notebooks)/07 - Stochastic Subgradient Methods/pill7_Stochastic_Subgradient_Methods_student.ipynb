{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>(c) October 2016 - This notebook was created by [Oriol Pujol Vila](http://www.maia.ub.es/~oriol).</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pill 7 - Stochastic Subgradient Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline:\n",
    "\n",
    "+ A quick introduction to Stochastic subgradient methods\n",
    "+ Mini batch approximation\n",
    "+ Speeding up subgradient methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic subgradient methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector $g \\in {\\bf R}^n$ is a *subgradient* of $f:{\\bf R}^n\\rightarrow{\\bf R}$ at $x\\in {\\bf \\text{dom}}\\; f$ if for all $z\\in {\\bf \\text{dom}}\\; f$,\n",
    "\n",
    "$$\n",
    "f(z)\\geq f(x) + g^T (z-x)\n",
    "$$\n",
    "\n",
    "The hyperplane with normal $(g,-1)$ supports ${\\bf \\text{epi}}\\; f$ at $(x,f(x))$. Thus if $f$ is convex and differentiable, then its gradient at $x$ is a subgradient. Note that the subgradient can exist even when $f$ is not differentiable at $x$, and more of one subgradient may exist. A function $f$ is called *subdifferentiable* at $x$ if there exists at least one subgradient at $x$. The set of subgradietns of $f$ at the point $x$ is called the *subdifferential* of $f$ at $x$, and is denoted as $\\partial f(x)$. A function $f$ is called subdifferentiable if it is subdifferentiable at all $x\\in {\\bf \\text{dom}}\\; f$. If $f$ is convex and differentiable at $x$, then $\\partial f(x) = \\{\\nabla f(x)\\}$, i.e. the gradient is the only subgradient. \n",
    "\n",
    "## Minimum of a non-differentiable function\n",
    "\n",
    "A point $x^*$ is a minimizer of a convex function $f$ if and only if $f$ is subdifferentiable at $x^*$ and \n",
    "$$0 \\in \\partial f(x^*)$$\n",
    "i.e. $0$ is a subgradient of $f$ at $x^*$. This follows from the definition of subgradient and the fact that $f(x)\\geq f(x^*)$. If $f$ is differentiable at $x^*$, the equation is reduced to $\\nabla f(x^*) = 0$.\n",
    "\n",
    "## Using subgradients for solving convex optimization problems\n",
    "\n",
    "In this section, the extension of the subgradient method is explained to solve an inequality constrained problem\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize } & \\quad f_0(x)\\\\\n",
    "\\text{subject to } & \\quad f_i(x)\\leq 0, \\quad  \\; i = 1,\\dots, m,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $f_i$ are convex. The algorithm takes the same form:\n",
    "\n",
    "$$x^{(k+1)} = x^{(k)} - \\alpha_k g^{(k)},$$\n",
    "\n",
    "where $\\alpha_k>0$ is a step size, and $g^{(k)}$ is a subgradient of the objective or one of the constraint functions at $x^{(k)}$. More specifically, we take\n",
    "\n",
    "$$\n",
    "g^{(k)}\\in \\left \\{ \\begin{align}\n",
    "\\partial f_0(x^{(k)}) & \\quad f_i(x^{(k)})\\leq 0, \\; i= 1,\\dots, m,\\\\\n",
    "\\partial f_j(x^{(k)}) & \\quad f_j(x^{(k)}) > 0.\\\\\n",
    "\\end{align}   \\right.\n",
    "$$\n",
    "\n",
    "i.e. if  $x^{(k)}$ is feasible ($f_i(x^{(k)})\\leq 0, \\; i= 1,\\dots, m$), we use an objective subgradient. Otherwise, if the current point is infeasible, we choose any violated constraint, and use a subgradient of the associated constraint function. Note that we may choose any of the violated constraints. Observe also that the iterates are often infeasible (this does not happen in the projected subgradient method). We can alternatively use a projection of the solution into the feasible set, i.e. we may project such that the violated constrained is no longer violated. As in the unconstrained case, we may keep track of the best feasible point found so far,\n",
    "$$f_{\\mbox{best}}^{(k)} = \\min \\{f_0(x^{(i)})|\\; x^{(i)} \\mbox{feasible}, i = 1,\\dots,k\\}.$$\n",
    "If there are no feasible points $f_{\\mbox{best}}^{(k)} = \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic subgradient methods \n",
    "\n",
    "Supose we have a noisy problem and we can not evaluate exactly the value of the subgradient. For example, if we are dealing with a random variable the instantaneous gradient is not reliable. There may be different sources of noise, e.g. noise due to a finite sampling, measurement noise, uncertainty in data collection, etc. \n",
    "\n",
    "Let $f: {\\bf R}^n \\rightarrow {\\bf R}$ be a convex function. We say that $\\tilde{g} \\in {\\bf R}^n$ is a *noisy (unbiased) subgradient* of $f$ at $x \\in {\\bf dom}\\; f$ if $g = {\\bf E} \\,\\tilde{g} \\in \\partial f(x)$, i.e. we have\n",
    "\n",
    "$$f(z) \\geq f(x) + ({\\bf E}\\, \\tilde{g})^T(z-x).$$\n",
    "\n",
    "This is, the expected value is a subgradient. Thus we can consider that $\\tilde{g} = g + v$, where $g\\in \\partial f(x)$ is a subgradient and $v$ is a zero mean random \"noise\". If $x$ is a random variable, then we say that $\\tilde{g}$ is a noisy subgradient if\n",
    "\n",
    "$$\\forall z \\quad f(z)\\geq f(x) + {\\bf E}\\, (\\tilde{g}|x)^T (z-x),$$\n",
    "\n",
    "holds almost surely. In other words, ${\\bf E}\\, (\\tilde{g}|x) \\in \\partial f(x)$.\n",
    "\n",
    "## The stochastic gradient method\n",
    "\n",
    "Stochastic gradient methods are essentially the subgradient method using noisy subgradients. Consider the minimization of a convex function $f$,  the stochastic subgradient method uses the standard update rule,\n",
    "\n",
    "$$x^{(k+1)} = x^{(k)} -\\alpha_k \\tilde{g}^{(k)},$$\n",
    "\n",
    "where $x^{(k)}$ is the $k$th iterate, $\\alpha_k > 0$ is the $k$th step size, and $\\tilde{g}^{(k)}$ is a noisy subgradient of $f$ at $x^{(k)}$,\n",
    "\n",
    "$${\\bf E}\\,(\\tilde{g}^{(k)}| x^{(k)}) = g^{(k)} \\in \\partial f (x^{(k)})$$\n",
    "\n",
    "We omit the convergence proof, but with square summable but not summable step size, e.g. $\\alpha_k = a / (b + k)$, it can be proved convergence in expectation, i.e. ${\\bf E}\\, f_{best}^{(k)}\\underset{k\\rightarrow \\infty}{\\rightarrow} f^*$, and convergence in probability, i.e. $\\underset{k\\rightarrow \\infty}{\\lim} {\\bf Prob} (f_{best}^{(k)}\\geq f^* + \\epsilon) = 0.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The algorithm\n",
    "\n",
    "We can use two possible algorithms, either a full stochastic subgradient iteration or a combination of subgradient and projection methods. Let us consider the hybrid version for the sake of simplicity:\n",
    "\n",
    "```\n",
    "while no_ending (iterations k):\n",
    "  1. Compute the subgradient of the loss function, g_tilde.\n",
    "  2. Take the step \n",
    "      x = x - 1/k * g_tilde\n",
    "  3. Project the into the feasible set \n",
    "      x = P(x)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving problems using Stochastic Subgradient Methods (SSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us solve our well known least squares problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def polyval(coefs, x):\n",
    "    res = coefs[0]*np.ones((1,x.shape[0]))\n",
    "    for i in range(1,len(coefs)):\n",
    "        res = res + coefs[i]*np.power(x,i)\n",
    "    return res\n",
    "        \n",
    "\n",
    "def chebys (coefs, x):\n",
    "    pol = {}\n",
    "    pol[0] = [1][::-1]\n",
    "    pol[1] = [1, 0][::-1]\n",
    "    pol[2] = [2, 0, -1][::-1]\n",
    "    pol[3] = [4, 0, -3, 0][::-1]\n",
    "    pol[4] = [8, 0, -8, 0, 1][::-1]\n",
    "    pol[5] = [16, 0, -20, 0, 5, 0][::-1]\n",
    "    pol[6] = [32, 0, -48, 0, 18, 0, -1][::-1]\n",
    "    pol[7] = [64, 0, -112, 0, 56, 0, -7, 0][::-1]\n",
    "    pol[8] = [128, 0, -256, 0, 160, 0, -32, 0, 1][::-1]\n",
    "    pol[9] = [256, 0, -576, 0, 432, 0, -120, 0, 9, 0][::-1]\n",
    "    pol[10] = [512, 0, -1280, 0, 1120, 0, -400, 0, 50, 0, -1][::-1]\n",
    "    pol[11] = [1024, 0, -2816, 0, 2816, 0, -1232, 0, 220, 0, -11, 0][::-1]\n",
    "\n",
    "    res = np.zeros((1,x.shape[0]))\n",
    "\n",
    "    for i in range(len(coefs)):\n",
    "        res= res + coefs[i]*polyval(pol[i],x)\n",
    "    return res\n",
    "\n",
    "        \n",
    "x = np.linspace(0,1,100)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(11):\n",
    "    co = np.zeros((11,))\n",
    "    co[i]=1.\n",
    "    plt.plot(x,chebys(co,x).ravel())\n",
    "    \n",
    "#Create a polynomial with noise\n",
    "np.random.seed(42)\n",
    "coefs = np.random.random(11)\n",
    "\n",
    "x = np.linspace(0,1,100)\n",
    "\n",
    "vals=chebys(coefs,x).ravel()\n",
    "#plt.plot(x,vals)\n",
    "\n",
    "N=15\n",
    "idx = np.random.randint(0,100,N)\n",
    "data = vals[idx]+0.5*np.random.normal(size=len(idx))\n",
    "plt.figure()\n",
    "plt.plot(x[idx],data,'ro')\n",
    "\n",
    "x_train = x[idx][:,np.newaxis]\n",
    "y_train = data[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones((15,1)),x_train,x_train**2,x_train**3,x_train**4,x_train**5,x_train**6,x_train**7,x_train**8,x_train**9,x_train**10].T\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#initialize weights\n",
    "w = np.zeros((11,1))\n",
    "conv = []\n",
    "n_iters = 10000\n",
    "step = 0.1\n",
    "\n",
    "\n",
    "for k in range(1,n_iters):\n",
    "    #get a noisy gradient by just taking one data point at random and get the instantaneous gradient\n",
    "    i = np.random.randint(0,X.shape[1])\n",
    "    x_sample = X[:,i][:,np.newaxis]\n",
    "    y_sample = y_train[i,:]\n",
    "    g_tilde = -2*x_sample*(y_sample-np.dot(x_sample.T,w))\n",
    "\n",
    "    w = w - step * g_tilde\n",
    "    \n",
    "    J=np.sum((np.dot(X.T,w)-y_train)*(np.dot(X.T,w)-y_train))\n",
    "    \n",
    "    conv.append(J)\n",
    "    \n",
    "plt.plot(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_draw = np.linspace(0,1,100)\n",
    "\n",
    "x_extended = np.c_[np.ones((100,1)),x_draw,x_draw**2,x_draw**3,x_draw**4,x_draw**5,x_draw**6,x_draw**7,x_draw**8,x_draw**9,x_draw**10].T\n",
    "\n",
    "y_hat = np.dot(x_extended.T,w)\n",
    "\n",
    "plt.plot(x_train,y_train,'ro')\n",
    "plt.plot(x_draw,y_hat)\n",
    "plt.plot(x,vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**EXERCISE** Use a **pocket** strategy and keep the best weight so far. Plot the new convergence plot.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_draw = np.linspace(0,1,100)\n",
    "\n",
    "x_extended = np.c_[np.ones((100,1)),x_draw,x_draw**2,x_draw**3,x_draw**4,x_draw**5,x_draw**6,x_draw**7,x_draw**8,x_draw**9,x_draw**10].T\n",
    "\n",
    "y_hat = np.dot(x_extended.T,wopt)\n",
    "\n",
    "plt.plot(x_train,y_train,'ro')\n",
    "plt.plot(x_draw,y_hat)\n",
    "plt.plot(x,vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity we consider a 2D subproblem of the former problem and visualize the weight pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.c_[np.ones((15,1)),x_train].T\n",
    "\n",
    "\n",
    "#initialize weights\n",
    "w = -np.ones((2,1))\n",
    "conv = []\n",
    "wpath = []\n",
    "wpath.append(w)\n",
    "n_iters = 100000\n",
    "Jopt = 9e16\n",
    "wopt = w\n",
    "\n",
    "step = 0.001\n",
    "for k in range(1,n_iters):\n",
    "    #get a noisy gradient by just taking one data point at random and get the instantaneous gradient\n",
    "    i = np.random.randint(0,X.shape[1])\n",
    "    x_sample = X[:,i][:,np.newaxis]\n",
    "    y_sample = y_train[i,:]\n",
    "    g_tilde = -2*x_sample*(y_sample-np.dot(x_sample.T,w))\n",
    "       \n",
    "    w = w - step * g_tilde\n",
    "    \n",
    "    #Keep the best weight up to that point \n",
    "    J=np.sum((np.dot(X.T,w)-y_train)*(np.dot(X.T,w)-y_train))\n",
    "    wpath.append(w)\n",
    "    conv.append(J)\n",
    "    #Project if needed\n",
    "    ## Add your projection code here\n",
    "    \n",
    "plt.plot(conv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#order = np.Inf\n",
    "order = 10\n",
    "delta = 0.5\n",
    "xx = np.arange(-2.0, 2.0, delta)\n",
    "yy = np.arange(-2.0, 2.0, delta)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "Xf = XX.flatten()\n",
    "Yf = YY.flatten()\n",
    "sz=XX.shape\n",
    "data = np.concatenate([Xf[:,np.newaxis],Yf[:,np.newaxis]],axis=1);\n",
    "Z = np.sum((np.dot(X.T,data.T)-y_train)*(np.dot(X.T,data.T)-y_train),axis = 0)/15.\n",
    "print (np.min(Z),np.max(Z))\n",
    "#print data.shape\n",
    "#Z = np.linalg.norm(data,ord = order,axis = 1)\n",
    "print (Z.shape)\n",
    "Z.shape=sz\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-2,2,-2,2),alpha=0.3, vmin=-10, vmax=10)\n",
    "plt.contour(XX,YY,Z,[0,1,2,3,4,5])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)\n",
    "plt.plot(-1,-1,'ro')\n",
    "plt.plot(np.array(wpath)[:,0],np.array(wpath)[:,1])\n",
    "plt.plot(np.array(wpath)[-1,0],np.array(wpath)[-1,1],'co')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#order = np.Inf\n",
    "order = 2\n",
    "delta = 0.05\n",
    "xx = np.arange(-4.0, 4.0, delta)\n",
    "yy = np.arange(-4.0, 4.0, delta)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "Xf = XX.flatten()\n",
    "Yf = YY.flatten()\n",
    "sz=XX.shape\n",
    "data = np.concatenate([Xf[:,np.newaxis],Yf[:,np.newaxis]],axis=1);\n",
    "Z = np.sum((np.dot(X.T,data.T)-y_train)*(np.dot(X.T,data.T)-y_train),axis = 0)/15.\n",
    "print (np.min(Z),np.max(Z))\n",
    "#print data.shape\n",
    "#Z = np.linalg.norm(data,ord = order,axis = 1)\n",
    "print (Z.shape)\n",
    "Z.shape=sz\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-4,4,-4,4),alpha=0.3, vmin=-20, vmax=20, cmap = \"PRGn\")\n",
    "CS = plt.contour(XX,YY,Z,[1.3,1.5,2,3,4,5,6,7,8], colors = 'k')\n",
    "plt.clabel(CS, inline=1, fontsize=10)\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)\n",
    "\n",
    "fig.savefig('contours.jpg',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "x_draw = np.linspace(0,1,100)\n",
    "\n",
    "x_extended = np.c_[np.ones((100,1)),x_draw].T\n",
    "\n",
    "y_hat = np.dot(x_extended.T,wopt)\n",
    "\n",
    "plt.plot(x_train,y_train,'ro')\n",
    "plt.plot(x_draw,y_hat)\n",
    "plt.plot(x,vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**EXERCISE** Plot the path when you use the **pocket** strategy.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my code\n",
    "\n",
    "X = np.c_[np.ones((15,1)),x_train].T\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#initialize weights\n",
    "w = np.zeros((2,1))\n",
    "conv = []\n",
    "wpath = []\n",
    "wpath.append(w)\n",
    "n_iters = 100000\n",
    "Jopt = 9e16\n",
    "wopt = w\n",
    "\n",
    "step = 0.01\n",
    "for k in range(1,n_iters):\n",
    "    #get a noisy gradient by just taking one data point at random and get the instantaneous gradient\n",
    "    i = np.random.randint(0,X.shape[1])\n",
    "    x_sample = X[:,i][:,np.newaxis]\n",
    "    y_sample = y_train[i,:]\n",
    "    g_tilde = -2*x_sample*(y_sample-np.dot(x_sample.T,w))\n",
    "       \n",
    "    w = w - step * g_tilde\n",
    "    \n",
    "    #Keep the best weight up to that point \n",
    "    J=np.sum((np.dot(X.T,w)-y_train)*(np.dot(X.T,w)-y_train))\n",
    "    if J<Jopt:\n",
    "        Jopt = J\n",
    "        wopt = w\n",
    "    \n",
    "    wpath.append(wopt)\n",
    "    #Project if needed\n",
    "    ## Add your projection code here\n",
    "    \n",
    "plt.plot(conv)\n",
    "\n",
    "#order = np.Inf\n",
    "order = 2\n",
    "delta = 0.5\n",
    "xx = np.arange(-2.0, 2.0, delta)\n",
    "yy = np.arange(-2.0, 2.0, delta)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "Xf = XX.flatten()\n",
    "Yf = YY.flatten()\n",
    "sz=XX.shape\n",
    "data = np.concatenate([Xf[:,np.newaxis],Yf[:,np.newaxis]],axis=1);\n",
    "Z = np.sum((np.dot(X.T,data.T)-y_train)*(np.dot(X.T,data.T)-y_train),axis = 0)/15.\n",
    "print (np.min(Z),np.max(Z))\n",
    "#print data.shape\n",
    "#Z = np.linalg.norm(data,ord = order,axis = 1)\n",
    "print (Z.shape)\n",
    "Z.shape=sz\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-2,2,-2,2),alpha=0.3, vmin=-10, vmax=10)\n",
    "plt.contour(XX,YY,Z,[0,1,2,3,4,5])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)\n",
    "plt.plot(0,0,'ro')\n",
    "plt.plot(np.array(wpath)[:,0],np.array(wpath)[:,1])\n",
    "plt.plot(np.array(wpath)[-1,0],np.array(wpath)[-1,1],'co')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The noisy gradient can be as noisy as a single point but we can approximate the true function using more data. This is the mini-batch method. In this case we use the average gradient of the subset of the data. Observe that this is simply a sub sampling of the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones((15,1)),x_train,x_train**2,x_train**3,x_train**4,x_train**5,x_train**6,x_train**7,x_train**8,x_train**9,x_train**10].T\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#initialize weights\n",
    "w = np.zeros((11,1))\n",
    "conv = []\n",
    "wpath = []\n",
    "wpath.append(w)\n",
    "n_iters = 10000\n",
    "Jopt = 9e16\n",
    "wopt = w\n",
    "mbatch_size=12\n",
    "step = 0.005\n",
    "for k in range(1,n_iters):\n",
    "    #get a noisy gradient by just taking one data point at random and get the instantaneous gradient\n",
    "    i = np.random.randint(0,X.shape[1],size=mbatch_size)\n",
    "    x_sample = X[:,i]\n",
    "    y_sample = y_train[i,:]\n",
    "    g_tilde = -2*np.dot(x_sample,(y_sample-np.dot(x_sample.T,w)))\n",
    "\n",
    "    w = w - step * g_tilde\n",
    "    \n",
    "    #Keep the best weight up to that point \n",
    "    J=np.sum((np.dot(X.T,w)-y_train)*(np.dot(X.T,w)-y_train))\n",
    "    if J<Jopt:\n",
    "        wopt = w\n",
    "        Jopt = J\n",
    "    conv.append(J)\n",
    "    wpath.append(w)\n",
    "    #Project if needed\n",
    "    ## Add your projection code here\n",
    "    \n",
    "plt.plot(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_draw = np.linspace(0,1,100)\n",
    "\n",
    "x_extended = np.c_[np.ones((100,1)),x_draw,x_draw**2,x_draw**3,x_draw**4,x_draw**5,x_draw**6,x_draw**7,x_draw**8,x_draw**9,x_draw**10].T\n",
    "\n",
    "y_hat = np.dot(x_extended.T,wopt)\n",
    "\n",
    "plt.plot(x_train,y_train,'ro')\n",
    "plt.plot(x_draw,y_hat)\n",
    "plt.plot(x,vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe now the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones((15,1)),x_train].T\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#initialize weights\n",
    "w = np.zeros((2,1))\n",
    "conv = []\n",
    "wpath = []\n",
    "wpath.append(w)\n",
    "n_iters = 100000\n",
    "Jopt = 9e16\n",
    "wopt = w\n",
    "mbatch_size=30\n",
    "step = 0.001\n",
    "for k in range(1,n_iters):\n",
    "    #get a noisy gradient by just taking one data point at random and get the instantaneous gradient\n",
    "   \n",
    "    i = np.random.randint(0,X.shape[1],size=mbatch_size)\n",
    "    x_sample = X[:,i]\n",
    "    y_sample = y_train[i,:]\n",
    "    g_tilde = -2*np.dot(x_sample,(y_sample-np.dot(x_sample.T,w)))\n",
    "\n",
    "    w = w - step * g_tilde\n",
    "    \n",
    "    #Keep the best weight up to that point \n",
    "    J=np.sum((np.dot(X.T,w)-y_train)*(np.dot(X.T,w)-y_train))\n",
    "    if J<Jopt:\n",
    "        wopt = w\n",
    "        Jopt = J\n",
    "    conv.append(Jopt)\n",
    "    wpath.append(wopt)\n",
    "    #Project if needed\n",
    "    ## Add your projection code here\n",
    "    \n",
    "plt.plot(conv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#order = np.Inf\n",
    "order = 2\n",
    "delta = 0.5\n",
    "xx = np.arange(-2.0, 2.0, delta)\n",
    "yy = np.arange(-2.0, 2.0, delta)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "Xf = XX.flatten()\n",
    "Yf = YY.flatten()\n",
    "sz=XX.shape\n",
    "data = np.concatenate([Xf[:,np.newaxis],Yf[:,np.newaxis]],axis=1);\n",
    "Z = np.sum((np.dot(X.T,data.T)-y_train)*(np.dot(X.T,data.T)-y_train),axis = 0)/15.\n",
    "print (np.min(Z),np.max(Z))\n",
    "#print data.shape\n",
    "#Z = np.linalg.norm(data,ord = order,axis = 1)\n",
    "print (Z.shape)\n",
    "Z.shape=sz\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-2,2,-2,2),alpha=0.3, vmin=-10, vmax=10)\n",
    "plt.contour(XX,YY,Z,[0,1,2,3,4,5])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)\n",
    "plt.plot(0,0,'ro')\n",
    "plt.plot(np.array(wpath)[:,0],np.array(wpath)[:,1])\n",
    "plt.plot(np.array(wpath)[-1,0],np.array(wpath)[-1,1],'co')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**EXERCISE** Change the `append` and use the optimal weights. Show the path of the optimal weights. How does it compare with the non-minibatch technique?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speeding up methods\n",
    "\n",
    "These methods may suffer from a lack of speed convergence. There are several methods that allow to speed up the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "\n",
    "In momentum we are trying to capture the notion of ‘inertia’ or that of ‘momentum’ (note that both are different concepts). The idea is that instead of moving in the direction of minus the gradient we will move in the direction of a combination of the previously accumulated directions combined with the new direction. This technique has a parameter $\\gamma$ that is related to the decay or memory of the previous directions.\n",
    "\n",
    "$$m^{(k+1)} = \\gamma m^{(k)} - \\alpha_k g^{(k)}$$\n",
    "\n",
    "$$x^{(k+1)} = x^{(k)} + m^{(k+1)}$$ \n",
    "\n",
    "where $\\gamma$ usually takes values around $0.9$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**EXERCISE** Fill the code gap with the momentum.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "X = np.c_[np.ones((15,1)),x_train].T\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#initialize weights\n",
    "w = np.zeros((2,1))\n",
    "conv = []\n",
    "wpath = []\n",
    "wpath.append(w)\n",
    "n_iters = 10000\n",
    "Jopt = 9e16\n",
    "wopt = w\n",
    "mbatch_size=10\n",
    "step = 0.0001\n",
    "momentum = 0.95\n",
    "m = 0.\n",
    "for k in range(1,n_iters):\n",
    "    #get a noisy gradient by just taking one data point at random and get the instantaneous gradient\n",
    "   \n",
    "    i = np.random.randint(0,X.shape[1],size=mbatch_size)\n",
    "    x_sample = X[:,i]\n",
    "    y_sample = y_train[i,:]\n",
    "    g_tilde = -2*np.dot(x_sample,(y_sample-np.dot(x_sample.T,w)))\n",
    "\n",
    "    ### PUT YOUR CODE FOR w UPDATE HERE\n",
    "    \n",
    "    #Keep the best weight up to that point \n",
    "    J=np.sum((np.dot(X.T,w)-y_train)*(np.dot(X.T,w)-y_train))\n",
    "    if J<Jopt:\n",
    "        wopt = w\n",
    "        Jopt = J\n",
    "    conv.append(Jopt)\n",
    "    wpath.append(w)\n",
    "    #Project if needed\n",
    "    ## Add your projection code here\n",
    "    \n",
    "plt.plot(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#order = np.Inf\n",
    "order = 2\n",
    "delta = 0.5\n",
    "xx = np.arange(-2.0, 2.0, delta)\n",
    "yy = np.arange(-2.0, 2.0, delta)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "Xf = XX.flatten()\n",
    "Yf = YY.flatten()\n",
    "sz=XX.shape\n",
    "data = np.concatenate([Xf[:,np.newaxis],Yf[:,np.newaxis]],axis=1);\n",
    "Z = np.sum((np.dot(X.T,data.T)-y_train)*(np.dot(X.T,data.T)-y_train),axis = 0)/15.\n",
    "print (np.min(Z),np.max(Z))\n",
    "#print data.shape\n",
    "#Z = np.linalg.norm(data,ord = order,axis = 1)\n",
    "print (Z.shape)\n",
    "Z.shape=sz\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-2,2,-2,2),alpha=0.3, vmin=-10, vmax=10)\n",
    "plt.contour(XX,YY,Z,[0,1,2,3,4,5])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)\n",
    "plt.plot(np.array(wpath)[0,0],np.array(wpath)[0,1],'ro')\n",
    "plt.plot(np.array(wpath)[:,0],np.array(wpath)[:,1])\n",
    "plt.plot(np.array(wpath)[-1,0],np.array(wpath)[-1,1],'co')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second family of methods tries to adapt the learning rate for each coordinate. This family of methods are of the form\n",
    "\n",
    "\n",
    "$$x^{(k+1)} = x^{(k)} - \\alpha_k D^{-1} g^{(k)}$$\n",
    "\n",
    "where $D$ is a diagonal matrix. Observe that this independently adapts each coordinate. In particular in Adagrad, $D$ is defined as \n",
    "\n",
    "$$D = \\sqrt{\\sum_{i=1}^k \\text{diag}(g^{(i)})}$$\n",
    "\n",
    "The algorithm can be written as follows,\n",
    "\n",
    "$$D^{(k+1)} = D^{(k)} + \\text{diag}(g^{(k)})^2$$\n",
    "\n",
    "\n",
    "$$x^{(k+1)} = x^{(k)} - \\alpha_k (D+\\epsilon I)^{-1/2}g^{(k)}$$ \n",
    "\n",
    "where $\\epsilon I$ is added for stability purposes. Note that if we implement this we do not build the diagonal matrix but operate element to element with the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones((15,1)),x_train].T\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#initialize weights\n",
    "#w = np.zeros((2,1))\n",
    "w = -1.*np.ones((2,1))\n",
    "conv = []\n",
    "wpath = []\n",
    "wpath.append(w)\n",
    "n_iters = 100000\n",
    "Jopt = 9e16\n",
    "wopt = w\n",
    "mbatch_size=10\n",
    "step = 0.01\n",
    "epsilon = 1e-16\n",
    "D = np.zeros((2,1))\n",
    "for k in range(1,n_iters):\n",
    "    #get a noisy gradient by just taking one data point at random and get the instantaneous gradient\n",
    "   \n",
    "    i = np.random.randint(0,X.shape[1],size=mbatch_size)\n",
    "    x_sample = X[:,i]\n",
    "    y_sample = y_train[i,:]\n",
    "    g_tilde = -2*np.dot(x_sample,(y_sample-np.dot(x_sample.T,w)))\n",
    "\n",
    "    D = D + g_tilde*g_tilde\n",
    "    \n",
    "    w = w  - step * g_tilde / np.sqrt(D + epsilon)\n",
    "    \n",
    "    #Keep the best weight up to that point \n",
    "    J=np.sum((np.dot(X.T,w)-y_train)*(np.dot(X.T,w)-y_train))\n",
    "    if J<Jopt:\n",
    "        wopt = w\n",
    "        Jopt = J\n",
    "    conv.append(Jopt)\n",
    "    wpath.append(w)\n",
    "    #Project if needed\n",
    "    ## Add your projection code here\n",
    "    \n",
    "plt.plot(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#order = np.Inf\n",
    "order = 2\n",
    "delta = 0.5\n",
    "xx = np.arange(-2.0, 2.0, delta)\n",
    "yy = np.arange(-2.0, 2.0, delta)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "Xf = XX.flatten()\n",
    "Yf = YY.flatten()\n",
    "sz=XX.shape\n",
    "data = np.concatenate([Xf[:,np.newaxis],Yf[:,np.newaxis]],axis=1);\n",
    "Z = np.sum((np.dot(X.T,data.T)-y_train)*(np.dot(X.T,data.T)-y_train),axis = 0)/15.\n",
    "print (np.min(Z),np.max(Z))\n",
    "#print data.shape\n",
    "#Z = np.linalg.norm(data,ord = order,axis = 1)\n",
    "print (Z.shape)\n",
    "Z.shape=sz\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-2,2,-2,2),alpha=0.3, vmin=-10, vmax=10)\n",
    "plt.contour(XX,YY,Z,[0,1,2,3,4,5])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)\n",
    "plt.plot(np.array(wpath)[0,0],np.array(wpath)[0,1],'ro')\n",
    "plt.plot(np.array(wpath)[:,0],np.array(wpath)[:,1])\n",
    "plt.plot(np.array(wpath)[-1,0],np.array(wpath)[-1,1],'co')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "\n",
    "Adam combines both momentum and adagrad. \n",
    "\n",
    "$$m^{(k+1)} = \\gamma_1 m^{(k)} - (1-\\gamma_1) g^{(k)}$$\n",
    "\n",
    "$$D^{(k+1)} = \\gamma_2 D^{(k)} + (1-\\gamma_2)\\text{diag}(g^{(k)})^2$$\n",
    "\n",
    "$$x^{(k+1)} = x^{(k)} + \\alpha_k (D^{(k+1)}+\\epsilon I)^{-1/2}m^{(k+1)}$$ \n",
    "\n",
    "Usual values of $\\gamma_1$ are $0.9$ and $\\gamma_2$ are $0.999$, $\\epsilon$ can take very small values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**EXERCISE** Code the Adam according to the former equations.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "X = np.c_[np.ones((15,1)),x_train].T\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#initialize weights\n",
    "w = np.zeros((2,1))\n",
    "conv = []\n",
    "wpath = []\n",
    "wpath.append(w)\n",
    "n_iters = 100000\n",
    "Jopt = 9e16\n",
    "wopt = w\n",
    "mbatch_size=10\n",
    "step = 0.0001\n",
    "epsilon = 1e-16\n",
    "D = w\n",
    "m = w\n",
    "g1 = 0.9\n",
    "g2 = 0.999\n",
    "for k in range(1,n_iters):\n",
    "    #get a noisy gradient by just taking one data point at random and get the instantaneous gradient\n",
    "   \n",
    "    i = np.random.randint(0,X.shape[1],size=mbatch_size)\n",
    "    x_sample = X[:,i]\n",
    "    y_sample = y_train[i,:]\n",
    "    g_tilde = -2*np.dot(x_sample,(y_sample-np.dot(x_sample.T,w)))\n",
    "    \n",
    "    #### YOUR ADAM CODE HERE\n",
    "    \n",
    "    #Keep the best weight up to that point \n",
    "    J=np.sum((np.dot(X.T,w)-y_train)*(np.dot(X.T,w)-y_train))\n",
    "    if J<Jopt:\n",
    "        wopt = w\n",
    "        Jopt = J\n",
    "    conv.append(Jopt)\n",
    "    wpath.append(w)\n",
    "    #Project if needed\n",
    "    ## Add your projection code here\n",
    "    \n",
    "plt.plot(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#order = np.Inf\n",
    "order = 2\n",
    "delta = 0.5\n",
    "xx = np.arange(-2.0, 2.0, delta)\n",
    "yy = np.arange(-2.0, 2.0, delta)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "Xf = XX.flatten()\n",
    "Yf = YY.flatten()\n",
    "sz=XX.shape\n",
    "data = np.concatenate([Xf[:,np.newaxis],Yf[:,np.newaxis]],axis=1);\n",
    "Z = np.sum((np.dot(X.T,data.T)-y_train)*(np.dot(X.T,data.T)-y_train),axis = 0)/15.\n",
    "print (np.min(Z),np.max(Z))\n",
    "#print data.shape\n",
    "#Z = np.linalg.norm(data,ord = order,axis = 1)\n",
    "print (Z.shape)\n",
    "Z.shape=sz\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-2,2,-2,2),alpha=0.3, vmin=-10, vmax=10)\n",
    "plt.contour(XX,YY,Z,[0,1,2,3,4,5])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)\n",
    "plt.plot(np.array(wpath)[0,0],np.array(wpath)[0,1],'ro')\n",
    "plt.plot(np.array(wpath)[:,0],np.array(wpath)[:,1])\n",
    "plt.plot(np.array(wpath)[-1,0],np.array(wpath)[-1,1],'co')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones((15,1)),x_train,x_train**2,x_train**3,x_train**4,x_train**5,x_train**6,x_train**7,x_train**8,x_train**9,x_train**10].T\n",
    "import numpy as np\n",
    "\n",
    "#initialize weights\n",
    "w = np.ones((11,1))\n",
    "conv = []\n",
    "wpath = []\n",
    "wpath.append(w)\n",
    "n_iters = 100000\n",
    "Jopt = 9e16\n",
    "wopt = w\n",
    "mbatch_size=10\n",
    "step = 0.001\n",
    "epsilon = 1e-16\n",
    "D = w\n",
    "m = w\n",
    "g1 = 0.9\n",
    "g2 = 0.999\n",
    "for k in range(1,n_iters):\n",
    "    #get a noisy gradient by just taking one data point at random and get the instantaneous gradient\n",
    "   \n",
    "    i = np.random.randint(0,X.shape[1],size=mbatch_size)\n",
    "    x_sample = X[:,i]\n",
    "    y_sample = y_train[i,:]\n",
    "    g_tilde = -2*np.dot(x_sample,(y_sample-np.dot(x_sample.T,w))) \n",
    "    \n",
    "    # UPDATE w\n",
    "    \n",
    "    #Keep the best weight up to that point \n",
    "    J=np.sum((np.dot(X.T,w)-y_train)*(np.dot(X.T,w)-y_train))\n",
    "    if J<Jopt:\n",
    "        wopt = w\n",
    "        Jopt = J\n",
    "    conv.append(Jopt)\n",
    "    wpath.append(w)\n",
    "    #Project if needed\n",
    "    ## Add your projection code here\n",
    "    \n",
    "plt.plot(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_draw = np.linspace(0,1,100)\n",
    "\n",
    "x_extended = np.c_[np.ones((100,1)),x_draw,x_draw**2,x_draw**3,x_draw**4,x_draw**5,x_draw**6,x_draw**7,x_draw**8,x_draw**9,x_draw**10].T\n",
    "\n",
    "y_hat = np.dot(x_extended.T,wopt)\n",
    "\n",
    "plt.plot(x_train,y_train,'ro')\n",
    "plt.plot(x_draw,y_hat)\n",
    "plt.plot(x,vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**EXERCISE** Plot the convergence curves for all methods at the same time.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
