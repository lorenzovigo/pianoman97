{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data hunting and gathering\n",
    "\n",
    "\n",
    "SOFTWARE REQUIREMENTS\n",
    "    \n",
    "    + mongoDB #create a MongoDB Atlas cluster on the cloud\n",
    "    + selenium #pip install selenium\n",
    "    + pymongo #pip install pymongo\n",
    "    + lxml #pip install  lxml\n",
    "    + tweepy #pip install tweepy\n",
    "    \n",
    "NOT REQUIRED BUT USED IN ONE EXAMPLE: \n",
    "\n",
    "    + MPV via Homebrew (OSX) or mplayer in Linux.\n",
    "    \n",
    "OTHER UTILITIES\n",
    "\n",
    "    + If you are using Firefox browser you may need Firebug.\n",
    "\n",
    "CONTENTS\n",
    "\n",
    "+ Introduction and warm-up project: A web crawler\n",
    "     \n",
    "+ Using the API\n",
    "\n",
    "    + Retrieving Twitter data\n",
    "\n",
    "+ Creating our own web API: Scraping\n",
    "\n",
    "    + Understanding HTML and CSS\n",
    "    + CSS selectors\n",
    "    + XPath selectors\n",
    "    + Scraping dynamic content with Selenium    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is the basis of this course. Although we usually find it in well structured formats such as a spreadsheet resulting from our last experiment, or the collection of company records in a classical relational database, with the advent of internet new information sources have to be taken into account. However, these new sources are home of unstructured data. In this lecture several methods for retrieving data and storing it are presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first introduce the big picture guiding this lecture. Whenever we want to retrieve data from a web site we should ask first if the web site is providing a simple way for that purpose. Many large sites such as google, facebook, twitter, etc, provide a **Application Programming Interface (API)** that can make data hunting easier. However, most of web sites do not have this interface. Even more, an API may not provide the desired information. In those cases we have to use **scraping** techniques. This means dealing with the raw information as it is provided to the web browser and code our data finding methods.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"border-radius:20px;\" src=\"./files/big_picture.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start connecting to the net and checking out how to retrieve a basic page. We will start using `urllib` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<http.client.HTTPResponse at 0x1108d19d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "source = urlopen('http://google.com')#Let us check what is in\n",
    "source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!doctype html><html itemscope=\"\" itemtype=\"http://schema.org/WebPage\" lang=\"es\"><head><meta content=\"Google.es permite acceder a la informaci\\xf3n mundial en castellano, catal\\xe1n, gallego, euskara e ingl\\xe9s.\" name=\"description\"><meta content=\"noodp\" name=\"robots\"><meta content=\"text/html; charset=UTF-8\" http-equiv=\"Content-Type\"><meta content=\"/images/branding/googleg/1x/googleg_standard_color_128dp.png\" itemprop=\"image\"><title>Google</title><script nonce=\"fMprxNz5aGYgYSFAnN8Vmw==\">(function(){window.google={kEI:\\'IVRUYZyYBeuiggfiv6mADw\\',kEXPI:\\'0,1302536,56873,1710,4348,207,4804,2316,383,246,5,1354,5251,1122515,1197714,687,328866,51224,16114,28683,17573,4858,1362,284,9007,3020,17588,4020,978,13228,3847,4192,6430,14763,4281,2778,919,5081,889,704,1279,2212,241,290,148,1103,840,1983,213,4101,109,3405,606,2023,2297,14670,3227,2845,7,12354,5096,16320,908,2,941,2614,13142,3,576,6459,149,13975,4,1528,2304,1236,5803,4684,2014,18375,2658,4243,2458,654,32,5616,8012,2305,638,1494,13406,3380,5812,2545,992,3102,3138,6,908,3,3541,1,5096,2,1,3,9614,1808,283,912,5992,16728,1715,2,9223,4799,1931,3911,1678,743,5853,2050,6823,453,1137,1160,1290,75,7715,2718,4546,3,1601,2376,1496,4635,3622,2,1,7,967,6005,32,731,1932,2635,2578,3132,543,2075,400,1519,3,1377,1264,85,181,549,2,1,107,2,4,2215,720,2,534,942,914,2632,903,1372,286,314,59,3,52,117,197,697,1278,2855,4,49,256,539,543,305,241,1264,243,1021,294,1590,40,503,302,391,429,372,37,43,247,1409,228,591,129,238,730,368,3,300,506,278,197,798,4,191,1615,86,2052,122,153,636,15,2,188,79,83,674,33,241,23,22,141,221,109,334,20,20,189,362,2595,5568402,446,73,88,45,133,1802550,4193980,2800696,882,444,1,2,80,1,1796,1,9,2553,1,748,141,795,563,1,4265,1,1,2,1331,4142,2609,155,17,13,72,139,4,2,20,2,169,13,19,46,5,39,96,548,29,2,2,1,2,1,2,2,7,4,1,2,2,2,2,2,2,353,513,186,1,1,158,3,2,2,2,2,2,4,2,3,3,269,441,1,19,2,45,3,31,9,17,3,2,6,3,8,86,4,1,49,1,4,1,4,1,2,16,1,3,27,23953932,4041352,338,3,2862,2,1041,9,1435,1517,291,117,1152,265,881,516,120,221\\',kBL:\\'MTWg\\'};google.sn=\\'webhp\\';google.kHL=\\'es\\';})();(function(){\\nvar f=this||self;var h,k=[];function l(a){for(var b;a&&(!a.getAttribute||!(b=a.getAttribute(\"eid\")));)a=a.parentNode;return b||h}function m(a){for(var b=null;a&&(!a.getAttribute||!(b=a.getAttribute(\"leid\")));)a=a.parentNode;return b}\\nfunction n(a,b,c,d,g){var e=\"\";c||-1!==b.search(\"&ei=\")||(e=\"&ei=\"+l(d),-1===b.search(\"&lei=\")&&(d=m(d))&&(e+=\"&lei=\"+d));d=\"\";!c&&f._cshid&&-1===b.search(\"&cshid=\")&&\"slh\"!==a&&(d=\"&cshid=\"+f._cshid);c=c||\"/\"+(g||\"gen_204\")+\"?atyp=i&ct=\"+a+\"&cad=\"+b+e+\"&zx=\"+Date.now()+d;/^http:/i.test(c)&&\"https:\"===window.location.protocol&&(google.ml&&google.ml(Error(\"a\"),!1,{src:c,glmm:1}),c=\"\");return c};h=google.kEI;google.getEI=l;google.getLEI=m;google.ml=function(){return null};google.log=function(a,b,c,d,g){if(c=n(a,b,c,d,g)){a=new Image;var e=k.length;k[e]=a;a.onerror=a.onload=a.onabort=function(){delete k[e]};a.src=c}};google.logUrl=n;}).call(this);(function(){\\ngoogle.y={};google.sy=[];google.x=function(a,b){if(a)var c=a.id;else{do c=Math.random();while(google.y[c])}google.y[c]=[a,b];return!1};google.sx=function(a){google.sy.push(a)};google.lm=[];google.plm=function(a){google.lm.push.apply(google.lm,a)};google.lq=[];google.load=function(a,b,c){google.lq.push([[a],b,c])};google.loadAll=function(a,b){google.lq.push([a,b])};google.bx=!1;google.lx=function(){};}).call(this);google.f={};(function(){\\ndocument.documentElement.addEventListener(\"submit\",function(b){var a;if(a=b.target){var c=a.getAttribute(\"data-submitfalse\");a=\"1\"===c||\"q\"===c&&!a.elements.q.value?!0:!1}else a=!1;a&&(b.preventDefault(),b.stopPropagation())},!0);document.documentElement.addEventListener(\"click\",function(b){var a;a:{for(a=b.target;a&&a!==document.documentElement;a=a.parentElement)if(\"A\"===a.tagName){a=\"1\"===a.getAttribute(\"data-nohref\");break a}a=!1}a&&b.preventDefault()},!0);}).call(this);</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}@media all{.gb1{height:22px;margin-right:.5em;vertical-align:top}#gbar{float:left}}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb4{color:#00c !important}.gbi .gb4{color:#dd8e27 !important}.gbf .gb4{color:#900 !important}\\n</style><style>body,td,a,p,.h{font-family:arial,sans-serif}body{margin:0;overflow-y:scroll}#gog{padding:3px 8px 0}td{line-height:.8em}.gac_m td{line-height:17px}form{margin-bottom:20px}.h{color:#1558d6}em{font-weight:bold;font-style:normal}.lst{height:25px;width:496px}.gsfi,.lst{font:18px arial,sans-serif}.gsfs{font:17px arial,sans-serif}.ds{display:inline-box;display:inline-block;margin:3px 0 4px;margin-left:4px}input{font-family:inherit}body{background:#fff;color:#000}a{color:#4b11a8;text-decoration:none}a:hover,a:active{text-decoration:underline}.fl a{color:#1558d6}a:visited{color:#4b11a8}.sblc{padding-top:5px}.sblc a{display:block;margin:2px 0;margin-left:13px;font-size:11px}.lsbb{background:#f8f9fa;border:solid 1px;border-color:#dadce0 #70757a #70757a #dadce0;height:30px}.lsbb{display:block}#WqQANb a{display:inline-block;margin:0 12px}.lsb{background:url(/images/nav_logo229.png) 0 -261px repeat-x;border:none;color:#000;cursor:pointer;height:30px;margin:0;outline:0;font:15px arial,sans-serif;vertical-align:top}.lsb:active{background:#dadce0}.lst:focus{outline:none}</style><script nonce=\"fMprxNz5aGYgYSFAnN8Vmw==\">(function(){window.google.erd={sp:\\'hp\\',jsr:0,bv:1450};\\nvar f=this||self;var g,h,k=null!==(g=f.mei)&&void 0!==g?g:1,l=null!==(h=f.sdo)&&void 0!==h?h:!0,n=0,p,q=google.erd,t=q.jsr;google.ml=function(a,b,e,m,d){d=void 0===d?2:d;b&&(p=a&&a.message);if(google.dl)return google.dl(a,d,e),null;if(0>t){window.console&&console.error(a,e);if(-2===t)throw a;b=!1}else b=!a||!a.message||\"Error loading script\"===a.message||n>=k&&!m?!1:!0;if(!b)return null;n++;e=e||{};b=encodeURIComponent;var c=\"/gen_204?atyp=i&ei=\"+b(google.kEI);google.kEXPI&&(c+=\"&jexpid=\"+b(google.kEXPI));c+=\"&srcpg=\"+b(q.sp)+\"&jsr=\"+b(q.jsr)+\"&bver=\"+b(q.bv)+(\"&jsel=\"+d);c+=\"&sn=\"+b(google.sn);for(var r in e)c+=\"&\",c+=b(r),c+=\"=\",c+=b(e[r]);c=c+\"&emsg=\"+b(a.name+\": \"+a.message);c=c+\"&jsst=\"+b(a.stack||\"N/A\");12288<=c.length&&(c=c.substr(0,12288));a=c;m||google.log(0,\"\",a);return a};window.onerror=function(a,b,e,m,d){p!==a&&google.ml(d instanceof Error?d:Error(a),!1,void 0,!1,0);p=null;l&&n>=k&&(window.onerror=null)};})();</script></head><body bgcolor=\"#fff\"><script nonce=\"fMprxNz5aGYgYSFAnN8Vmw==\">(function(){var src=\\'/images/nav_logo229.png\\';var iesg=false;document.body.onload = function(){window.n && window.n();if (document.images){new Image().src=src;}\\nif (!iesg){document.f&&document.f.q.focus();document.gbqf&&document.gbqf.q.focus();}\\n}\\n})();</script><div id=\"mngb\"><div id=gbar><nobr><b class=gb1>B\\xfasqueda</b> <a class=gb1 href=\"http://www.google.es/imghp?hl=es&tab=wi\">Im\\xe1genes</a> <a class=gb1 href=\"http://maps.google.es/maps?hl=es&tab=wl\">Maps</a> <a class=gb1 href=\"https://play.google.com/?hl=es&tab=w8\">Play</a> <a class=gb1 href=\"http://www.youtube.com/?gl=ES&tab=w1\">YouTube</a> <a class=gb1 href=\"https://news.google.com/?tab=wn\">Noticias</a> <a class=gb1 href=\"https://mail.google.com/mail/?tab=wm\">Gmail</a> <a class=gb1 href=\"https://drive.google.com/?tab=wo\">Drive</a> <a class=gb1 style=\"text-decoration:none\" href=\"https://www.google.es/intl/es/about/products?tab=wh\"><u>M\\xe1s</u> &raquo;</a></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a href=\"http://www.google.es/history/optout?hl=es\" class=gb4>Historial web</a> | <a  href=\"/preferences?hl=es\" class=gb4>Ajustes</a> | <a target=_top id=gb_70 href=\"https://accounts.google.com/ServiceLogin?hl=es&passive=true&continue=http://www.google.com/&ec=GAZAAQ\" class=gb4>Iniciar sesi\\xf3n</a></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div></div><center><br clear=\"all\" id=\"lgpd\"><div id=\"lga\"><img alt=\"Google\" height=\"92\" src=\"/images/branding/googlelogo/1x/googlelogo_white_background_color_272x92dp.png\" style=\"padding:28px 0 14px\" width=\"272\" id=\"hplogo\"><br><br></div><form action=\"/search\" name=\"f\"><table cellpadding=\"0\" cellspacing=\"0\"><tr valign=\"top\"><td width=\"25%\">&nbsp;</td><td align=\"center\" nowrap=\"\"><input name=\"ie\" value=\"ISO-8859-1\" type=\"hidden\"><input value=\"es\" name=\"hl\" type=\"hidden\"><input name=\"source\" type=\"hidden\" value=\"hp\"><input name=\"biw\" type=\"hidden\"><input name=\"bih\" type=\"hidden\"><div class=\"ds\" style=\"height:32px;margin:4px 0\"><input class=\"lst\" style=\"margin:0;padding:5px 8px 0 6px;vertical-align:top;color:#000\" autocomplete=\"off\" value=\"\" title=\"Buscar con Google\" maxlength=\"2048\" name=\"q\" size=\"57\"></div><br style=\"line-height:0\"><span class=\"ds\"><span class=\"lsbb\"><input class=\"lsb\" value=\"Buscar con Google\" name=\"btnG\" type=\"submit\"></span></span><span class=\"ds\"><span class=\"lsbb\"><input class=\"lsb\" id=\"tsuid1\" value=\"Voy a tener suerte\" name=\"btnI\" type=\"submit\"><script nonce=\"fMprxNz5aGYgYSFAnN8Vmw==\">(function(){var id=\\'tsuid1\\';document.getElementById(id).onclick = function(){if (this.form.q.value){this.checked = 1;if (this.form.iflsig)this.form.iflsig.disabled = false;}\\nelse top.location=\\'/doodles/\\';};})();</script><input value=\"ALs-wAMAAAAAYVRiMcOtNWPw2wp_L_EBP6J9Gc6_BAeC\" name=\"iflsig\" type=\"hidden\"></span></span></td><td class=\"fl sblc\" align=\"left\" nowrap=\"\" width=\"25%\"><a href=\"/advanced_search?hl=es&amp;authuser=0\">B\\xfasqueda avanzada</a></td></tr></table><input id=\"gbv\" name=\"gbv\" type=\"hidden\" value=\"1\"><script nonce=\"fMprxNz5aGYgYSFAnN8Vmw==\">(function(){\\nvar a,b=\"1\";if(document&&document.getElementById)if(\"undefined\"!=typeof XMLHttpRequest)b=\"2\";else if(\"undefined\"!=typeof ActiveXObject){var c,d,e=[\"MSXML2.XMLHTTP.6.0\",\"MSXML2.XMLHTTP.3.0\",\"MSXML2.XMLHTTP\",\"Microsoft.XMLHTTP\"];for(c=0;d=e[c++];)try{new ActiveXObject(d),b=\"2\"}catch(h){}}a=b;if(\"2\"==a&&-1==location.search.indexOf(\"&gbv=2\")){var f=google.gbvu,g=document.getElementById(\"gbv\");g&&(g.value=a);f&&window.setTimeout(function(){location.href=f},0)};}).call(this);</script></form><div id=\"gac_scont\"></div><div style=\"font-size:83%;min-height:3.5em\"><br><div id=\"prm\"><style>.szppmdbYutt__middle-slot-promo{font-size:small;margin-bottom:32px}.szppmdbYutt__middle-slot-promo a.ZIeIlb{display:inline-block;text-decoration:none}.szppmdbYutt__middle-slot-promo img{border:none;margin-right:5px;vertical-align:middle}</style><div class=\"szppmdbYutt__middle-slot-promo\" data-ved=\"0ahUKEwjcrNeFkKTzAhVrkeAKHeJfCvAQnIcBCAQ\"><a class=\"NKcBbd\" href=\"https://www.google.com/url?q=https://artsandculture.google.com/project/caminos-de-santiago%3Futm_source%3Dgoogle%26utm_medium%3Dhppromo%26utm_campaign%3Dcaminos-de-santiago&amp;source=hpp&amp;id=19026088&amp;ct=3&amp;usg=AFQjCNHMJPYxXzWxOwaSXDUn11FuSkOmQw&amp;sa=X&amp;ved=0ahUKEwjcrNeFkKTzAhVrkeAKHeJfCvAQ8IcBCAU\" rel=\"nofollow\">Emprende un recorrido virtual por el Camino de Santiago</a></div></div><div id=\"gws-output-pages-elements-homepage_additional_languages__als\"><style>#gws-output-pages-elements-homepage_additional_languages__als{font-size:small;margin-bottom:24px}#SIvCob{color:#3c4043;display:inline-block;line-height:28px;}#SIvCob a{padding:0 3px;}.H6sW5{display:inline-block;margin:0 2px;white-space:nowrap}.z4hgWe{display:inline-block;margin:0 2px}</style><div id=\"SIvCob\">Ofrecido por Google en:  <a href=\"http://www.google.com/setprefs?sig=0_oDls5vwF0gvt9p7A2FlGolsWbBs%3D&amp;hl=ca&amp;source=homepage&amp;sa=X&amp;ved=0ahUKEwjcrNeFkKTzAhVrkeAKHeJfCvAQ2ZgBCAc\">catal\\xe0</a>    <a href=\"http://www.google.com/setprefs?sig=0_oDls5vwF0gvt9p7A2FlGolsWbBs%3D&amp;hl=gl&amp;source=homepage&amp;sa=X&amp;ved=0ahUKEwjcrNeFkKTzAhVrkeAKHeJfCvAQ2ZgBCAg\">galego</a>    <a href=\"http://www.google.com/setprefs?sig=0_oDls5vwF0gvt9p7A2FlGolsWbBs%3D&amp;hl=eu&amp;source=homepage&amp;sa=X&amp;ved=0ahUKEwjcrNeFkKTzAhVrkeAKHeJfCvAQ2ZgBCAk\">euskara</a>  </div></div></div><span id=\"footer\"><div style=\"font-size:10pt\"><div style=\"margin:19px auto;text-align:center\" id=\"WqQANb\"><a href=\"/intl/es/ads/\">Programas de publicidad</a><a href=\"http://www.google.es/intl/es/services/\">Soluciones Empresariales</a><a href=\"/intl/es/about.html\">Todo acerca de Google</a><a href=\"http://www.google.com/setprefdomain?prefdom=ES&amp;prev=http://www.google.es/&amp;sig=K_0zzQLMIXFC369_iHbmGC0mR0YbI%3D\">Google.es</a></div></div><p style=\"font-size:8pt;color:#70757a\">&copy; 2021 - <a href=\"/intl/es/policies/privacy/\">Privacidad</a> - <a href=\"/intl/es/policies/terms/\">T\\xe9rminos</a></p></span></center><script nonce=\"fMprxNz5aGYgYSFAnN8Vmw==\">(function(){window.google.cdo={height:757,width:1440};(function(){\\nvar a=window.innerWidth,b=window.innerHeight;if(!a||!b){var c=window.document,d=\"CSS1Compat\"==c.compatMode?c.documentElement:c.body;a=d.clientWidth;b=d.clientHeight}a&&b&&(a!=google.cdo.width||b!=google.cdo.height)&&google.log(\"\",\"\",\"/client_204?&atyp=i&biw=\"+a+\"&bih=\"+b+\"&ei=\"+google.kEI);}).call(this);})();</script> <script nonce=\"fMprxNz5aGYgYSFAnN8Vmw==\">(function(){google.xjs={ck:\\'\\',cs:\\'\\',excm:[]};})();</script>  <script nonce=\"fMprxNz5aGYgYSFAnN8Vmw==\">(function(){var u=\\'/xjs/_/js/k\\\\x3dxjs.hp.en.TiW-1h-m3o0.O/am\\\\x3dAPgEWA/d\\\\x3d1/ed\\\\x3d1/rs\\\\x3dACT90oHebc3SyN9IsxC_ppKxOyFalyqbHw/m\\\\x3dsb_he,d\\';\\nvar e=this||self,f=function(a){return a};var g;var l=function(a,b){this.g=b===h?a:\"\"};l.prototype.toString=function(){return this.g+\"\"};var h={};\\nfunction m(){var a=u;google.lx=function(){n(a);google.lx=function(){}};google.bx||google.lx()}\\nfunction n(a){google.timers&&google.timers.load&&google.tick&&google.tick(\"load\",\"xjsls\");var b=document;var c=\"SCRIPT\";\"application/xhtml+xml\"===b.contentType&&(c=c.toLowerCase());c=b.createElement(c);if(void 0===g){b=null;var k=e.trustedTypes;if(k&&k.createPolicy){try{b=k.createPolicy(\"goog#html\",{createHTML:f,createScript:f,createScriptURL:f})}catch(p){e.console&&e.console.error(p.message)}g=b}else g=b}a=(b=g)?b.createScriptURL(a):a;a=new l(a,h);c.src=a instanceof l&&a.constructor===l?a.g:\"type_error:TrustedResourceUrl\";var d;a=(c.ownerDocument&&c.ownerDocument.defaultView||window).document;(d=(b=null===(d=a.querySelector)||void 0===d?void 0:d.call(a,\"script[nonce]\"))?b.nonce||b.getAttribute(\"nonce\")||\"\":\"\")&&c.setAttribute(\"nonce\",d);document.body.appendChild(c);google.psa=!0};setTimeout(function(){m()},0);})();(function(){window.google.xjsu=\\'/xjs/_/js/k\\\\x3dxjs.hp.en.TiW-1h-m3o0.O/am\\\\x3dAPgEWA/d\\\\x3d1/ed\\\\x3d1/rs\\\\x3dACT90oHebc3SyN9IsxC_ppKxOyFalyqbHw/m\\\\x3dsb_he,d\\';})();function _DumpException(e){throw e;}\\nfunction _F_installCss(c){}\\n(function(){google.jl={attn:false,blt:\\'none\\',chnk:0,dw:false,dwu:false,emtn:0,end:0,ine:false,lls:\\'default\\',pdt:0,rep:0,snet:true,strt:0,ubm:false,uwp:true};})();(function(){var pmc=\\'{\\\\x22d\\\\x22:{},\\\\x22sb_he\\\\x22:{\\\\x22agen\\\\x22:true,\\\\x22cgen\\\\x22:true,\\\\x22client\\\\x22:\\\\x22heirloom-hp\\\\x22,\\\\x22dh\\\\x22:true,\\\\x22dhqt\\\\x22:true,\\\\x22ds\\\\x22:\\\\x22\\\\x22,\\\\x22ffql\\\\x22:\\\\x22es\\\\x22,\\\\x22fl\\\\x22:true,\\\\x22host\\\\x22:\\\\x22google.com\\\\x22,\\\\x22isbh\\\\x22:28,\\\\x22jsonp\\\\x22:true,\\\\x22lm\\\\x22:true,\\\\x22msgs\\\\x22:{\\\\x22cibl\\\\x22:\\\\x22Borrar b\\xfasqueda\\\\x22,\\\\x22dym\\\\x22:\\\\x22Quiz\\xe1s quisiste decir:\\\\x22,\\\\x22lcky\\\\x22:\\\\x22Voy a tener suerte\\\\x22,\\\\x22lml\\\\x22:\\\\x22M\\xe1s informaci\\xf3n\\\\x22,\\\\x22oskt\\\\x22:\\\\x22Herramientas de introducci\\xf3n de texto\\\\x22,\\\\x22psrc\\\\x22:\\\\x22Esta b\\xfasqueda se ha eliminado de tu \\\\\\\\u003Ca href\\\\x3d\\\\\\\\\\\\x22/history\\\\\\\\\\\\x22\\\\\\\\u003Ehistorial web\\\\\\\\u003C/a\\\\\\\\u003E.\\\\x22,\\\\x22psrl\\\\x22:\\\\x22Quitar\\\\x22,\\\\x22sbit\\\\x22:\\\\x22Buscar por imagen\\\\x22,\\\\x22srch\\\\x22:\\\\x22Buscar con Google\\\\x22},\\\\x22ovr\\\\x22:{},\\\\x22pq\\\\x22:\\\\x22\\\\x22,\\\\x22refpd\\\\x22:true,\\\\x22rfs\\\\x22:[],\\\\x22sbas\\\\x22:\\\\x220 3px 8px 0 rgba(0,0,0,0.2),0 0 0 1px rgba(0,0,0,0.08)\\\\x22,\\\\x22sbpl\\\\x22:16,\\\\x22sbpr\\\\x22:16,\\\\x22scd\\\\x22:10,\\\\x22stok\\\\x22:\\\\x228RvJBFGm_bVnNKEKDSWY9Hg7NwE\\\\x22,\\\\x22uhde\\\\x22:false}}\\';google.pmc=JSON.parse(pmc);})();</script>        </body></html>'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hurray we got a socket. An all sockets behave like files, so let us go read() the \"file\"\n",
    "something = source.read()\n",
    "something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b''\n"
     ]
    }
   ],
   "source": [
    "#What!!!!\n",
    "#Let us read more\n",
    "print (source.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ooooppss nothing else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, hands on!!! \n",
    "Some first warm up exercises. Check the str api of python!\n",
    "\n",
    "<div class = \"alert alert-success\" style = \"border-radius:10px;border-width:3px;border-color:darkgreen;font-family:Verdana,sans-serif;font-size:16px;\">**WARM UP EXERCISES**\n",
    "<ol>\n",
    "<li>Is there the word python in python.org?(hint: find python in source) </li>\n",
    "<li>Does http://google.com contain an image? (hint: < img  TAG ) </li>\n",
    "<li>What are the first ten characters of python.org?</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: - True\n"
     ]
    }
   ],
   "source": [
    "# Is there the word python in python.org?(hint: find python in source) \n",
    "# Write your code here \n",
    "from urllib.request import urlopen\n",
    "source = urlopen('http://python.org').read()\n",
    "result = \"1: - \" + str(source.decode('UTF8').lower().find(\"python\")>0)\n",
    "print (result)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: - True\n"
     ]
    }
   ],
   "source": [
    "# Does http://google.com contain an image? (hint: < img  TAG )\n",
    "#Write your code here \n",
    "from urllib.request import urlopen\n",
    "source = urlopen('http://google.com').read()\n",
    "result = \"2: - \" + str(source.decode('latin1').lower().find(\"<img \")>0)\n",
    "print (result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3: - <!doctype \n"
     ]
    }
   ],
   "source": [
    "#What are the first ten characters of python.org?\n",
    "from urllib.request import urlopen\n",
    "\n",
    "source = urlopen('http://python.org').read()\n",
    "\n",
    "print (\"3: - \"+source.decode('UTF8')[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are retrieving data from an URL! So we are done! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling and Scraping\n",
    "\n",
    "Scraping and **crawling** are two very related techniques. While scraping is used for retrieving data from a web page, crawling is used to retrieve the web pages. Scraping and crawling are found at the core of search engines. Scraping is used to get keywords, analyze, and extract useful information from the web pages so that given a user query it may return related results. On the other hand, crawling allows to retrieve the actual pages and uses scraping to get the links in each web site. This allows to create a graph of the connection among web sites and this information can be used to order the results of a query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we might want not only to get data from a single page but probably retrieve from several related pages. In those cases crawling is the way to go. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px;border-width:3px;border-color:darkgreen;font-family:Verdana,sans-serif;font-size:16px;\">**WARM-UP PROJECT:** Let us build a very simple spider. The basic functionality of an spider is to crawl and store all the data in web pages. In this simple project we will take care of single site. \n",
    "\n",
    "<ol>\n",
    "<li>A crawler must recognize the links to crawl. Take a minute and think how to retrieve the links of a web site.</li>\n",
    "<li>Let us start the project by creating a Spider class. The constructor will have the following parameters: starting_url, crawl_domain, and max_iter. crawl_domain will be the domain that validates if an absolute link will be considered or not. max_iter is the maximum amount of web items to crawl.</li>\n",
    "<li>The main method can be Spider.run(). Enumerate the big functionalities/building blocks of the crawler.</li>\n",
    "</ol>\n",
    "</div>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import time\n",
    "\n",
    "def getLinks(html, max_links=10):\n",
    "    url = []\n",
    "    cursor = 0\n",
    "    nlinks=0\n",
    "    while (cursor>=0 and nlinks<max_links):\n",
    "        start_link = html.find(\"a href\",cursor)\n",
    "        if start_link==-1:\n",
    "            return url\n",
    "        start_quote = html.find('\"', start_link)\n",
    "        end_quote = html.find('\"', start_quote + 1)\n",
    "        url.append(html[start_quote + 1: end_quote])\n",
    "        cursor = end_quote+1\n",
    "        nlinks = nlinks +1\n",
    "    return url\n",
    "\n",
    "class Spider:\n",
    "    def __init__(self,starting_url,crawl_domain,max_iter):\n",
    "        self.crawl_domain = crawl_domain\n",
    "        self.max_iter = max_iter\n",
    "        self.links_to_crawl=[]\n",
    "        self.links_to_crawl.append(starting_url)\n",
    "        self.links_visited=[]\n",
    "        self.collection=[]\n",
    "        \n",
    "    def retrieveHtml(self):\n",
    "        try:\n",
    "            socket = urllib.request.urlopen(self.url);\n",
    "            encoding =   socket.headers.get_content_charset()\n",
    "            if encoding is None:\n",
    "                    encoding = \"utf-8\"\n",
    "            self.html = socket.read().decode(encoding)\n",
    "            return 0\n",
    "        except UnicodeDecodeError:\n",
    "            print (\"Bad Encoding\")\n",
    "            return -1\n",
    "        except urllib.error.HTTPError:\n",
    "            # Most probably an url not found 404, possibly due to malformating of the links in retrieveAndValidateLinks\n",
    "            print (\"Broken Link\")\n",
    "            return -1\n",
    "        except urllib.error.URLError:\n",
    "            # Most probably an url not found 404, possibly due to malformating of the links in retrieveAndValidateLinks\n",
    "            print (\"Broken Link\")\n",
    "            return -1\n",
    "    def storeHtml(self):\n",
    "        doc = {}\n",
    "        doc['url'] = self.url\n",
    "        doc['date'] = time.strftime(\"%d/%m/%Y\")\n",
    "        doc['html'] = self.html\n",
    "        self.collection.append(doc)          \n",
    "   \n",
    "    def retrieveAndValidateLinks(self):\n",
    "        tmpList=[]\n",
    "        items = getLinks(self.html)\n",
    "        # Check the validity of a link\n",
    "        for item in items:\n",
    "            item = item.strip('\"')\n",
    "            if self.crawl_domain in item:\n",
    "                tmpList.append(item)\n",
    "            if not(\":\") in item: #Take care of http:// https:// and mailto:\n",
    "                tmpList.append(self.crawl_domain+item)\n",
    "        # Check that the link has not been previously retrieved or is currently on the links_to_crawl list\n",
    "        for item in tmpList:\n",
    "            if item not in self.links_visited:\n",
    "                if item not in self.links_to_crawl:\n",
    "                    self.links_to_crawl.append(item)\n",
    "                    print ('Adding: '+item)\n",
    "                \n",
    "     \n",
    "        \n",
    "    def run(self):\n",
    "        while (len(self.links_to_crawl)>0 and len(self.collection)<self.max_iter):\n",
    "            \n",
    "            self.url = self.links_to_crawl.pop(0)\n",
    "            print (\"Visiting: \"+ self.url)\n",
    "            self.links_visited.append(self.url)\n",
    "            if self.retrieveHtml()>=0:\n",
    "                self.storeHtml()\n",
    "                self.retrieveAndValidateLinks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us validate the crawler with the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visiting: http://www.ub.edu\n",
      "Adding: http://www.ub.edu/#\n",
      "Adding: http://www.ub.edu//web/portal/ca\n",
      "Visiting: http://www.ub.edu/#\n",
      "Visiting: http://www.ub.edu//web/portal/ca\n"
     ]
    }
   ],
   "source": [
    "spider = Spider('http://www.ub.edu','http://www.ub.edu/',20)\n",
    "spider.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us go for a more complex web site. Run the code on http://hunch.net (a machine learning blog by John Langford)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visiting: http://hunch.net\n",
      "Adding: http://hunch.net#content\n",
      "Visiting: http://hunch.net#content\n"
     ]
    }
   ],
   "source": [
    "spider = Spider('http://hunch.net','http://hunch.net',10)\n",
    "spider.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://hunch.net', 'http://hunch.net#content']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And check the urls retrieved\n",
    "[spider.collection[i]['url'] for i in range(len(spider.collection))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the simple crawler more or less works as expected. There are still many functionalities to work on , such as valid domains, valid urls, etc. One important issue to consider is **persistence**, or how to store the data retrieved for further analysis. In this basic scraping tutorial we us MongoDB as a Non-SQL database for persistence purposes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Finishing the warm up project with MongoDB storage\n",
    "\n",
    "We just have to change two lines of code ... literally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import time\n",
    "import pymongo\n",
    "\n",
    "def getLinks(html, max_links=10):\n",
    "    url = []\n",
    "    cursor = 0\n",
    "    nlinks=0\n",
    "    while (cursor>=0 and nlinks<max_links):\n",
    "        start_link = html.find(\"a href\",cursor)\n",
    "        if start_link==-1:\n",
    "            return url\n",
    "        start_quote = html.find('\"', start_link)\n",
    "        end_quote = html.find('\"', start_quote + 1)\n",
    "        url.append(html[start_quote + 1: end_quote])\n",
    "        cursor = end_quote+1\n",
    "        nlinks = nlinks +1\n",
    "    return url\n",
    "\n",
    "class Spider:\n",
    "    def __init__(self,starting_url,crawl_domain,max_iter):\n",
    "        self.crawl_domain = crawl_domain\n",
    "        self.max_iter = max_iter\n",
    "        self.links_to_crawl=[]\n",
    "        self.links_to_crawl.append(starting_url)\n",
    "        self.links_visited=[]\n",
    "        try:\n",
    "            with open(\"credentials.txt\", 'r', encoding='utf-8') as f:\n",
    "                [name,password,url]=f.read().splitlines()\n",
    "                \n",
    "                self.conn=pymongo.MongoClient(\"mongodb+srv://{}:{}@{}\".format(name,password,url))\n",
    "            print (\"Connected successfully!!!\")\n",
    "        except pymongo.errors.ConnectionFailure as e:\n",
    "            print (\"Could not connect to MongoDB: %s\" % e) \n",
    "        self.db = self.conn[\"Crawler\"]\n",
    "        self.collection = self.db[starting_url[7:]+'DB']\n",
    "        \n",
    "    def retrieveHtml(self):\n",
    "        try:\n",
    "            socket = urllib.request.urlopen(self.url);\n",
    "            encoding = socket.headers.get_content_charset()\n",
    "            if encoding is None:\n",
    "                    encoding = \"utf-8\"\n",
    "            self.html = socket.read().decode(encoding)\n",
    "            return 0\n",
    "        except UnicodeDecodeError:\n",
    "            print (\"Bad Encoding\")\n",
    "            return -1\n",
    "        except urllib.error.HTTPError:\n",
    "            # Most probably an url not found 404, possibly due to malformating of the links in retrieveAndValidateLinks\n",
    "            print (\"Broken Link\")\n",
    "            return -1\n",
    "        except urllib.error.URLError:\n",
    "            # Most probably an url not found 404, possibly due to malformating of the links in retrieveAndValidateLinks\n",
    "            print (\"Broken Link\")\n",
    "            return -1\n",
    "    def storeHtml(self):\n",
    "        doc = {}\n",
    "        doc['url'] = self.url\n",
    "        doc['date'] = time.strftime(\"%d/%m/%Y\")\n",
    "        doc['html'] = self.html\n",
    "        #Insert in the collection\n",
    "        self.collection.insert_one(doc)        \n",
    "   \n",
    "    def retrieveAndValidateLinks(self):\n",
    "        tmpList=[]\n",
    "        items = getLinks(self.html)\n",
    "        # Check the validity of a link\n",
    "        for item in items:\n",
    "            item = item.strip('\"')\n",
    "            if self.crawl_domain in item:\n",
    "                tmpList.append(item)\n",
    "            if not(\":\") in item: #Take care of http:// https:// and mailto:\n",
    "                tmpList.append(self.crawl_domain+item)\n",
    "        # Check that the link has not been previously retrieved or is currently on the links_to_crawl list\n",
    "        for item in tmpList:\n",
    "            if item not in self.links_visited:\n",
    "                if item not in self.links_to_crawl:\n",
    "                    self.links_to_crawl.append(item)\n",
    "                    print ('Adding: '+item)\n",
    "                \n",
    "     \n",
    "        \n",
    "    def run(self):\n",
    "        #Change the count on the collection\n",
    "        count_i = 0\n",
    "        while (len(self.links_to_crawl)>0 and count_i<self.max_iter):   \n",
    "            self.url = self.links_to_crawl.pop(0)\n",
    "            print (\"Visiting: \"+ self.url)\n",
    "            self.links_visited.append(self.url)\n",
    "            if self.retrieveHtml()>=0:\n",
    "                self.storeHtml()\n",
    "                self.retrieveAndValidateLinks()\n",
    "                count_i = count_i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected successfully!!!\n",
      "Visiting: http://www.ub.edu\n",
      "Adding: http://www.ub.edu#\n",
      "Adding: http://www.ub.edu/web/portal/ca\n",
      "Visiting: http://www.ub.edu#\n",
      "Visiting: http://www.ub.edu/web/portal/ca\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "spider = Spider('http://www.ub.edu','http://www.ub.edu',20)\n",
    "spider.run()\n",
    "print(\"END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the collection:\n",
    "\n",
    "https://cloud.mongodb.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['www.ub.eduDB']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spider.db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = spider.db['www.ub.eduDB']\n",
    "collection.count_documents({})\n",
    "#spider.db.drop_collection(\"www.ub.eduDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29/09/2021] http://www.ub.edu\n",
      "[29/09/2021] http://www.ub.edu#\n",
      "[29/09/2021] http://www.ub.edu/web/portal/ca\n",
      "[29/09/2021] http://www.ub.edu\n",
      "[29/09/2021] http://www.ub.edu#\n",
      "[29/09/2021] http://www.ub.edu/web/portal/ca\n"
     ]
    }
   ],
   "source": [
    "for doc in collection.find():\n",
    "    print (\"[{}] {}\".format(doc['date'], doc['url']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\" style = \"border-radius:10px;border-width:3px;border-color:darkblue;font-family:Verdana,sans-serif;font-size:16px;\">\n",
    "\n",
    "**PROS and CONS:**\n",
    "<p>\n",
    "**MongoDB** querying is powerful but based on basic string operations. This actually tells us that storing full HTML pages is not going to be effiecient for retrieval. Actually, we will see that it is important to break the information in the pieces we really want. However, this is a good starting point before a post processing if we are not sure what we are going to do with the data or further scraping is going to take long. </p>\n",
    "</div>\n",
    "\n",
    "In the next section we will see more efficient ways of dealing with web based data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\" style = \"border-radius:10px;border-width:3px;border-color:darkblue;font-family:Verdana,sans-serif;font-size:16px;\">\n",
    "\n",
    "**URLLIB** is good for getting simple things. In the end you end up with a large HTML string you want to do something on it. \n",
    "So the next thing you want to do is to parse data. But you want to do it in the same way you do when you interact with the web page. You see a menu, a frame on the left side, a nice colorful block where the price for your flight is. So **you want to parse data the way you see data in the webpage so that you can target it**.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
